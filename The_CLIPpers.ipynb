{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bFGmcDnBXwsA"
      },
      "source": [
        "# **Words Are All You Need?: Using CLIP Images and Captions to Predict fMRI Responses from the Algonauts Challenge 2023**\n",
        "\n",
        "# ToDo:\n",
        "\n",
        "## **Abstract**\n",
        "\n",
        "Short Abstract to what we are doing: Predict fMRI, Algonauts, CLIP...\n",
        "* About the project: Predict fMRI (data), Algonauts\n",
        "* Research question: CLIP and text\n",
        "* Results\n",
        "* Conclusions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RGe47NAuY0bS"
      },
      "source": [
        "# ToDo:\n",
        "\n",
        "## **Introduction and Data**\n",
        "\n",
        "(Theoretical background of project and how it leads to the research question of the current study (RQ should be embedded in the literature, motivation) - cite CLIP paper + Words are all you need? Language as an approximation for human similarity judgments - R Marjieh et al.\n",
        "\n",
        "Overall goal of current study + Explanation of your hypothesis)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The Algonaut Challenge 2023 is established to investigate how well current computational models are doing. It is an open challenge to stimulate the combination of the biological and artificial intelligence field. Participants are expected to build computational models to predict brain responses for images from which brain data was held out. In this report we describe what computational model we propose to predict brain data. \n",
        "\n",
        "We propose the use of CLIP (Contrastive Language-Image Pre-Training, Radford et al., 2021), which is a state-of-the-art neural network model developed by OpenAI that can understand the relationship between images and text. It is based on a transformer architecture, similar to those used in language models like GPT-3, and is pre-trained on a large amount of text and images in an unsupervised way. What sets CLIP apart from other models is its ability to learn cross-modal representations of images and text, allowing it to understand the relationship between the two modalities. This is achieved through a contrastive learning objective, where the model learns to associate text and images that refer to the same concepts. For example, the model learns that an image of a dog and a caption that says \"a dog running in a park\" is related to the concept of a dog. The CLIP model has shown impressive results on several natural language understanding and computer vision tasks, including image classification, object detection, and image captioning.\n",
        "\n",
        "\n",
        "The 2023 Challenge data comes from the Natural Scenes Dataset (NSD) (Allen et al., 2022), a massive 8-subjects dataset of 7T fMRI responses to images of natural scenes coming from the COCO database (Lin et al., 2014). The COCO dataset contains a large number of images labeled with object categories, captions, and annotations. As previously mentioned, the key advantage of CLIP is its ability to learn cross-modal representations of images and text, allowing it to understand the relationship between the two modalities (Radford et al., 2021). This is particularly relevant for the COCO dataset, which contains both visual and textual information, such as image captions and object annotations. By leveraging this cross-modal understanding, CLIP can more effectively learn to recognize objects and infer relationships between them. Given that the Challenge evaluation score is computed over the whole visual brain, we hypothesized that a combined computational model in which visual properties interact with more abstract semantic information, offers the best solution for this challenge. Also, because CLIP has been trained on a large amount of text and images in an unsupervised way, it can generalize to new and unseen data. This is crucial in a challenge like Algonauts 2023, where the goal is to build models that can understand language and vision in a way that can generalize to new contexts and tasks. CLIP's pre-training on large amounts of data also means that it can be fine-tuned to smaller datasets, like those used in the Algonauts challenge, to further improve its performance.\n",
        "\n",
        "In the following we will load packages and functions to assist in answering the research question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nJ8S9FFhbVC"
      },
      "source": [
        "### **Initialization**\n",
        "\n",
        "Importing all required packages and models to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8Smlp2-hNu2",
        "outputId": "51e6d674-93c8-43b8-f226-a895ea240a90"
      },
      "outputs": [],
      "source": [
        "# Import required packages\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch_directml\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, CLIPTextModel, CLIPVisionModel, PreTrainedModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.decomposition import IncrementalPCA, PCA\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import euclidean_distances\n",
        "from scipy.stats import pearsonr as corr\n",
        "from scipy.stats import spearmanr\n",
        "from scipy.spatial.distance import squareform\n",
        "from tqdm import tqdm\n",
        "from typing import List, Dict, Tuple\n",
        "from nilearn import datasets\n",
        "from nilearn import plotting\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") # to reduce unnecessary output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQaCMqn2XFdB"
      },
      "source": [
        "We used an AMD RX 5700XT GPU and AMD Ryzen 5 3600XT CPU to run this code. To enable GPU support we relied on torch_directml == 0.1.13.1.dev230413.\n",
        "\n",
        "If you want to run this notebook on a cuda device, set AMD to False."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "k_eYGo4o-aI8"
      },
      "outputs": [],
      "source": [
        "# Setup cuda device\n",
        "global device\n",
        "AMD = True\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if AMD:\n",
        "    device = torch_directml.device()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFEbM3vA-b6Z"
      },
      "source": [
        "Let's already define the models we are going to use in this notebook. This is necessarry as we will use some of the functionalities throughout the notebook. We downloaded pretrained CLIPModels and the CLIPProcessor from huggingface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 883,
          "referenced_widgets": [
            "ad4d456dd48842a0840466b009fa14fe",
            "705443d223dd4f4499578ccc9e70f1dc",
            "c0700fa7799b4aed83bda55ef1bdfb1f",
            "41d8638ec4864ccdb7ac02f1927cf114",
            "fbe7063254014f5291997bf29e6c348b",
            "39d71e707edb4c6a897f79a509bf4a59",
            "30bb378acad54c34b78519adb15b7f4c",
            "7c9508713a7845e2bd6b8ea2e3787334",
            "fde355ae288d4902a1e87bc0a21e4845",
            "9aa279ff9ed245ea86a8e7a602e09d0d",
            "500f199f21f543ad94a292c126340ea5",
            "5249f98d892743ffb30aa57b2d1d9ef4",
            "faaa6d3b623f4216b4fceecdd8cd5160",
            "4775615048f44d9d843ab443cf83c8cb",
            "334f81744357430baa4b414cc46f5a44",
            "051cfcdd831c4ffa9d7dd733f15a8c13",
            "5c370cb2109d4fc5adcd11dbe37bc01a",
            "f3a0e1fe32864099ab696f37e50a3a3e",
            "001b02ca3dc74604a9c22836bae59597",
            "37d56c9464504d2fa7a9c265e2e0072e",
            "59981536dd814077b35c73f8a5627148",
            "27862ec0aa8d44cdaec17fd1805576ca",
            "9ad55ab2e2ea42ce87361f48c1d005b8",
            "7231997bb5784f02b80755fc040d6cef",
            "bf8c86299d9c4fbe82949ad0d9ec95c8",
            "b54a83fc541547bb9bd379e3955fb9f7",
            "a0bd1c6d972446a3be08b15c4db92c9f",
            "15a368b20c464bdd812fb4bc9a1053e1",
            "06a15544d53644499336eab320c8aa95",
            "44d5cbc41774425f8c3af4feb7874973",
            "9998add0831444c3b4f06e1e6cf59f64",
            "e679fac3d9d74ac0965334c0351407db",
            "88b3fe4e4423438f86130298b4a0864c",
            "79bd52f8de9848aaad6a6bcd91e66c14",
            "771860af34e34ad49b205e7abf30dc3a",
            "b7e26a6e7bd246c2aee46a9754c492e9",
            "c4abd9365ec64c98ad24735c2c5dbda0",
            "72bdbb183bee4b34b9c0dc01e357ce55",
            "ec989ac6694a40a0affe8944f2914683",
            "8852719cd30a4bac9248214657ee1c3b",
            "0857f1e9c73a4166847cd60c33e7cf80",
            "d4b46c23f306406691e94acdc848141c",
            "bea8b402738341b18efb86863774081e",
            "df612418202f47cba6e9419dd8a9a5f4",
            "bbcfcae1f7ff4ab18314876e73761a86",
            "2c1fd69c69f14eafa46aaf7c6af2f56a",
            "d5068b71be774d82b9b9f9bdfbace393",
            "c31617c1198848fc8dc37e02e5f6f021",
            "943c019d75f34694b446d9655f85e0b3",
            "912d934e59fc49ebab1010a7058ea73d",
            "9bf0cd86aa0d44258b1852b6042dbe67",
            "08f9891abc1944c8b05d6c2870ba9a73",
            "f480668dc8a942baa8415814ef878647",
            "d67a25caaa1348fb957cc3c45a52f7f6",
            "935c68ca9cd14009b8e577ad12e7cd53",
            "b8b9dd9e331944d4b863d98d78f44bca",
            "972cdd7daaef4627bdf11577aaed5d5c",
            "f39bc82e387748b8867634e7d0188fcb",
            "3b5a44d407a6419eade6f5aed2269916",
            "f221ca976f35450ea60590381bbecb31",
            "69dcec1406a94c60aec85254920ec193",
            "33ee0fe1762a48129537562d4f8b1a9f",
            "c805ccecb74048308da68c22791d8b6b",
            "954619d623b4404d8687f901badc96d5",
            "13088f0d783b44bea5c28ab181b4b87e",
            "fc8d05a96d1f46d4bb9d027cdb07f92c",
            "1d63d31e02314f70b978d320c12e63ff",
            "6c2290a3a2ab49c5a275a49fc3699b92",
            "5a35a89a46ef475bba9e3ce5c7f318a7",
            "a4fc7e2a1dcf4e9bbd942f91d7957192",
            "dc8ddff14c15407d951393db93f29641",
            "dce48adf5a7f414c800c2b8fdb2de61b",
            "c3d7716a99f04ce99c7d8a83f88145b9",
            "4e3453eb91074e52b80f43d739e0c9fb",
            "76aae9de81e2493d8e5b04fb45009d43",
            "5606173f54044918819a30f2df91023a",
            "e00b499de982455ba2a1f3a3d0a84e57",
            "dac1f64b0a4c42d5971a8f2a2290e565",
            "52cc8ea18aa54c97bebe7f6c285fb9a1",
            "c908c38eb9534f76ab0f4a23dec936aa",
            "6902a68963db4d9aa477ba5af56c39b6",
            "c49047ca0d054b68aa2a658930bf9b09",
            "2e05a00d6d84442baabd06f4233a099b",
            "f24601e7d8484bf1a7a7db6b62e91ada",
            "3d9dcf4c7889486a95d2d3e7ed57da6b",
            "db44e6edf3c7451b83c1d48bb420fd8f",
            "a162c415af8e400fb38f19769128eca4",
            "ea9ea6ac747d4c428f601f2f465fe796"
          ]
        },
        "id": "_sqS1yT6XEvE",
        "outputId": "687f0f5c-96cc-4592-ba7d-287fc0d9a662"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading (…)lve/main/config.json: 100%|██████████| 4.19k/4.19k [00:00<00:00, 837kB/s]\n",
            "Downloading pytorch_model.bin: 100%|██████████| 605M/605M [01:34<00:00, 6.41MB/s] \n",
            "Some weights of the model checkpoint at openai/clip-vit-base-patch32 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_projection.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'logit_scale', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'visual_projection.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight']\n",
            "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at openai/clip-vit-base-patch32 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'text_projection.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'logit_scale', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.pre_layrnorm.weight', 'visual_projection.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias']\n",
            "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Downloading (…)rocessor_config.json: 100%|██████████| 316/316 [00:00<00:00, 106kB/s]\n",
            "Downloading (…)okenizer_config.json: 100%|██████████| 568/568 [00:00<00:00, 116kB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100%|██████████| 862k/862k [00:00<00:00, 6.43MB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100%|██████████| 525k/525k [00:00<00:00, 6.26MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.22M/2.22M [00:00<00:00, 3.99MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100%|██████████| 389/389 [00:00<00:00, 126kB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CLIPTextModel(\n",
              "  (text_model): CLIPTextTransformer(\n",
              "    (embeddings): CLIPTextEmbeddings(\n",
              "      (token_embedding): Embedding(49408, 512)\n",
              "      (position_embedding): Embedding(77, 512)\n",
              "    )\n",
              "    (encoder): CLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): QuickGELUActivation()\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Defining Models\n",
        "global vis_model, txt_model, processor\n",
        "vis_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "txt_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "vis_model.eval()\n",
        "txt_model.eval()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Pla4gW1pwvZU"
      },
      "source": [
        "### **Helper Functions and Classes**\n",
        "\n",
        "In the following section we define a lot of helpful classes and methods we will use throughout the notebook to prepare, modify, and fit the data. Explanations for each sub-sections are provided."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Utility Functions**\n",
        "Some utility functions to reduce code chunks and streamline some across-subject operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **Shared Neural RDMs**\n",
        "The below function calculates and prepares the neural RDMs from the neural observations corresponding to the shared images (in the trainig data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def neural_rdm_shared(subs: str = [f\"subj0{i}\" for i in range(1, 9)]):\n",
        "    \"\"\"\"Function to compute the shared neural RDMs for the specified subjects\n",
        "    Args:\n",
        "            subs: list of strings indicating a subjects identification\"\"\"\n",
        "    train_cap_file = pd.read_csv('data/algonauts_2023_caption_data.csv')\n",
        "\n",
        "    # Save all shared trials per subject\n",
        "    shared_img = []\n",
        "    for sub in subs:\n",
        "        dirs = Subject(sub)\n",
        "        dirs.load_image_paths()\n",
        "\n",
        "        img_match = [int(i[-9:-4]) for i in dirs.train_img_list]\n",
        "        sub_df = train_cap_file[(train_cap_file['subject'] == sub) & (train_cap_file['nsdId'].isin(img_match))].reset_index(drop=True)\n",
        "        shared_img.append(sub_df[(sub_df['n'] == 8)][['nsdId']])\n",
        "\n",
        "    # Counting the shared images that occur in all subjects training split and saving the IDs\n",
        "    counts_id = pd.concat(shared_img, axis=0, join=\"inner\").groupby(\"nsdId\").size()\n",
        "    keep_id = counts_id[counts_id == 8].index.tolist()\n",
        "\n",
        "    # Calculating the neural RDMs based on the retained IDs and corresponding indexes\n",
        "    neural_rdms_shared = []\n",
        "    for i, sub in enumerate(subs):\n",
        "        dirs = Subject(sub)\n",
        "        dirs.load_neural_data()\n",
        "\n",
        "        shared_idx = shared_img[i][shared_img[i][\"nsdId\"].isin(keep_id)].index.values\n",
        "        lh_fmri_shared, rh_fmri_shared = dirs.lh_fmri[shared_idx], dirs.rh_fmri[shared_idx]\n",
        "\n",
        "        neural_rdms_shared.append(np.stack([euclidean_distances(lh_fmri_shared), euclidean_distances(rh_fmri_shared)]))\n",
        "    \n",
        "    return np.stack(neural_rdms_shared)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **Plot Neural RDMs**\n",
        "\n",
        "The below function plots the neural RDMs and allows different zooms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_neural_rdm(neural_rdms: dict = None, zoom: list = None):\n",
        "    \"\"\"Plots the neural RDMs for the given subject.\n",
        "    Args:\n",
        "            neural_rdms: dictionary containing the neural RDMs for the left and right hemisphere\n",
        "            zoom: list containing the range of the x and y axis\"\"\"\n",
        "    \n",
        "    fig, ax = plt.subplots(1, len(neural_rdms), figsize=(10, 10))\n",
        "    plt.subplots_adjust(wspace=0.3)\n",
        "    for i, hemisphere in enumerate(neural_rdms):\n",
        "        ax[i].imshow(neural_rdms[hemisphere])\n",
        "        ax[i].set_xlabel(\"Trials\", fontsize=8)\n",
        "        ax[i].set_title(f\"{hemisphere} Hemisphere RDM\", fontsize=10)\n",
        "        if zoom:\n",
        "            ax[i].set_xlim(zoom)\n",
        "            zoom.reverse()\n",
        "            ax[i].set_ylim(zoom)\n",
        "    plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **ROI Class Index**\n",
        "\n",
        "Defining a function that returns the index of a given ROI class. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def roi_class_index(roi: str = \"V1v\"):\n",
        "    \"\"\"\"Function to return the roi class index given the specified roi\n",
        "    Args:\n",
        "            subs: string indicating the roi\"\"\"\n",
        "    \n",
        "    if roi in [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\"]:\n",
        "        roi_class = 0\n",
        "    elif roi in [\"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\"]:\n",
        "        roi_class = 1\n",
        "    elif roi in [\"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\"]:\n",
        "        roi_class = 2\n",
        "    elif roi in [\"OPA\", \"PPA\", \"RSC\"]:\n",
        "        roi_class = 3\n",
        "    elif roi in [\"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\"]:\n",
        "        roi_class = 4\n",
        "    elif roi in [\"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]:\n",
        "        roi_class = 5\n",
        "    else:\n",
        "        roi_class = 6\n",
        "\n",
        "    return roi_class"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **ImageDataset & TextDataset**\n",
        "The ImageDataset and TextDataset classes are used to create datasets for PyTorch dataloaders. The ImageDataset is used for the NSD images and the TextDataset for the captions of the corresponding coco images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cU9pJBGHrBsF"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "    \"\"\"Class to prepare the image data for the PyTorch DataLoader\"\"\"\n",
        "    def __init__(self, image_list, processor):\n",
        "        self.image_list = image_list\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_list[idx])\n",
        "        image = self.processor(images=image, return_tensors=\"pt\", padding=True)\n",
        "        return image[\"pixel_values\"].squeeze()\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    \"\"\"\"Class to prepare the text data for the PyTorch DataLoader\"\"\"\n",
        "    def __init__(self, text, max_length, processor):\n",
        "        self.text = processor(text=text, return_tensors=\"pt\", padding=\"max_length\", max_length=max_length)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.text[\"input_ids\"][idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtZW44DBNXcp"
      },
      "source": [
        "#### **Subject Class**\n",
        "\n",
        "The Subject class is initialized with a valid subject id (e.g., \"subj01\"). It stores all relevant paths and can load the data for the given subject."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XXZOgmcT_OR3"
      },
      "outputs": [],
      "source": [
        "class Subject:\n",
        "    \"\"\"Class to access all relevant data for a given subject\"\"\"\n",
        "    def __init__(self, subject=\"subj01\"):\n",
        "        assert subject in [\"subj01\", \"subj02\", \"subj03\", \"subj04\", \"subj05\", \"subj06\", \"subj07\", \"subj08\",], \"Invalid subject\"\n",
        "        self.subject = subject\n",
        "        self.data_dir = \"data/algonauts_2023_challenge_data\"\n",
        "        self.training_images_dir = f\"{self.data_dir}/{subject}/training_split/training_images\"\n",
        "        self.test_images_dir = f\"{self.data_dir}/{subject}/test_split/test_images\"\n",
        "        self.training_fmri_dir = f\"{self.data_dir}/{subject}/training_split/training_fmri\"\n",
        "        self.roi_masks_dir = f\"{self.data_dir}/{subject}/roi_masks\"\n",
        "        self.submission_dir = f\"algonauts_2023_challenge_submission\"\n",
        "        # Load these as needed\n",
        "        self.train_img_list = None\n",
        "        self.test_img_list = None\n",
        "        self.train_cap_list = None\n",
        "        self.test_cap_list = None\n",
        "        self.lh_fmri = None\n",
        "        self.rh_fmri = None\n",
        "        self.lh_roi_masks = None\n",
        "        self.rh_roi_masks = None\n",
        "        self.roi_name_maps = None\n",
        "        self.lh_challenge_rois = None\n",
        "        self.rh_challenge_rois = None\n",
        "        self.train_img_dataloader = None\n",
        "        self.test_img_dataloader = None\n",
        "        self.train_cap_dataloader = None\n",
        "        self.test_cap_dataloader = None            \n",
        "        \n",
        "    def load_image_paths(self) -> None:\n",
        "        \"\"\"Loads the image paths from the training and test directories\"\"\"\n",
        "        self.train_img_list = glob.glob(f\"{self.training_images_dir}/*.png\")\n",
        "        self.train_img_list.sort()\n",
        "        self.test_img_list = glob.glob(f\"{self.test_images_dir}/*.png\")\n",
        "        self.test_img_list.sort()\n",
        "        # print(f\"Training images: {len(self.train_img_list)}\")\n",
        "        # print(f\"Test images: {len(self.test_img_list)}\")\n",
        "\n",
        "    def load_captions(self) -> None:\n",
        "        \"\"\"Loads and matches the captions from the csv file\"\"\"\n",
        "        if self.train_img_list is None:\n",
        "            self.load_image_paths()\n",
        "        train_cap_file = pd.read_csv('data/algonauts_2023_caption_data.csv')\n",
        "        img_match = [int(i[-9:-4]) for i in self.train_img_list]\n",
        "        self.train_cap_list = train_cap_file[(train_cap_file['subject'] == self.subject) & (train_cap_file['nsdId'].isin(img_match))]['caption'].tolist()\n",
        "        self.test_cap_list = train_cap_file[(train_cap_file['subject'] == self.subject) & (~train_cap_file['nsdId'].isin(img_match))]['caption'].tolist()\n",
        "        # print(f\"Training captions: {len(self.train_cap_list)}\")\n",
        "        # print(f\"Test captions: {len(self.test_cap_list)}\")\n",
        "    \n",
        "    def load_neural_data(self) -> None:\n",
        "        \"\"\"Loads the neural data from the .npy files\"\"\"\n",
        "        self.lh_fmri = np.load(f\"{self.training_fmri_dir}/lh_training_fmri.npy\")\n",
        "        self.rh_fmri = np.load(f\"{self.training_fmri_dir}/rh_training_fmri.npy\")\n",
        "        # print(f\"Left hemisphere neural data loaded. Shape: {self.lh_fmri.shape}\")\n",
        "        # print(f\"Right hemisphere neural data loaded. Shape: {self.rh_fmri.shape}\")\n",
        "\n",
        "    def create_dataloaders(self, processor, batch_size) -> None:\n",
        "        \"\"\"Creates the dataloaders for the images and captions\"\"\"\n",
        "        if self.train_img_list is None:\n",
        "            self.load_image_paths()\n",
        "        if self.train_cap_list is None:\n",
        "            self.load_captions()\n",
        "        max_caption_len = processor(text=self.train_cap_list + self.test_cap_list, return_tensors=\"pt\", padding=True)[\"input_ids\"].shape[1]   \n",
        "        train_txt_dataset = TextDataset(self.train_cap_list, max_caption_len, processor)\n",
        "        test_txt_dataset = TextDataset(self.test_cap_list, max_caption_len, processor)\n",
        "        train_img_dataset = ImageDataset(self.train_img_list, processor)\n",
        "        test_img_dataset = ImageDataset(self.test_img_list, processor)\n",
        "        self.train_img_dataloader = DataLoader(train_img_dataset, batch_size=batch_size, shuffle=False)\n",
        "        self.test_img_dataloader = DataLoader(test_img_dataset, batch_size=batch_size, shuffle=False)\n",
        "        self.train_txt_dataloader = DataLoader(train_txt_dataset, batch_size=batch_size, shuffle=False)\n",
        "        self.test_txt_dataloader = DataLoader(test_txt_dataset, batch_size=batch_size, shuffle=False)\n",
        "        print(f\"Train image dataloader: {len(self.train_img_dataloader)} batches\")\n",
        "        print(f\"Test image dataloader: {len(self.test_img_dataloader)} batches\")\n",
        "        print(f\"Train caption dataloader: {len(self.train_txt_dataloader)} batches\")\n",
        "        print(f\"Test caption dataloader: {len(self.test_txt_dataloader)} batches\")\n",
        "\n",
        "    def load_challenge_rois(self) -> None:\n",
        "        \"\"\"Loads the challenge rois from the .npy files\"\"\"\n",
        "        # Load the ROI classes mapping dictionaries\n",
        "        roi_mapping_files = ['mapping_prf-visualrois.npy', 'mapping_floc-bodies.npy',\n",
        "            'mapping_floc-faces.npy', 'mapping_floc-places.npy',\n",
        "            'mapping_floc-words.npy', 'mapping_streams.npy']\n",
        "        self.roi_name_maps = []\n",
        "        for r in roi_mapping_files:\n",
        "            self.roi_name_maps.append(np.load(f\"{self.roi_masks_dir}/{r}\", allow_pickle=True).item())\n",
        "\n",
        "        # Load the ROI brain surface maps\n",
        "        lh_challenge_roi_files = ['lh.prf-visualrois_challenge_space.npy',\n",
        "            'lh.floc-bodies_challenge_space.npy', 'lh.floc-faces_challenge_space.npy',\n",
        "            'lh.floc-places_challenge_space.npy', 'lh.floc-words_challenge_space.npy',\n",
        "            'lh.streams_challenge_space.npy']\n",
        "        rh_challenge_roi_files = ['rh.prf-visualrois_challenge_space.npy',\n",
        "            'rh.floc-bodies_challenge_space.npy', 'rh.floc-faces_challenge_space.npy',\n",
        "            'rh.floc-places_challenge_space.npy', 'rh.floc-words_challenge_space.npy',\n",
        "            'rh.streams_challenge_space.npy']\n",
        "        self.lh_challenge_rois = []\n",
        "        self.rh_challenge_rois = []\n",
        "        for r in range(len(lh_challenge_roi_files)):\n",
        "            self.lh_challenge_rois.append(np.load(f\"{self.roi_masks_dir}/{lh_challenge_roi_files[r]}\"))\n",
        "            self.rh_challenge_rois.append(np.load(f\"{self.roi_masks_dir}/{rh_challenge_roi_files[r]}\"))\n",
        "\n",
        "    def load_roi_masks(self, roi=\"V1v\", hemisphere=\"lh\"):\n",
        "        valid_roi = [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \n",
        "                     \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \n",
        "                     \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \n",
        "                     \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \n",
        "                     \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \n",
        "                     \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \n",
        "                     \"midparietal\", \"ventral\", \"lateral\", \"parietal\",\n",
        "                     \"all-vertices\"]\n",
        "        valid_hemisphere = [\"lh\", \"rh\"]\n",
        "        assert roi in valid_roi, \"Invalid ROI\"\n",
        "        assert hemisphere in valid_hemisphere, \"Invalid hemisphere\"\n",
        "\n",
        "        # Define the ROI class based on the selected ROI\n",
        "        roi_class = ['prf-visualrois', 'floc-bodies', 'floc-faces', 'floc-places', 'floc-words', 'streams' , 'all-vertices'][roi_class_index(roi)]\n",
        "\n",
        "        roi_class_dir = f\"{hemisphere}.{roi_class}_fsaverage_space.npy\"\n",
        "        roi_map_dir = f\"mapping_{roi_class}.npy\"\n",
        "        fsaverage_roi_class = np.load(f\"{self.roi_masks_dir}/{roi_class_dir}\")\n",
        "        roi_map = None\n",
        "        if roi != \"all-vertices\":\n",
        "            roi_map = np.load(f\"{self.roi_masks_dir}/{roi_map_dir}\", allow_pickle=True).item()\n",
        "        return fsaverage_roi_class, roi_map"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "faFr51PBrqmO"
      },
      "source": [
        "#### **CLIPFeatureExtractor Class**\n",
        "\n",
        "The CLIPFeatureExtractor class is used to extract the hidden states from any clip model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "anlXCb1LGc-H"
      },
      "outputs": [],
      "source": [
        "class CLIPFeatureExtractor():\n",
        "    \"\"\"Extracts the features from hidden states of a CLIP model.\"\"\"\n",
        "    def __init__(\n",
        "            self, \n",
        "            idxs: list = [i for i in range(13)], # hidden layer indices to extract features from. Standard CLIP has an embedding layer and 12 transformer layers.\n",
        "            last_hidden_layer: bool = False, # whether to extract features from the last hidden layer\n",
        "            model: PreTrainedModel = None, # CLIP model\n",
        "            dataloader: DataLoader = None, # dataloader for batching\n",
        "            ) -> None:\n",
        "        self.idxs = idxs\n",
        "        self.last_hidden_layer = last_hidden_layer\n",
        "        self.generate_feature_dict()\n",
        "        if self.last_hidden_layer:\n",
        "            self.idxs.append(13) # adds an additional idx to allow for loop zip()\n",
        "        self.model = model\n",
        "        self.dataloader = dataloader\n",
        "        print(f\"idxs: {self.idxs}\")\n",
        "        print(f\"feature dict keys: {self.feature_dict.keys()}\")\n",
        "    \n",
        "    def generate_feature_dict(self) -> None:\n",
        "        \"\"\"Generates a feature dict according to the idxs and last_hidden_layer attributes.\"\"\"\n",
        "        feature_dict = {}\n",
        "        for idx in self.idxs:\n",
        "            if idx == 0:\n",
        "                feature_dict[\"Embedding Layer\"] = None\n",
        "            else:\n",
        "                feature_dict[f\"Transformer Layer {idx}\"] = None\n",
        "        if self.last_hidden_layer:\n",
        "            feature_dict[\"Final Layer\"] = None\n",
        "        self.feature_dict = feature_dict\n",
        "    \n",
        "    def concat_features(self, features: dict) -> None:\n",
        "        \"\"\"Adds extracted features to the feature dict.\n",
        "        Args:\n",
        "            features: features extracted from the output of a CLIP model\"\"\"\n",
        "        keys = list(self.feature_dict.keys())\n",
        "        # check if feature_dict is empty\n",
        "        if self.feature_dict[keys[0]] is None:\n",
        "            self.feature_dict = features\n",
        "        else:\n",
        "            for key in keys:\n",
        "                self.feature_dict[key] = np.concatenate((self.feature_dict[key], features[key]), axis=0)\n",
        "\n",
        "    def extract_raw_features(self, output) -> None: \n",
        "        \"\"\"Extracts features from the hidden states of a CLIP model and concates them to the feature_dict.\n",
        "        Args:\n",
        "            output: output of a CLIP model\n",
        "        \"\"\"\n",
        "        features = {}\n",
        "        for idx, key in zip(self.idxs, self.feature_dict.keys()):\n",
        "            if key == \"Final Layer\":\n",
        "                features[key] = output.last_hidden_state.cpu().detach().numpy()\n",
        "            else:\n",
        "                features[key] = output.hidden_states[idx].cpu().detach().numpy()\n",
        "        self.concat_features(features)\n",
        "    \n",
        "    def extract_raw_features_from_model(self) -> None:\n",
        "        \"\"\"Runs the CLIP model on the dataloader and extracts features from the hidden states.\"\"\"\n",
        "        self.model = self.model.to(device)\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(self.dataloader):\n",
        "                batch = batch.to(device)\n",
        "                output = self.model(batch, output_hidden_states=True)\n",
        "                self.extract_raw_features(output)\n",
        "                batch = None # clear batch from memory\n",
        "                output = None # clear output from memory\n",
        "        self.model = self.model.to(\"cpu\")        "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MbjZCFo3r9kF"
      },
      "source": [
        "#### **KFoldProcedure & KFold Classes**\n",
        "\n",
        "The KFoldProcedure class is used to define a procedure that is executed during each fold of the k-fold validation. It can be supplied to a KFold class which executes its run() function on all folds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9YGF3MSMr94i"
      },
      "outputs": [],
      "source": [
        "class KFoldProcedure:\n",
        "    \"\"\"This class is used to define a procedure that is run on each fold of a k-fold cross validation.\"\"\"\n",
        "    def __init__(self) -> None:\n",
        "        assert isinstance(self.model_name, str) and len(self.model_name) > 0, \"Please define a model name as part of the KFold Procedure.\"\n",
        "        assert isinstance(self.description, str ) and len(self.description) > 0 , \"Please define a description as part of the KFold Procedure.\"\n",
        "\n",
        "    def prepare(self) -> None:\n",
        "        \"\"\"Operations that should be executed before the fold loop\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def run(self, train_idxs: np.ndarray, val_idxs: np.ndarray) -> Dict[str, Dict[str, np.ndarray]]:\n",
        "        \"\"\"This should return a dict of correlations.\n",
        "        dict format: {\"layer\": {\"lh\": np.ndarray, \"lh\": np.ndarray}}\"\"\"\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def return_idxs(self):\n",
        "        \"\"\"Returns idxs to create folds in the KFold class.\"\"\"\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def return_roi_names(self) -> List[str]:\n",
        "        \"\"\"Required for the plot function in the KFold class.\"\"\"\n",
        "        return self.roi_names    \n",
        "    \n",
        "    def return_model_name_and_description(self) -> Tuple[str, str]:\n",
        "        return self.model_name, self.description\n",
        "\n",
        "    def calculate_correlations(self, lh_pred, rh_pred, lh_fmri, rh_fmri) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Calculate correlation between prediction and fmri activation\"\"\"\n",
        "        lh_correlation = np.zeros(lh_pred.shape[1])\n",
        "        for v in tqdm(range(lh_pred.shape[1])):\n",
        "            lh_correlation[v] = corr(lh_pred[:,v], lh_fmri[:,v])[0]\n",
        "        # Right hemisphere\n",
        "        rh_correlation = np.zeros(rh_pred.shape[1])\n",
        "        for v in tqdm(range(rh_pred.shape[1])):\n",
        "            rh_correlation[v] = corr(rh_pred[:,v], rh_fmri[:,v])[0]\n",
        "        return lh_correlation, rh_correlation\n",
        "\n",
        "    def calculate_median_correlations(self, lh_correlation, rh_correlation) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Calculate median correlation for each ROI.\"\"\"\n",
        "        # Select the correlation results vertices of each ROI\n",
        "        lh_challange_rois = self.lh_challenge_rois\n",
        "        rh_challange_rois = self.rh_challenge_rois\n",
        "        self.roi_names = []\n",
        "        lh_roi_correlation = []\n",
        "        rh_roi_correlation = []\n",
        "        for r1 in range(len(lh_challange_rois)):\n",
        "            for r2 in self.roi_name_maps[r1].items():\n",
        "                if r2[0] != 0: # zeros indicate to vertices falling outside the ROI of interest\n",
        "                    self.roi_names.append(r2[1])\n",
        "                    lh_roi_idx = np.where(lh_challange_rois[r1] == r2[0])[0]\n",
        "                    rh_roi_idx = np.where(rh_challange_rois[r1] == r2[0])[0]\n",
        "                    lh_roi_correlation.append(lh_correlation[lh_roi_idx])\n",
        "                    rh_roi_correlation.append(rh_correlation[rh_roi_idx])\n",
        "        self.roi_names.append('All vertices')\n",
        "        lh_roi_correlation.append(lh_correlation)\n",
        "        rh_roi_correlation.append(rh_correlation)\n",
        "        lh_median_roi_correlation = [np.median(lh_roi_correlation[r])\n",
        "            for r in range(len(lh_roi_correlation))]\n",
        "        rh_median_roi_correlation = [np.median(rh_roi_correlation[r])\n",
        "            for r in range(len(rh_roi_correlation))]\n",
        "        return lh_median_roi_correlation, rh_median_roi_correlation\n",
        "    \n",
        "class KFold:\n",
        "    \"\"\"Run a k-fold cross validation with a given procedure.\"\"\"\n",
        "    def __init__(self, folds: int = 8, seed: int = 5, procedure: KFoldProcedure = None) -> None:\n",
        "        assert folds > 1, \"folds must be greater than 1\"\n",
        "        assert seed > 0, \"seed must be greater than 0\"\n",
        "        assert isinstance(folds, int), \"folds must be an integer\"\n",
        "        assert isinstance(seed, int), \"seed must be an integer\"\n",
        "        #assert isinstance(procedure, KFoldProcedure), \"procedure must be an instance of KFoldProcedure\"\n",
        "        self.folds = folds\n",
        "        self.seed = seed\n",
        "        self.procedure = procedure\n",
        "        self.fold_correlations = {}\n",
        "        self.mean_correlations = None\n",
        "\n",
        "    def run(self) -> None:\n",
        "        \"\"\"Runs the procedure on each fold and accesses the correlations.\"\"\"\n",
        "        self.procedure.prepare()\n",
        "        # Create k folds   \n",
        "        fold_idxs = self.procedure.return_idxs()\n",
        "        np.random.seed(self.seed)\n",
        "        np.random.shuffle(fold_idxs)\n",
        "        self.fold_idxs = np.array_split(fold_idxs, self.folds)\n",
        "\n",
        "        for fold in range(self.folds):\n",
        "            # Select validation and train set\n",
        "            val_idxs = self.fold_idxs[fold]\n",
        "            train_idxs = np.concatenate([self.fold_idxs[j] for j in range(self.folds) if j != fold])\n",
        "            \n",
        "            # Info for current fold\n",
        "            print(f\"#############################################\")\n",
        "            print(f\"# Fold: {fold+1}/ {self.folds}\")         \n",
        "            print(f\"# Train size: {len(train_idxs)}\")\n",
        "            print(f\"# Validation size: {len(val_idxs)}\")\n",
        "            print(f\"#############################################\")\n",
        "\n",
        "            # Run procedure\n",
        "            self.fold_correlations[fold] = self.procedure.run(train_idxs, val_idxs)\n",
        "        # Get model name and description\n",
        "        self.model_name, self.description = self.procedure.return_model_name_and_description()\n",
        "        self.roi_names = self.procedure.return_roi_names()\n",
        "        self.calculate_mean_accross_folds()\n",
        "        self.mean_correlations_to_csv()\n",
        "    \n",
        "    def calculate_mean_accross_folds(self):\n",
        "        \"\"\"Calculates the mean across folds for each layer\"\"\"\n",
        "        self.mean_correlations = {}\n",
        "        for layer in self.fold_correlations[0].keys():\n",
        "            self.mean_correlations[layer] = {}\n",
        "            for hemi in self.fold_correlations[0][layer].keys():\n",
        "                self.mean_correlations[layer][hemi] = np.nanmean([self.fold_correlations[fold][layer][hemi] for fold in range(self.folds)], axis=0)\n",
        "    \n",
        "    def mean_correlations_to_csv(self) -> None:\n",
        "        df = pd.DataFrame(columns=[\"model\", \"layer\", \"hemisphere\", \"roi\", \"correlation\"])\n",
        "        for layer in self.mean_correlations.keys():\n",
        "                for hemisphere in self.mean_correlations[layer].keys():\n",
        "                    for i in range(len(self.roi_names)):\n",
        "                        df = df.append({\"model\": self.model_name, \"layer\": layer, \"hemisphere\": hemisphere, \"roi\": self.roi_names[i], \"correlation\": self.mean_correlations[layer][hemisphere][i]}, ignore_index=True)\n",
        "\n",
        "        validations = glob.glob(f\"validations/validation*\")\n",
        "        if len(validations) == 0:\n",
        "            # create first validation folder\n",
        "            folder_name = \"validation001\"\n",
        "            os.mkdir(f\"validations/{folder_name}\")\n",
        "        else:\n",
        "            # create next validation folder\n",
        "            last_validation = sorted(validations)[-1]\n",
        "            last_validation_number = int(last_validation.split(\"/\")[-1].split(\"validation\")[-1])\n",
        "            next_validation_number = last_validation_number + 1\n",
        "            folder_name = f\"validation{str(next_validation_number).zfill(3)}\"\n",
        "            os.mkdir(f\"validations/{folder_name}\")\n",
        "\n",
        "        # Write text file with model description\n",
        "        with open(f\"validations/{folder_name}/info.txt\", \"w\") as f:\n",
        "            f.write(self.description)\n",
        "\n",
        "        # Save dataframe\n",
        "        df.to_csv(f\"validations/{folder_name}/results.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f4KbvREBN-B"
      },
      "source": [
        "Additionally we define a function to plot the validation results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2ADW5-xNBO0_"
      },
      "outputs": [],
      "source": [
        "def plot_kfold_result(validation = \"001\"):\n",
        "    \"\"\"Plots the validation results from the csv file in the given validaiton folder.\"\"\"\n",
        "    folder = f\"validations/validation{validation}\"\n",
        "    with open(f\"{folder}/info.txt\", 'r') as f:\n",
        "        info = f.read()\n",
        "    df = pd.read_csv(f\"{folder}/results.csv\")\n",
        "    # drop model column\n",
        "    df = df.drop(\"model\", axis=1)\n",
        "\n",
        "    # Define color palette and assign colors to layers\n",
        "    palette = sns.color_palette(\"colorblind\", 14)\n",
        "    layer_colors = {layer: palette[i] for i, layer in enumerate(df.layer.unique())}\n",
        "\n",
        "    # Split data into left and right hemispheres\n",
        "    left_df = df[df['hemisphere'] == 'lh']\n",
        "    right_df = df[df['hemisphere'] == 'rh']\n",
        "\n",
        "    # Create bar plots\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(30, 10))\n",
        "    fig.suptitle(info)\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "\n",
        "    bar_width = 0.05\n",
        "    # Plot left hemisphere data\n",
        "    for i, layer in enumerate(df.layer.unique()):\n",
        "        layer_data = left_df[left_df['layer'] == layer]\n",
        "        x = np.arange(len(layer_data['roi']))\n",
        "        # center bars around xtick\n",
        "        axes[0].bar(x - len(df.layer.unique())/2 * 0.05 + i * 0.05, layer_data['correlation'], width=bar_width, label=layer, color=layer_colors[layer])\n",
        "\n",
        "    axes[0].margins(x=0.01) # reduce white space before first x tick\n",
        "    axes[0].set_xticks([i for i in range(len(layer_data['roi']))])\n",
        "    axes[0].set_xticklabels([roi for roi in layer_data['roi']])\n",
        "    axes[0].set_title('Left Hemisphere')\n",
        "    axes[0].set_xlabel('ROI')\n",
        "    axes[0].tick_params(axis='x', labelrotation=45)\n",
        "    axes[0].set_ylabel('Correlation')\n",
        "    axes[0].legend(loc='upper center', bbox_to_anchor=(0.5, 1), ncol=5)\n",
        "    axes[0].set_ylim([0, 1])\n",
        "    row_colors = [layer_colors[layer] for layer in left_df.groupby('layer').mean().sort_values(by='correlation', ascending=False).index]\n",
        "    axes[0].table(cellText=left_df.groupby('layer').mean().sort_values(by='correlation', ascending=False).round(3).values, rowLabels=df.groupby('layer').mean().sort_values(by='correlation', ascending=False).index, colLabels=[\"Mean Correlation\"], rowColours=row_colors, bbox = [0.95, 0.5, 0.05, 0.5])\n",
        "\n",
        "\n",
        "    # Plot right hemisphere data\n",
        "    for i, layer in enumerate(df.layer.unique()):\n",
        "        layer_data = right_df[right_df['layer'] == layer]\n",
        "        x = np.arange(len(layer_data['roi']))\n",
        "        axes[1].bar(x - len(df.layer.unique())/2 * 0.05 + i * 0.05, layer_data['correlation'], width=bar_width, label=layer, color=layer_colors[layer])\n",
        "    \n",
        "    axes[1].margins(x=0.01)\n",
        "    axes[1].set_xticks([i for i in range(len(layer_data['roi']))])\n",
        "    axes[1].set_xticklabels([roi for roi in layer_data['roi']])\n",
        "    axes[1].set_title('Right Hemisphere')\n",
        "    axes[1].set_xlabel('ROI')\n",
        "    axes[1].tick_params(axis='x', labelrotation=45)\n",
        "    axes[1].set_ylabel('Correlation')\n",
        "    axes[1].legend(loc='upper center', bbox_to_anchor=(0.5, 1), ncol=5)\n",
        "    axes[1].set_ylim([0, 1])\n",
        "    row_colors = [layer_colors[layer] for layer in right_df.groupby('layer').mean().sort_values(by='correlation', ascending=False).index]\n",
        "    axes[1].table(cellText=right_df.groupby('layer').mean().sort_values(by='correlation', ascending=False).round(3).values, rowLabels=df.groupby('layer').mean().sort_values(by='correlation', ascending=False).index, colLabels=[\"Mean Correlation\"], rowColours=row_colors, bbox = [0.95, 0.5, 0.05, 0.5])\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E24aIUwFQTqx"
      },
      "source": [
        "### **Subject**\n",
        "\n",
        "Which Subject are you interested in investigating?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "sEYna_ZDQUdf"
      },
      "outputs": [],
      "source": [
        "# Select the subject\n",
        "subject = \"subj01\" # [\"subj01\", \"subj02\", \"subj03\", \"subj04\", \"subj05\", \"subj06\", \"subj07\", \"subj08\"]\n",
        "dirs = Subject(subject)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "W5m8AFjsxl6n"
      },
      "source": [
        "### **Load the fMRI training data**\n",
        "\n",
        "First we load the fMRI training split data of the selected subject. The fMRI data consists of two `'.npy'` files:\n",
        "* ```lh_training_fmri.npy```: the left hemisphere (LH) fMRI data.\n",
        "* ```rh_training_fmri.npy```: the right hemisphere (RH) fMRI data.\n",
        "\n",
        "Both files are 2-dimensional arrays with training stimulus images as rows and fMRI vertices as columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L28oKgfKxlnP",
        "outputId": "f7d49f2b-7808-492a-f502-f9358c541993"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/algonauts_2023_challenge_data/subj01/training_split/training_fmri/lh_training_fmri.npy'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Load Neural data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dirs\u001b[39m.\u001b[39;49mload_neural_data()\n\u001b[0;32m      3\u001b[0m lh_fmri, rh_fmri \u001b[39m=\u001b[39m dirs\u001b[39m.\u001b[39mlh_fmri, dirs\u001b[39m.\u001b[39mrh_fmri\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLeft hemisphere neural data loaded. Shape: \u001b[39m\u001b[39m{\u001b[39;00mlh_fmri\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[16], line 51\u001b[0m, in \u001b[0;36mSubject.load_neural_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_neural_data\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Loads the neural data from the .npy files\"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlh_fmri \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_fmri_dir\u001b[39m}\u001b[39;49;00m\u001b[39m/lh_training_fmri.npy\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     52\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrh_fmri \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_fmri_dir\u001b[39m}\u001b[39;00m\u001b[39m/rh_training_fmri.npy\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\mirth\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    403\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    406\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/algonauts_2023_challenge_data/subj01/training_split/training_fmri/lh_training_fmri.npy'"
          ]
        }
      ],
      "source": [
        "# Load Neural data\n",
        "dirs.load_neural_data()\n",
        "lh_fmri, rh_fmri = dirs.lh_fmri, dirs.rh_fmri\n",
        "\n",
        "print(f\"Left hemisphere neural data loaded. Shape: {lh_fmri.shape}\")\n",
        "print(f\"Right hemisphere neural data loaded. Shape: {rh_fmri.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIPIMJs0dlf-"
      },
      "source": [
        "Let's look at the neural RDMs, for both hemispheres, of subject 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "Y941nSEpdtTl",
        "outputId": "3d98e210-1421-4ce9-9e8c-c17235920878"
      },
      "outputs": [],
      "source": [
        "# Store the neural RDMs\n",
        "neural_rdms = {'Left': euclidean_distances(lh_fmri), 'Right': euclidean_distances(rh_fmri)}\n",
        "\n",
        "plot_neural_rdm(neural_rdms)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unfortunately, due to the size of the RDMs, clusters are difficult to spot. Let's zoom into a section to identify some clusters (feel free to play around with the zoom)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_neural_rdm(neural_rdms, zoom=[1200, 1500])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ToDo: Maybe write something regarding the spotted clusters?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2cksYtvWN-TX"
      },
      "source": [
        "### **Noise Ceiling**\n",
        "\n",
        "Let's evaluate what model performance we could expect, not only compared to other groups, but given the data and its associated signal and noise levels. To that extent, we decided to investigate the noise ceiling of our data, which gives us an indication of how much variance our model should preferably explain and the range of noise within the data. As subjects viewed different pictures for the majority of the experiment, averaging across their full neural RDMs is more difficult, especially as the number of trials differ. Therefore, below we extracted the images that were shared across subjects and calculated the noise ceiling based on the corresponding subset of neural observations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "hXvhWfINx4i8",
        "outputId": "a8675cf1-f0a5-425b-da29-9719a87d08e6"
      },
      "outputs": [],
      "source": [
        "subs = [f\"subj0{i}\" for i in range(1, 9)]\n",
        "neural_rdms_shared = neural_rdm_shared(subs = subs)\n",
        "\n",
        "# Calculate Noise Ceiling\n",
        "NCs = np.zeros((len(subs), 2))\n",
        "rdms_subjects = np.mean(neural_rdms_shared, axis=1)\n",
        "rdms_average_upp = np.mean(rdms_subjects, axis=0)\n",
        "rdms_average_low = [np.mean(rdms_subjects[np.arange(len(subs)) != i,:,:], axis=0) for i in range(len(subs))]\n",
        "\n",
        "# Computing the Individual Lower Noise Ceiling Bound\n",
        "NCs[:, 0] = [spearmanr(squareform(rdms_subjects[i,:,:]), squareform(rdms_average_low[i]))[0] for i in range(len(subs))]\n",
        "# Computing the Individual Upper Noise Ceiling Bound\n",
        "NCs[:, 1] = [spearmanr(squareform(rdms_subjects[i,:,:]), squareform(rdms_average_upp))[0] for i in range(len(subs))]\n",
        "# Compute Average Upper & Lower Noise Ceiling Bound \n",
        "mean_NC = np.mean(NCs, axis=0)\n",
        "\n",
        "print(f'The Noise Ceiling for the shared images has a Lower Bound of {mean_NC[0]:.2f} and an Upper Bound of {mean_NC[1]:.2f}')\n",
        "(noiseCeiling := pd.DataFrame({\"LowerBound\": [mean_NC[0]], \"UpperBound\": [mean_NC[1]]}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRVGT_-5IcLt"
      },
      "outputs": [],
      "source": [
        "# Plot Noise Ceiling\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot([0,1], [noiseCeiling['UpperBound'], noiseCeiling['UpperBound']], 'r--', label='Upper bound')\n",
        "plt.plot([0,1], [noiseCeiling['LowerBound'], noiseCeiling['LowerBound']], 'b--', label='Lower bound')\n",
        "plt.fill_between([0,1], noiseCeiling['UpperBound'], noiseCeiling['LowerBound'], color='grey', alpha=0.5)\n",
        "plt.fill_between([0,1], noiseCeiling['LowerBound'], 0, color='khaki', alpha=0.2)\n",
        "plt.ylabel('Spearman correlation')\n",
        "plt.ylim([0,1])\n",
        "plt.xticks([])\n",
        "plt.title('Noise ceiling')\n",
        "plt.legend()\n",
        "plt.margins(x=0.01) \n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The above Noise Ceiling indicates that there is a substantial amount of variance that can be explained and that the amount not reliably shared across subjects (i.e., noise) is rather thin/small. This finding gives us a good indication of what to expect when evaluating the later model fits."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1DAB8_Eu0fOF"
      },
      "source": [
        "### **Load the Stimulus Images**\n",
        "\n",
        "All images consist of natural scenes coming from the [COCO dataset][coco]. The images are divided into a training and a test split (corresponding to the fMRI training and test data splits). The amount of training and test images varies between subjects.\n",
        "\n",
        "[coco]: https://cocodataset.org/#home"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yALXfd9T06jI",
        "outputId": "28ac7c5e-a740-41de-fedf-062652ef05ce"
      },
      "outputs": [],
      "source": [
        "# Load image paths\n",
        "dirs.load_image_paths()\n",
        "train_img_list, test_img_list = dirs.train_img_list, dirs.test_img_list\n",
        "\n",
        "print(f\"Training images: {len(train_img_list)}\")\n",
        "print(f\"Test images: {len(test_img_list)}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6D3knf_NzRo8"
      },
      "source": [
        "### **Load the Stimulus Captions**\n",
        "\n",
        "All NSD images also have a caption associated with them that can be traced back to their origin in the [COCO dataset][coco] (visible above each image in the dataset). First, a matching between new NSD and old COCO IDs was retrieved, available in the [NSD AWS repository][nsd_exp] under `nsd_stim_info_merged.csv`. Next, JSON files containing the captions and corresponding IDs were downlaoded from the official COCO Website. Lastly, captions were matched based on old and new IDs. \n",
        "\n",
        "[coco]: https://cocodataset.org/#home\n",
        "[nsd_exp]: https://natural-scenes-dataset.s3.amazonaws.com/index.html#nsddata/experiments/nsd/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7cOKFl5zTCa",
        "outputId": "f880e04f-812c-442a-8fe7-3d3996fd0a69"
      },
      "outputs": [],
      "source": [
        "# Load captions\n",
        "dirs.load_captions()\n",
        "train_cap_list, test_cap_list = dirs.train_cap_list, dirs.test_cap_list\n",
        "max_caption_len = processor(text=train_cap_list + test_cap_list, return_tensors=\"pt\", padding=True)[\"input_ids\"].shape[1]\n",
        "\n",
        "print(f\"Training captions: {len(train_cap_list)}\")\n",
        "print(f\"Test captions: {len(test_cap_list)}\")\n",
        "print(f\"Max caption length: {max_caption_len}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bz78gab82nbX"
      },
      "source": [
        "### **Visualize the fMRI ROIs and Training Images**\n",
        "\n",
        "The visual cortex is divided into multiple areas having different functional properties, referred to as regions-of-interest (ROIs). Along with the fMRI data, ROI indices are provided for selecting vertices belonging to specific visual ROIs.\n",
        "\n",
        "Following is the list of ROIs (ROI class file names in parenthesis):\n",
        "* **Early retinotopic visual regions:** V1v, V1d, V2v, V2d, V3v, V3d, hV4.\n",
        "* **Body-selective regions:** EBA, FBA-1, FBA-2, mTL-bodies.\n",
        "* **Face-selective regions:** OFA, FFA-1, FFA-2, mTL-faces, aTL-faces.\n",
        "* **Place-selective regions:** OPA, PPA, RSC.\n",
        "* **Word-selective regions:** OWFA, VWFA-1, VWFA-2, mfs-words, mTL-words.\n",
        "* **Anatomical streams:** early, midventral, midlateral, midparietal, ventral, lateral, parietal.\n",
        "\n",
        "Next we plot the vertices belonging to specific fMRI ROIs. Additionally, we will show the displayed images (stimuli) and captions associated with these.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        },
        "id": "X_CiXiVZ2Ea_",
        "outputId": "dfcf3580-d3e2-483d-92c1-e6a577555a46"
      },
      "outputs": [],
      "source": [
        "img = 354 #@param\n",
        "hemisphere = \"left\" # ['left', 'right']\n",
        "roi = \"EBA\" # [\"all-vertices\", \"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]\n",
        "\n",
        "# Load the image\n",
        "img_dir = os.path.join(train_img_list[img])\n",
        "train_img = Image.open(img_dir).convert('RGB')\n",
        "\n",
        "# Plot the image\n",
        "plt.figure()\n",
        "plt.axis('off')\n",
        "plt.imshow(train_img)\n",
        "plt.title('Training image: ' + str(img+1) + '\\n' + train_cap_list[img]);\n",
        "\n",
        "fsaverage_roi_class, roi_map = dirs.load_roi_masks(roi, \"lh\" if hemisphere == \"left\" else \"rh\")\n",
        "if roi != \"all-vertices\":\n",
        "    dirs.load_challenge_rois()\n",
        "    challenge_roi_class = dirs.lh_challenge_rois if hemisphere == \"left\" else dirs.rh_challenge_rois\n",
        "\n",
        "    roi_mapping = list(roi_map.keys())[list(roi_map.values()).index(roi)]\n",
        "    fsaverage_roi = np.asarray(fsaverage_roi_class == roi_mapping, dtype=int)\n",
        "    challenge_roi = np.asarray(challenge_roi_class[roi_class_index(roi)] == roi_mapping, dtype=int)\n",
        "\n",
        "    # Map the fMRI data onto the brain surface map\n",
        "    fsaverage_response = np.zeros(len(fsaverage_roi))\n",
        "    if hemisphere == 'left':\n",
        "        fsaverage_response[np.where(fsaverage_roi)[0]] = lh_fmri[img,np.where(challenge_roi)[0]]\n",
        "    elif hemisphere == 'right':\n",
        "        fsaverage_response[np.where(fsaverage_roi)[0]] = rh_fmri[img,np.where(challenge_roi)[0]]\n",
        "\n",
        "else:\n",
        "    fsaverage_response = np.zeros(len(fsaverage_roi_class))\n",
        "    if hemisphere == 'left':\n",
        "        fsaverage_response[np.where(fsaverage_roi_class)[0]] = lh_fmri[img]\n",
        "    elif hemisphere == 'right':\n",
        "        fsaverage_response[np.where(fsaverage_roi_class)[0]] = rh_fmri[img]\n",
        "\n",
        "# # Create the interactive brain surface map\n",
        "fsaverage = datasets.fetch_surf_fsaverage('fsaverage')\n",
        "view = plotting.view_surf(\n",
        "    surf_mesh=fsaverage['infl_'+hemisphere],\n",
        "    surf_map=fsaverage_response,\n",
        "    bg_map=fsaverage['sulc_'+hemisphere],\n",
        "    threshold=1e-14,\n",
        "    cmap='cold_hot',\n",
        "    colorbar=True,\n",
        "    title=roi+', '+hemisphere+' hemisphere'\n",
        "    )\n",
        "view"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Methods: Training and Evaluating Linearizing Encoding Models**\n",
        "\n",
        "The below methods section serves as an illustraton of what we do in general. For the final submission/k-fold model fits we will use a predefined pipeline that does all of the data preparation and model fitting procedures automatically, which you can look up in more detail under [Helper Functions and Classes](#helper-functions-and-classes)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hftu175C4mv5"
      },
      "source": [
        "### **Model Approach and Data Preparation**\n",
        "\n",
        "We will build and evaluate a linearizing encoding models] using the [CLIP][clip] architecture pre-trained by OpenAI. We train the model on a training partition and cross-validate it on the validation partitions (an independent partition of the training data). The linearizing encoding algorithm involves the following general steps:\n",
        "\n",
        "1. Split the data into training, validation and test partitions.\n",
        "\n",
        "2. Extract image and/or text features from CLIP (Vision and/or Text model) providing either images and/or captions.\n",
        "\n",
        "3. Linearly map the CLIP image/text feature to fMRI responses. Usually, these features have a substantial amount of features, therefore, a PCA will be used for dimensionality reduction (when appropriate) \n",
        "\n",
        "4. Evaluate and visualize the encoding model's prediction accuracy (i.e., encoding accuracy) using the validation partition.\n",
        "\n",
        "First, we will split the data into training, validation and test partitions (by creating the corresponding indices). We split the training data into a training and validation partition, used respectively to train and evaluate our encoding models. The test partition corresponds to the test split of the data (of which we only have the images), and is only used to predict the fMRI responses.\n",
        "\n",
        "[clip]: https://openai.com/research/clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taIg_nF04nBt",
        "outputId": "ba11e0f0-140f-4c7a-86a1-0b9a425f30dc"
      },
      "outputs": [],
      "source": [
        "rand_seed = 5\n",
        "np.random.seed(rand_seed)\n",
        "\n",
        "# Calculate how many stimulus images correspond to 90% of the training data\n",
        "num_train = int(np.round(len(train_img_list)*0.9))\n",
        "# Shuffle all training stimulus images\n",
        "idxs = np.arange(len(train_img_list))\n",
        "np.random.shuffle(idxs)\n",
        "# Assign 90% of the shuffled stimulus images to the training partition,\n",
        "# and 10% to the test partition\n",
        "idxs_train, idxs_val = idxs[:num_train], idxs[num_train:]\n",
        "# No need to shuffle or split the test stimulus images\n",
        "idxs_test = np.arange(len(test_img_list))\n",
        "\n",
        "print('Training stimulus images: ' + format(len(idxs_train)))\n",
        "print('Validation stimulus images: ' + format(len(idxs_val)))\n",
        "print('Test stimulus images: ' + format(len(idxs_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rP_EU2XfA3EA"
      },
      "source": [
        "Let's split the training fMRI data into training and validation partitions using the previously defined indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7uy4O6PAzaw"
      },
      "outputs": [],
      "source": [
        "# Split neural data into train and validation\n",
        "lh_fmri_train = lh_fmri[idxs_train]\n",
        "lh_fmri_val = lh_fmri[idxs_val]\n",
        "rh_fmri_train = rh_fmri[idxs_train]\n",
        "rh_fmri_val = rh_fmri[idxs_val]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to compare the neural RDMs to later feature/layer RDMs from our model activations, we will store the lower triangle of both train neural rdms below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "neural_rdms_train = [euclidean_distances(lh_fmri_train), euclidean_distances(rh_fmri_train)]\n",
        "neural_rdvs_train = [squareform(n_rdms.round(5)) for n_rdms in neural_rdms_train]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's delete the original neural RDMs to save memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzhfwQzbRUVH",
        "outputId": "2a01d511-3763-49c3-f069-e9477565fa86"
      },
      "outputs": [],
      "source": [
        "del lh_fmri, rh_fmri"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rDHn6oqK-qNd"
      },
      "source": [
        "As we are working with a [PyTorch instantiation of CLIP][coco_pt] we are using the corresponding PyTorch `Dataset` and `DataLoader` classes to create our three data partitions.\n",
        "\n",
        "[coco_pt]: [https://huggingface.co/docs/transformers/model_doc/clip]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKOPEaNcalpr",
        "outputId": "510a3657-f344-4e11-971e-d9500ead25ac"
      },
      "outputs": [],
      "source": [
        "batch_size = 400\n",
        "\n",
        "# Prepare data for dataloader \n",
        "train_img_dataset = ImageDataset(list(np.array(train_img_list)[idxs_train]), processor)\n",
        "val_img_dataset = ImageDataset(list(np.array(train_img_list)[idxs_val]), processor)\n",
        "test_img_dataset = ImageDataset(test_img_list, processor)\n",
        "\n",
        "train_img_dataloader = DataLoader(train_img_dataset, batch_size=batch_size, shuffle=False)\n",
        "val_img_dataloader = DataLoader(val_img_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_img_dataloader = DataLoader(test_img_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Train caption dataloader: {len(train_img_dataloader)} batches\")\n",
        "print(f\"Validation caption dataloader: {len(val_img_dataloader)} batches\")\n",
        "print(f\"Test caption dataloader: {len(test_img_dataloader)} batches\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "el3-N-CZOp0i"
      },
      "source": [
        "### **Running the model**\n",
        "\n",
        "For illustrative purposes, we will execute an example run of our approach, involving CLIPs Vision model and a subselection of feature layers we found interesting when running more elaborate k-fold validations. By splitting up the entire procedure, we will guide you through our reasoning regarding model fitting decisions and showcase the utilized model evaluation steps."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a first step, we will extract raw image features for the tranformer layers 4, 7, 11, 12, and 13 (final layer). The [CLIPFeatureExtractor](#clipfeatureextractor-class) class takes the required arguments and returns the desired feature spaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMYQSS4VPday",
        "outputId": "b2ad5148-5823-4624-c88d-e70aabdfef10"
      },
      "outputs": [],
      "source": [
        "raw_features = []\n",
        "for loader in [train_img_dataloader, val_img_dataloader, test_img_dataloader]:\n",
        "    feature_extractor = CLIPFeatureExtractor(idxs=[4, 7, 11, 12], last_hidden_layer=True, model=vis_model, dataloader=loader)\n",
        "    feature_extractor.extract_raw_features_from_model()\n",
        "    raw_features.append(feature_extractor.feature_dict)\n",
        "\n",
        "del feature_extractor"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GGk4dlFct86j"
      },
      "source": [
        "Before we go into any fitting procedure, let's check out the RDMs of the visual features we extracted per layer and their correlations with the neural rdms stored above for the training neural observations. This gives us an indication of what layers might be more interesting to investigate in the model fitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "qf5GHzkqgOll",
        "outputId": "fb0aaf5f-81b6-4dd1-a02d-d19f81de8fb2"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, len(raw_features[0]), figsize=(20, 20))\n",
        "for i, layer in enumerate(raw_features[0].keys()):\n",
        "    train_img_pca_features = torch.tensor(raw_features[0][layer]).flatten(1).numpy()\n",
        "    layer_rdm = euclidean_distances(train_img_pca_features)\n",
        "    layer_rdv = squareform(layer_rdm.round(5))\n",
        "    rdm_corrs = [spearmanr(n_rdv, layer_rdv)[0] for n_rdv in neural_rdvs_train]\n",
        "\n",
        "    axs[i].set_title(f\"Visual {layer}\")\n",
        "    axs[i].set_xlabel(f\"Correlation wit neural RDM: {np.mean(rdm_corrs):.2f}\")\n",
        "    axs[i].imshow(layer_rdm)\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The above feature RDMs show an interesting trend: The higher the layers the closer are the computed pairwise distances. Notably, transformer layer 7 seems to show the highest correlations with the neural RDMS, making it a good candidate for the following fitting process. Although this layer shows the highest correlation, it is still far from what could potentially be explaned in the data given the above [Noise Ceiling](#noise-ceiling). \n",
        "\n",
        "Keep in mind when looking a the above correlations and later results:\n",
        "* Our overall goal is to predict the fMRI activity in the hemispheres and correlate the prediction with the actual fMRI pattern (on the validation split)\n",
        "* The above, however, is a direct correlation between layer and neural activations!\n",
        "\n",
        "However, one problem remains before we move on to the modelling: The dimensionality of our current feature space is too high, potentially resulting in overfitting the training data, model assumption violations, and computational bottlenecks (as already noticable when executing the plotting on the single feature RDMs). Therefore, we define a PCA to reduce the dimensionality of our preferred layer in order to fit a reasonable amount of predictors for our model (linear regression). Additionally, let's check visually how the variance is distributed across the PCA components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "BA0QrDLeXGvl",
        "outputId": "783f2ba9-4f52-4df0-f374-ec25d69b605c"
      },
      "outputs": [],
      "source": [
        "# PCA Components\n",
        "pca_components = 1000\n",
        "pca = PCA(n_components=pca_components)\n",
        "\n",
        "example_pca = pca.fit_transform(torch.tensor(raw_features[0]['Transformer Layer 7']).flatten(1).numpy())\n",
        "\n",
        "# Evaluate how the variance is distributed across the PCA components\n",
        "plt.plot(np.arange(pca_components), pca.explained_variance_ratio_)\n",
        "plt.xlabel('Component')\n",
        "plt.ylabel('Variance explained')\n",
        "plt.title('Total variance explained: ' + str(np.sum(pca.explained_variance_ratio_)));"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vWa8eRLoPySJ"
      },
      "source": [
        "As we can see above, we are by no means capturing the entire variation within our image features (transformer layer 7), however, 1000 PCA components seem not worth the small additional variance they capture. Therefore, we will use 200 PCA components for our subsequent linear models as, judging by the plot, they capture a major part of the variation already. Let's fit a PCA and transform the training image features accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAiFjZtTPyCT"
      },
      "outputs": [],
      "source": [
        "pca_components = 200\n",
        "pca = PCA(n_components=pca_components)\n",
        "\n",
        "train_img_pca_features = pca.fit_transform(torch.tensor(raw_features[0]['Transformer Layer 7']).flatten(1).numpy())\n",
        "\n",
        "# Fit linear regressions on the training data\n",
        "img_reg_lh = LinearRegression().fit(train_img_pca_features, lh_fmri_train)\n",
        "img_reg_rh = LinearRegression().fit(train_img_pca_features, rh_fmri_train)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sOfTzYBueysx"
      },
      "source": [
        "Next, we transform the validation image features according to the fitted PCA and use these transformed features to predict the left and right hemisphere fMRI activity, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWYs0guTeM-H"
      },
      "outputs": [],
      "source": [
        "val_img_pca_features = pca.transform(torch.tensor(raw_features[1]['Transformer Layer 7']).flatten(1).numpy())\n",
        "\n",
        "# Use fitted linear regressions to predict the validation fMRI data\n",
        "lh_fmri_val_pred = img_reg_lh.predict(val_img_pca_features)\n",
        "rh_fmri_val_pred = img_reg_rh.predict(val_img_pca_features)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FtP8UIA0g0hF"
      },
      "source": [
        "### **Model Performance and Statistical Evaluation**\n",
        "\n",
        "Now that we fitted a model and predicted the outcome variable of interest, let's evaluate how good the chosen model feature space and modelling approach are. To that extent, we will compute the correlations between predicted and actual fmri activity for the validation set (for both, left and right hemisphere)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDTd4GPLg47U",
        "outputId": "de63f594-9edb-4ea8-a7b3-26d325c349c0"
      },
      "outputs": [],
      "source": [
        "# Empty correlation array of shape: (LH vertices)\n",
        "lh_correlation = np.zeros(lh_fmri_val_pred.shape[1])\n",
        "# Correlate each predicted LH vertex with the corresponding ground truth vertex\n",
        "for v in tqdm(range(lh_fmri_val_pred.shape[1])):\n",
        "    lh_correlation[v] = corr(lh_fmri_val_pred[:,v], lh_fmri_val[:,v])[0]\n",
        "\n",
        "# Empty correlation array of shape: (RH vertices)\n",
        "rh_correlation = np.zeros(rh_fmri_val_pred.shape[1])\n",
        "# Correlate each predicted RH vertex with the corresponding ground truth vertex\n",
        "for v in tqdm(range(rh_fmri_val_pred.shape[1])):\n",
        "    rh_correlation[v] = corr(rh_fmri_val_pred[:,v], rh_fmri_val[:,v])[0]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A nice way to illustrate the resulting correlations is by visualizing the corresponding encoding accuracy of all vertices on a brain surface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "BtnqvfqwhZxQ",
        "outputId": "51865ade-768e-4c98-a3e9-b2bb7df609a1"
      },
      "outputs": [],
      "source": [
        "hemisphere = 'left' # ['left', 'right']\n",
        "\n",
        "# Load the brain surface map of all vertices\n",
        "fsaverage_all_vertices, _ = dirs.load_roi_masks(\"all-vertices\", \"lh\" if hemisphere == \"left\" else \"rh\")\n",
        "\n",
        "# Map the correlation results onto the brain surface map\n",
        "fsaverage_correlation = np.zeros(len(fsaverage_all_vertices))\n",
        "if hemisphere == 'left':\n",
        "    fsaverage_correlation[np.where(fsaverage_all_vertices)[0]] = lh_correlation\n",
        "elif hemisphere == 'right':\n",
        "    fsaverage_correlation[np.where(fsaverage_all_vertices)[0]] = rh_correlation\n",
        "\n",
        "# Create the interactive brain surface map\n",
        "fsaverage = datasets.fetch_surf_fsaverage('fsaverage')\n",
        "view = plotting.view_surf(\n",
        "    surf_mesh=fsaverage['infl_'+hemisphere],\n",
        "    surf_map=fsaverage_correlation,\n",
        "    bg_map=fsaverage['sulc_'+hemisphere],\n",
        "    threshold=1e-14,\n",
        "    cmap='cold_hot',\n",
        "    colorbar=True,\n",
        "    title='Encoding accuracy, '+hemisphere+' hemisphere'\n",
        "    )\n",
        "view"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, let's plot the achieved median correlation for each ROI in an overall bar plot, split into left and right hemisphere."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        },
        "id": "Eip5-ZuLhxRa",
        "outputId": "0f88043e-943f-4a93-f136-dd094c965677"
      },
      "outputs": [],
      "source": [
        "dirs.load_challenge_rois()\n",
        "\n",
        "# Load the ROI classes mapping dictionaries\n",
        "roi_name_maps = dirs.roi_name_maps\n",
        "\n",
        "# Load the ROI brain surface maps\n",
        "lh_challenge_rois, rh_challenge_rois = dirs.lh_challenge_rois, dirs.rh_challenge_rois\n",
        "\n",
        "# Select the correlation results vertices of each ROI\n",
        "roi_names = []\n",
        "lh_roi_correlation = []\n",
        "rh_roi_correlation = []\n",
        "for r1 in range(len(lh_challenge_rois)):\n",
        "    for r2 in roi_name_maps[r1].items():\n",
        "        if r2[0] != 0: # zeros indicate to vertices falling outside the ROI of interest\n",
        "            roi_names.append(r2[1])\n",
        "            lh_roi_idx = np.where(lh_challenge_rois[r1] == r2[0])[0]\n",
        "            rh_roi_idx = np.where(rh_challenge_rois[r1] == r2[0])[0]\n",
        "            lh_roi_correlation.append(lh_correlation[lh_roi_idx])\n",
        "            rh_roi_correlation.append(rh_correlation[rh_roi_idx])\n",
        "roi_names.append('All vertices')\n",
        "lh_roi_correlation.append(lh_correlation)\n",
        "rh_roi_correlation.append(rh_correlation)\n",
        "\n",
        "# Create the plot\n",
        "lh_median_roi_correlation = [np.median(lh_roi_correlation[r]) for r in range(len(lh_roi_correlation))]\n",
        "rh_median_roi_correlation = [np.median(rh_roi_correlation[r]) for r in range(len(rh_roi_correlation))]\n",
        "plt.figure(figsize=(18,6))\n",
        "x = np.arange(len(roi_names))\n",
        "width = 0.30\n",
        "plt.bar(x - width/2, lh_median_roi_correlation, width, label='Left Hemisphere')\n",
        "plt.bar(x + width/2, rh_median_roi_correlation, width,\n",
        "    label='Right Hemishpere')\n",
        "plt.xlim(left=min(x)-.5, right=max(x)+.5)\n",
        "plt.ylim(bottom=0, top=1)\n",
        "plt.xlabel('ROIs')\n",
        "plt.xticks(ticks=x, labels=roi_names, rotation=60)\n",
        "plt.ylabel('Median Pearson\\'s $r$')\n",
        "plt.legend(frameon=True, loc=1)\n",
        "plt.title(f\"Model: CLIP Visual {list(raw_features[0].keys())[1]}\");"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ToDo: Write something about findings: CLIP Visual is good blablabla\n",
        "I wrote this without actually seeing the plot, but from my memory of the meeting, so might be good to check. also add on it based on image?\n",
        "\n",
        "The image shows that CLIP vision model has high correlation for early vision layers ROIs. This is intuative, as the early visual brain areas are important for visual processing of images. That is closely related to what our model tries to do: predicting the brain activation in response to image stimuli. \n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **k-Fold Cross-Validation**\n",
        "\n",
        "The above execution of code is just a visual display of what is going under the hood. In order to properly evaluate the model, we need to run a k-fold cross-validation procedure. To that extent we wrote custom classes to run this procedure. Depending on the selected k-fold procedure, different requirements like creating a subject or feature extractor are needed. We ran multiple 8-fold cross validation procedures on the vision only, text only and combined model (all on data of subject 1).\n",
        "\n",
        "All k-Folds were run with seed number 5."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **CLIP Vision Model, PCA200, Linear Regression** <a id='cv_vis_pca200_linreg'></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We ran 8 folds on image data of subject 1. For each iteration the current fold was selected as the validation set and the remaining folds were used as the training set.\n",
        "\n",
        "The validation procedure for each fold was as follows: \n",
        "\n",
        "- Supplying the training set to a CLIPVisionModel.\n",
        "- Extracting the raw features for all layers (Embedding, Transformer 1 - 12, Final Layer).\n",
        "- Looping over folds:\n",
        "    - Assing training and validation sets for images and fmri activity.\n",
        "    - Looping over each layer:\n",
        "        - Fitting a PCA with 200 components using the raw training features.\n",
        "        - Transforming the raw training features using the fitted PCA.\n",
        "        - Fitting a linear regression, predicting training fmri activity from the pca transformed training features. (for left and right hemisphere)\n",
        "        - Transforming the raw validation features using the fitted PCA.\n",
        "        - Using the fitted linear models to predict validation fmri activity from the pca transformed validation features. (for left and right hemisphere)\n",
        "        - Calculating correlations between predicted and actual fmri activity for the validation set. (for left and right hemisphere)\n",
        "        - Calculating median correlations for each of the 36 challenge ROI. (for left and right hemisphere)\n",
        "    - Storing the median correlations for each hemisphere and each layer for the current fold.\n",
        "- Calculating mean median correlations accross all folds for each layer. (for left and right hemisphere)\n",
        "- Plotting the mean median correlations for each layer for each ROI and select the layer with the highest mean median correlation averaged over all ROIs.\n",
        "\n",
        "Transformer layer 7 achieved the highest overall performance (left hemisphere = 0.472, right hemisphere = 0.484) and was selected for submission (submission score = 49.3178).\n",
        "    \n",
        "Interestingly the plot below suggests, that earlier layers are better at predicting the activity of the earlier visual cortex (V1, V2, V3, V4) and later layers are better at predicting the activity of the other ROIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_kfold_result(\"001\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **KFoldProcedure Class** <a id='kfpc_single_clip_single_subject'></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KFoldSingleCLIPSingleSubject(KFoldProcedure):\n",
        "    \"\"\"A procedure that runs k-fold on all layers of a single CLIP model on a single subject.\"\"\"\n",
        "    def __init__(self, \n",
        "                 feature_extractor: CLIPFeatureExtractor,\n",
        "                 subject: Subject, \n",
        "                 pca: PCA,\n",
        "                 model_name: str = None,\n",
        "                 description: str = None) -> None:\n",
        "        assert isinstance(feature_extractor, CLIPFeatureExtractor), \"feature_extractor must be an instance of CLIPFeatureExtractor\"\n",
        "        assert isinstance(subject, Subject), \"subject must be an instance of Subject\"\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.subject = subject\n",
        "        self.pca = pca\n",
        "        self.model_name = model_name\n",
        "        self.description = description\n",
        "        self.correlations = {}\n",
        "        super().__init__()\n",
        "\n",
        "    def prepare(self):\n",
        "        # Extract raw features\n",
        "        self.feature_extractor.extract_raw_features_from_model()\n",
        "        self.raw_features = self.feature_extractor.feature_dict\n",
        "        self.feature_extractor = None # free memory\n",
        "\n",
        "        # Load challenge rois\n",
        "        self.subject.load_challenge_rois()\n",
        "        self.lh_challenge_rois = self.subject.lh_challenge_rois\n",
        "        self.rh_challenge_rois = self.subject.rh_challenge_rois\n",
        "        self.roi_name_maps = self.subject.roi_name_maps\n",
        "\n",
        "        # Load neural data\n",
        "        self.subject.load_neural_data()\n",
        "        self.lh_fmri = self.subject.lh_fmri\n",
        "        self.rh_fmri = self.subject.rh_fmri\n",
        "        self.subject = None # free memory\n",
        "\n",
        "        # Prepare correlation dict\n",
        "        self.fold_correlations = {}\n",
        "\n",
        "    def run(self, train_idxs: np.ndarray, val_idxs: np.ndarray) -> Dict[str, Dict[str, np.ndarray]]:\n",
        "        # Loop over all layers          \n",
        "        correlations = {}\n",
        "        for layer in self.raw_features.keys():\n",
        "            print(f\"> {layer}\")\n",
        "            # Assign train and val features\n",
        "            train_features = self.raw_features[layer][train_idxs]\n",
        "            val_features = self.raw_features[layer][val_idxs]\n",
        "\n",
        "            # Assign train and val fmri\n",
        "            train_lh_fmri = self.lh_fmri[train_idxs]\n",
        "            train_rh_fmri = self.rh_fmri[train_idxs]\n",
        "            val_lh_fmri = self.lh_fmri[val_idxs]\n",
        "            val_rh_fmri = self.rh_fmri[val_idxs]\n",
        "\n",
        "            # Fit PCA models\n",
        "            print(f\"Fitting PCA model for {layer}...\")\n",
        "            train_pca_features = self.pca.fit_transform(torch.tensor(train_features).flatten(1).numpy())\n",
        "            del train_features # free memory\n",
        "\n",
        "            # Fit linear regression\n",
        "            print(f\"Fitting linear regression models for {layer}...\")\n",
        "            lh_lin_reg = LinearRegression().fit(train_pca_features, train_lh_fmri)\n",
        "            rh_lin_reg = LinearRegression().fit(train_pca_features, train_rh_fmri)\n",
        "            del train_pca_features, train_lh_fmri, train_rh_fmri # free memory\n",
        "\n",
        "            # Transform validation features\n",
        "            print(f\"Transforming validation features for {layer}...\")\n",
        "            val_txt_pca_features = self.pca.transform(torch.tensor(val_features).flatten(1).numpy())\n",
        "            del val_features # free memory\n",
        "\n",
        "            # Predict validation set\n",
        "            print(f\"Predicting validation set for {layer}...\")\n",
        "            lh_val_pred = lh_lin_reg.predict(val_txt_pca_features)\n",
        "            rh_val_pred = rh_lin_reg.predict(val_txt_pca_features)\n",
        "            del val_txt_pca_features, lh_lin_reg, rh_lin_reg # free memory\n",
        "            \n",
        "            # Calculate correlations\n",
        "            print(f\"Calculating correlations for {layer}...\\n\")\n",
        "            lh_correlation, rh_correlation = self.calculate_correlations(lh_val_pred, rh_val_pred, val_lh_fmri, val_rh_fmri)\n",
        "            lh_median_roi_correlation, rh_median_roi_correlation = self.calculate_median_correlations(lh_correlation, rh_correlation)\n",
        "            \n",
        "            # Store correlations\n",
        "            correlations[layer] = {\"lh\": lh_median_roi_correlation, \"rh\": rh_median_roi_correlation} \n",
        "        return correlations\n",
        "\n",
        "    def return_idxs(self) -> np.ndarray:\n",
        "        return np.arange(len(self.raw_features[list(self.raw_features.keys())[0]])) \n",
        "    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **Executing the k-Fold Procedure**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Raise exception to prevent accidental execution\n",
        "raise Exception(\"This code is not meant to be run. It is only meant to be used as a reference for the code used to generate the submission.\")\n",
        "\n",
        "# Initializing all required objects\n",
        "subject = Subject(\"subj01\")\n",
        "subject.create_dataloaders(processor=processor, batch_size=300)\n",
        "feature_extractor = CLIPFeatureExtractor(\n",
        "    idxs=[0,1,2,3,4,5,6,7,8,9,10,11,12], \n",
        "    last_hidden_layer=True, \n",
        "    model=vis_model, # here we use the vis_model\n",
        "    dataloader=subject.train_img_dataloader) # and the img_dataloader\n",
        "\n",
        "# Initialize the kfold procedure\n",
        "kfold_procedure = KFoldSingleCLIPSingleSubject(\n",
        "    feature_extractor=feature_extractor,\n",
        "    subject=subject,\n",
        "    pca=PCA(n_components=200),\n",
        "    model_name=\"CLIP Vision\",\n",
        "    description=\"CLIP Vision Model, all layers, PCA 200, Single layer linear regression, 8-fold cross validation\"\n",
        ")\n",
        "\n",
        "# Run KFold\n",
        "KFold(folds=8, seed=5, procedure=kfold_procedure).run()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **Additional Data Analysis**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because The cross validation plot indicated that earlier layers are better at predicting earlier ROIs, we checked which layer would have the highest mean median correlation averaged over early ROI (V1, V2, V3) and which layer has the highest mean median correlation averaged over all other ROI. \n",
        "\n",
        "The results indicate that Transformer Layer 4 is best at predicting activity in the early visual cortex, while layer 8 is the best layer of the rest of the cortex. \n",
        "Because of these findings we decided to cross validate another model against our strongest model so far. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "validation_folder = \"validations\"\n",
        "validation = \"001\"\n",
        "\n",
        "df = pd.read_csv(f\"{validation_folder}/validation{validation}/results.csv\")\n",
        "df = df.drop(columns=\"model\")\n",
        "\n",
        "early_roi = [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\"]\n",
        "best_early_lh = df[(df.roi.isin(early_roi)) & (df.hemisphere == \"lh\")].groupby([\"layer\"]).mean([\"correlations\"]).sort_values(by='correlation', ascending=False).round(3).head(1).index.values[0]\n",
        "best_early_lh_cor = df[(df.roi.isin(early_roi)) & (df.hemisphere == \"lh\")].groupby([\"layer\"]).mean([\"correlations\"]).sort_values(by='correlation', ascending=False).round(3).head(1).values[0][0]\n",
        "best_early_rh = df[(df.roi.isin(early_roi)) & (df.hemisphere == \"rh\")].groupby([\"layer\"]).mean([\"correlations\"]).sort_values(by='correlation', ascending=False).round(3).head(1).index.values[0]\n",
        "best_early_rh_cor = df[(df.roi.isin(early_roi)) & (df.hemisphere == \"rh\")].groupby([\"layer\"]).mean([\"correlations\"]).sort_values(by='correlation', ascending=False).round(3).head(1).values[0][0]\n",
        "best_late_lh = df[~(df.roi.isin(early_roi)) & (df.hemisphere == \"lh\")].groupby([\"layer\"]).mean([\"correlations\"]).sort_values(by='correlation', ascending=False).round(3).head(1).index.values[0]\n",
        "best_late_lh_cor = df[~(df.roi.isin(early_roi)) & (df.hemisphere == \"lh\")].groupby([\"layer\"]).mean([\"correlations\"]).sort_values(by='correlation', ascending=False).round(3).head(1).values[0][0]\n",
        "best_late_rh = df[~(df.roi.isin(early_roi)) & (df.hemisphere == \"rh\")].groupby([\"layer\"]).mean([\"correlations\"]).sort_values(by='correlation', ascending=False).round(3).head(1).index.values[0]\n",
        "best_late_rh_cor = df[~(df.roi.isin(early_roi)) & (df.hemisphere == \"rh\")].groupby([\"layer\"]).mean([\"correlations\"]).sort_values(by='correlation', ascending=False).round(3).head(1).values[0][0]\n",
        "print(f\"The best layer for early ROI on the left hemisphere is\\t\\t==>\\t{best_early_lh} ({best_early_lh_cor})\")\n",
        "print(f\"The best layer for early ROI on the right hemisphere is\\t\\t==>\\t{best_early_rh} ({best_early_rh_cor})\")\n",
        "print(f\"The best layer for later ROI on the left hemisphere is\\t\\t==>\\t{best_late_lh} ({best_late_lh_cor})\")\n",
        "print(f\"The best layer for later ROI on the right hemisphere is\\t\\t==>\\t{best_late_rh} ({best_late_rh_cor})\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **CLIP Text Model, PCA200, Linear Regression** <a id='cv_txt_pca200_linreg'></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We ran 8 folds on caption data of subject 1. For each iteration the current fold was selected as the validation set and the remaining folds were used as the training set.\n",
        "\n",
        "The validation procedure for each fold was as follows: \n",
        "\n",
        "- Supplying the training set to a CLIPTextModel.\n",
        "- Extracting the raw features for all layers (Embedding, Transformer 1 - 12, Final Layer).\n",
        "- Looping over folds:\n",
        "    - Assing training and validation sets for coco captions and fmri activity.\n",
        "    - Looping over each layer:\n",
        "        - Fitting a PCA with 200 components using the raw training features.\n",
        "        - Transforming the raw training features using the fitted PCA.\n",
        "        - Fitting a linear regression, predicting training fmri activity from the pca transformed training features. (for left and right hemisphere)\n",
        "        - Transforming the raw validation features using the fitted PCA.\n",
        "        - Using the fitted linear models to predict validation fmri activity from the pca transformed validation features. (for left and right hemisphere)\n",
        "        - Calculating correlations between predicted and actual fmri activity for the validation set. (for left and right hemisphere)\n",
        "        - Calculating median correlations for each of the 36 challenge ROI. (for left and right hemisphere)\n",
        "    - Storing the median correlations for each hemisphere and each layer for the current fold.\n",
        "- Calculating mean median correlations accross all folds for each layer. (for left and right hemisphere)\n",
        "- Plotting the mean median correlations for each layer for each ROI and select the layer with the highest mean median correlation averaged over all ROIs.\n",
        "\n",
        "Transformer layer 11 achieved the highest overall performance (left hemisphere = 0.293, right hemisphere = 0.306) but was much worse than transformer layer 7 of the vision only model (left hemisphere = 0.472, right hemisphere = 0.484), therefore we did not submit this.\n",
        "\n",
        "As opposed to the vision only model, the plot below indicates that later levels of the text only model are better to predict activity in all ROIs. Additionally, the text only model performed much worse on the early visual cortex (V1, V2, V3, V4) than the vision only model but performed comparably for the later layers.\n",
        "\n",
        "Note: The **KFoldProcedure Class** is the [same as for the Vision Model](#kfpc_single_clip_single_subject)!\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_kfold_result(\"002\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **Executing the k-Fold Procedure**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Raise exception to prevent accidental execution\n",
        "raise Exception(\"This code is not meant to be run. It is only meant to be used as a reference for the code used to generate the submission.\")\n",
        "\n",
        "# Initializing all required objects\n",
        "subject = Subject(\"subj01\")\n",
        "subject.create_dataloaders(processor=processor, batch_size=300)\n",
        "feature_extractor = CLIPFeatureExtractor(\n",
        "    idxs=[0,1,2,3,4,5,6,7,8,9,10,11,12], \n",
        "    last_hidden_layer=True, \n",
        "    model=txt_model, # here we use the txt_model \n",
        "    dataloader=subject.train_txt_dataloader) # and the txt_dataloader\n",
        "\n",
        "# Initialize the kfold procedure\n",
        "kfold_procedure = KFoldSingleCLIPSingleSubject(\n",
        "    feature_extractor=feature_extractor,\n",
        "    subject=subject,\n",
        "    pca=PCA(n_components=200),\n",
        "    model_name=\"CLIP Text\",\n",
        "    description=\"CLIP Text Model, all layers, PCA 200, Single layer linear regression, 8-fold cross validation\"\n",
        ")\n",
        "\n",
        "# Run KFold\n",
        "KFold(folds=8, seed=5, procedure=kfold_procedure).run()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **CLIP Text + Vision Model, PCA200, Linear Regression** <a id='cv_txtvis_pca200_linreg'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We ran 8 folds on caption and image data of subject 1. For each iteration the current fold was selected as the validation set and the remaining folds were used as the training set.\n",
        "Because of RAM limitations we ran the cross validation for the combined model in three batches (Embedding Layer to Transformer Layer 4, Transformer Layers 5 to 9, Transformer Layer 10 to Final Layer). Afterwards we manually combined the csv files that contained the results to provide one plot including all layers.\n",
        "\n",
        "The validation procedure for each fold was as follows: \n",
        "\n",
        "- Supplying the caption training set to a CLIPTextModel and the image training set to a CLIPVisionModel.\n",
        "- Extracting the raw features for all layers (Embedding, Transformer 1 - 12, Final Layer). (for text and images)\n",
        "- Looping over folds:\n",
        "    - Assing training and validation sets for images, coco captions and fmri activity.\n",
        "    - Looping over each layer:\n",
        "        - Fitting a PCA with 200 components using the raw training features. (for text and images)\n",
        "        - Transforming the raw training features using the fitted PCA. (for text and images)\n",
        "        - Fitting a linear regression, predicting training fmri activity from the combination of pca transformed image and caption training features. (for left and right hemisphere)\n",
        "        - Transforming the raw validation features using the fitted PCA. (for text and images)\n",
        "        - Using the fitted linear models to predict validation fmri activity from the combination of pca transformed image and caption validation features. (for left and right hemisphere)\n",
        "        - Calculating correlations between predicted and actual fmri activity for the validation set. (for left and right hemisphere)\n",
        "        - Calculating median correlations for each of the 36 challenge ROI. (for left and right hemisphere)\n",
        "    - Storing the median correlations for each hemisphere and each layer for the current fold.\n",
        "- Calculating mean median correlations accross all folds for each layer. (for left and right hemisphere)\n",
        "- Plotting the mean median correlations for each layer for each ROI and select the layer with the highest mean median correlation averaged over all ROIs.\n",
        "\n",
        "Similar to the vision only model, transformer layer 7 achieved the highest overall performance (left hemisphere = 0.468, right hemisphere = 0.48) and was selected for submission (submission score = NA).\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_kfold_result(\"006\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **KFoldProcedure Class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KFoldCombinedCLIPSingleSubject(KFoldProcedure):\n",
        "    \"\"\"A procedure that runs k-fold on all layers of a combination of the text and vision CLIP model on a single subject.\"\"\"\n",
        "    def __init__(self, \n",
        "                 subject: Subject, \n",
        "                 txt_pca: PCA,\n",
        "                 img_pca: PCA,\n",
        "                 model_name: str = None,\n",
        "                 description: str = None,\n",
        "                 layers: list = [],\n",
        "                 last_hidden_state = False) -> None:\n",
        "        self.subject = subject\n",
        "        self.txt_pca = txt_pca\n",
        "        self.img_pca = img_pca\n",
        "        self.model_name = model_name\n",
        "        self.description = description\n",
        "        self.layers = layers.copy()\n",
        "        self.layers2 = layers.copy()\n",
        "        self.last_hidden_state = last_hidden_state\n",
        "        self.correlations = {}\n",
        "        super().__init__()\n",
        "\n",
        "    def prepare(self):\n",
        "        # Prepare data\n",
        "        self.subject.create_dataloaders(processor, batch_size=400)\n",
        "        train_img_dataloader = self.subject.train_img_dataloader\n",
        "        train_txt_dataloader = self.subject.train_txt_dataloader\n",
        "\n",
        "        # Extract raw features from text model\n",
        "        feature_extractor = CLIPFeatureExtractor(idxs=self.layers, last_hidden_layer=self.last_hidden_state, model=txt_model, dataloader=train_txt_dataloader)\n",
        "        feature_extractor.extract_raw_features_from_model()\n",
        "        self.raw_txt_features = feature_extractor.feature_dict\n",
        "        del feature_extractor # free memory\n",
        "\n",
        "        # Extract raw features from vision model\n",
        "        feature_extractor = CLIPFeatureExtractor(idxs=self.layers2, last_hidden_layer=self.last_hidden_state, model=vis_model, dataloader=train_img_dataloader)\n",
        "        feature_extractor.extract_raw_features_from_model()\n",
        "        self.raw_img_features = feature_extractor.feature_dict\n",
        "        del feature_extractor # free memory\n",
        "\n",
        "        # Load challenge rois\n",
        "        self.subject.load_challenge_rois()\n",
        "        self.lh_challenge_rois = self.subject.lh_challenge_rois\n",
        "        self.rh_challenge_rois = self.subject.rh_challenge_rois\n",
        "        self.roi_name_maps = self.subject.roi_name_maps\n",
        "\n",
        "        # Load neural data\n",
        "        self.subject.load_neural_data()\n",
        "        self.lh_fmri = self.subject.lh_fmri\n",
        "        self.rh_fmri = self.subject.rh_fmri\n",
        "        self.subject = None # free memory\n",
        "\n",
        "        # Prepare correlation dict\n",
        "        self.fold_correlations = {}\n",
        "\n",
        "    def run(self, train_idxs: np.ndarray, val_idxs: np.ndarray) -> Dict[str, Dict[str, np.ndarray]]:\n",
        "        # Loop over all layers          \n",
        "        correlations = {}\n",
        "        for layer in self.raw_txt_features.keys():\n",
        "            print(f\"> {layer}\")\n",
        "            # Assign train and val text features\n",
        "            train_txt_features = self.raw_txt_features[layer][train_idxs]\n",
        "            val_txt_features = self.raw_txt_features[layer][val_idxs]\n",
        "            # Assign train and val vision features\n",
        "            train_img_features = self.raw_img_features[layer][train_idxs]\n",
        "            val_img_features = self.raw_img_features[layer][val_idxs]\n",
        "\n",
        "            # Assign train and val fmri\n",
        "            train_lh_fmri = self.lh_fmri[train_idxs]\n",
        "            train_rh_fmri = self.rh_fmri[train_idxs]\n",
        "            val_lh_fmri = self.lh_fmri[val_idxs]\n",
        "            val_rh_fmri = self.rh_fmri[val_idxs]\n",
        "\n",
        "            # Fit PCA models\n",
        "            print(f\"Fitting PCA model for {layer}...\")\n",
        "            train_txt_pca_features = self.txt_pca.fit_transform(torch.tensor(train_txt_features).flatten(1).numpy())\n",
        "            del train_txt_features # free memory\n",
        "            train_img_pca_features = self.img_pca.fit_transform(torch.tensor(train_img_features).flatten(1).numpy())\n",
        "            del train_img_features # free memory\n",
        "\n",
        "            # Fit linear regression\n",
        "            print(f\"Fitting linear regression models for {layer}...\")\n",
        "            lh_lin_reg = LinearRegression().fit(np.hstack([train_txt_pca_features, train_img_pca_features]), train_lh_fmri)\n",
        "            rh_lin_reg = LinearRegression().fit(np.hstack([train_txt_pca_features, train_img_pca_features]), train_rh_fmri)\n",
        "            del train_txt_pca_features, train_img_pca_features, train_lh_fmri, train_rh_fmri # free memory\n",
        "\n",
        "            # Transform validation features\n",
        "            print(f\"Transforming validation features for {layer}...\")\n",
        "            val_txt_pca_features = self.txt_pca.transform(torch.tensor(val_txt_features).flatten(1).numpy())\n",
        "            del val_txt_features # free memory\n",
        "            val_img_pca_features = self.img_pca.transform(torch.tensor(val_img_features).flatten(1).numpy())\n",
        "            del val_img_features # free memory\n",
        "\n",
        "            # Predict validation set\n",
        "            print(f\"Predicting validation set for {layer}...\")\n",
        "            lh_val_pred = lh_lin_reg.predict(np.hstack([val_txt_pca_features, val_img_pca_features]))\n",
        "            rh_val_pred = rh_lin_reg.predict(np.hstack([val_txt_pca_features, val_img_pca_features]))\n",
        "            del val_txt_pca_features, val_img_pca_features, lh_lin_reg, rh_lin_reg # free memory\n",
        "            \n",
        "            # Calculate correlations\n",
        "            print(f\"Calculating correlations for {layer}...\\n\")\n",
        "            lh_correlation, rh_correlation = self.calculate_correlations(lh_val_pred, rh_val_pred, val_lh_fmri, val_rh_fmri)\n",
        "            lh_median_roi_correlation, rh_median_roi_correlation = self.calculate_median_correlations(lh_correlation, rh_correlation)\n",
        "            \n",
        "            # Store correlations\n",
        "            correlations[layer] = {\"lh\": lh_median_roi_correlation, \"rh\": rh_median_roi_correlation} \n",
        "        return correlations\n",
        "\n",
        "    def return_idxs(self) -> np.ndarray:\n",
        "        return np.arange(len(self.raw_txt_features[list(self.raw_txt_features.keys())[0]])) "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **Executing the k-Fold Procedure**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Raise exception to prevent accidental execution\n",
        "raise Exception(\"This code is not meant to be run. It is only meant to be used as a reference for the code used to generate the submission.\")\n",
        "\n",
        "# Initialize required objects\n",
        "subject = Subject(\"subj01\")\n",
        "\n",
        "# Initialize kfold procedure\n",
        "kfold_procedure = KFoldCombinedCLIPSingleSubject(\n",
        "    subject=subject, \n",
        "    txt_pca=PCA(n_components=200), \n",
        "    img_pca=PCA(n_components=200), \n",
        "    model_name=\"CLIP Vision + Text\", \n",
        "    description=\"CLIP Vision + Text Model, Embedding layer and first 4 Transformer Layers, PCA 200, Single layer linear regression, 8-fold cross validation\", \n",
        "    layers=[0,1,2,3,4], # only first 5 layers\n",
        "    last_hidden_state=False\n",
        "    )\n",
        "\n",
        "# Run first kfold\n",
        "KFold(folds=8, procedure=kfold_procedure).run()\n",
        "\n",
        "# Initalize kfold procedure for second batch\n",
        "kfold_procedure = KFoldCombinedCLIPSingleSubject(\n",
        "    subject=subject, \n",
        "    txt_pca=PCA(n_components=200), \n",
        "    img_pca=PCA(n_components=200), \n",
        "    model_name=\"CLIP Vision + Text\", \n",
        "    description=\"CLIP Vision + Text Model, Transformer layers 5 to 9, PCA 200, Single layer linear regression, 8-fold cross validation\", \n",
        "    layers=[5,6,7,8,9], # the next 5 layers\n",
        "    last_hidden_state=False\n",
        "    )\n",
        "# Run second kfold\n",
        "KFold(folds=8, procedure=kfold_procedure).run()\n",
        "\n",
        "# Initalize kfold procedure for third batch\n",
        "kfold_procedure = KFoldCombinedCLIPSingleSubject(\n",
        "    subject=subject, \n",
        "    txt_pca=PCA(n_components=200), \n",
        "    img_pca=PCA(n_components=200), \n",
        "    model_name=\"CLIP Vision + Text\", \n",
        "    description=\"CLIP Vision + Text Model, Transformer layers 10 to 12 and Final Layer, PCA 200, Single layer linear regression, 8-fold cross validation\", \n",
        "    layers=[10,11,12], # the next 4 layers\n",
        "    last_hidden_state=True # and final layer\n",
        "    )\n",
        "# Run third kfold\n",
        "KFold(folds=8, procedure=kfold_procedure).run()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **CLIP Vision Model, Layer 4 + 8, Normal & Combined PCA200, Linear Regression**  <a id='cv_vis4&8_pca200_linreg'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because of our findings in the first cross validation we decided to compare our best Vision Model (PCA200, Layer 7) with a combined Vision Model (PCA200, Layer4 + Layer8). \n",
        "\n",
        "We ran 8 folds on image data of subject 1. For each iteration the current fold was selected as the validation set and the remaining folds were used as the training set.\n",
        "\n",
        "The validation procedure for each fold was as follows: \n",
        "\n",
        "Preparation\n",
        "\n",
        "- Supplying the training set to a CLIPVisionModel.\n",
        "- Extracting the raw features for Transformer Layers 4, 7 and 8.\n",
        "\n",
        "Looping over all folds\n",
        "- For each fold:\n",
        "    - Assigning training and validation sets for images and fmri activity.\n",
        "    \n",
        "    Layer 7\n",
        "    \n",
        "    - Fit PCA with 200 components using the raw training features of layer 7.\n",
        "    - Transforming the raw training features using the fitted PCA.\n",
        "    - Fitting a linear regression, predicting training fmri activity from the pca transformed training features. (for left and right hemisphere)\n",
        "    - Transforming the raw validation features using the fitted PCA.\n",
        "    - Using the fitted linear models to predict validation fmri activity from the pca transformed validation features. (for left and right hemisphere)\n",
        "    - Calculating correlations between predicted and actual fmri activity for the validation set. (for left and right hemisphere)\n",
        "    - Calculating median correlations for each of the 36 challenge ROI. (for left and right hemisphere)\n",
        "    - Storing the median correlations for each hemisphere for layer 7 for the current fold.\n",
        "\n",
        "    Layers 4 and 8\n",
        "    \n",
        "    - Combine raw training features of layer 4 and 8.\n",
        "    - Fit PCA with 200 components using the combined raw training features of layers 4 and 8.\n",
        "    - Transforming the raw training features using the fitted PCA.\n",
        "    - Fitting a linear regression, predicting training fmri activity from the pca transformed training features. (for left and right hemisphere)\n",
        "    - Transforming the raw validation features using the fitted PCA.\n",
        "    - Using the fitted linear models to predict validation fmri activity from the pca transformed validation features. (for left and right hemisphere)\n",
        "    - Calculating correlations between predicted and actual fmri activity for the validation set. (for left and right hemisphere)\n",
        "    - Calculating median correlations for each of the 36 challenge ROI. (for left and right hemisphere)\n",
        "    - Storing the median correlations for each hemisphere for the combination of layers 4 and 8 for the current fold.\n",
        "    \n",
        "After the validation\n",
        "\n",
        "- Calculating mean median correlations accross all folds for each layer. (for left and right hemisphere)\n",
        "- Plotting the mean median correlations for each layer for each ROI and select the layer with the highest mean median correlation averaged over all ROIs.   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_kfold_result(\"007\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **KFoldProcedure CLass**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KFoldVisPCA200L4L8(KFoldProcedure):\n",
        "    \"\"\"A procedure that runs k-fold on all layers of a single CLIP model on a single subject.\"\"\"\n",
        "    def __init__(self, \n",
        "                 subject: Subject, \n",
        "                 model_name: str = None,\n",
        "                 description: str = None) -> None:\n",
        "        assert isinstance(subject, Subject), \"subject must be an instance of Subject\"\n",
        "        self.subject = subject\n",
        "        self.model_name = model_name\n",
        "        self.description = description\n",
        "        self.correlations = {}\n",
        "        super().__init__()\n",
        "\n",
        "    def prepare(self):\n",
        "        # Prepare data\n",
        "        self.subject.create_dataloaders(processor, batch_size=400)\n",
        "        train_dataloader = self.subject.train_img_dataloader\n",
        "        \n",
        "        # Extract raw features\n",
        "        feature_extractor = CLIPFeatureExtractor(idxs=[4,8], last_hidden_layer=False, model=vis_model, dataloader=train_dataloader)\n",
        "        feature_extractor.extract_raw_features_from_model()\n",
        "        self.raw_features = feature_extractor.feature_dict\n",
        "        del feature_extractor # free memory\n",
        "\n",
        "        # Load challenge rois\n",
        "        self.subject.load_challenge_rois()\n",
        "        self.lh_challenge_rois = self.subject.lh_challenge_rois\n",
        "        self.rh_challenge_rois = self.subject.rh_challenge_rois\n",
        "        self.roi_name_maps = self.subject.roi_name_maps\n",
        "\n",
        "        # Load neural data\n",
        "        self.subject.load_neural_data()\n",
        "        self.lh_fmri = self.subject.lh_fmri\n",
        "        self.rh_fmri = self.subject.rh_fmri\n",
        "        self.subject = None # free memory\n",
        "\n",
        "        # Prepare correlation dict\n",
        "        self.fold_correlations = {}\n",
        "\n",
        "    def run(self, train_idxs: np.ndarray, val_idxs: np.ndarray) -> Dict[str, Dict[str, np.ndarray]]:        \n",
        "        correlations = {}\n",
        "\n",
        "        # Assign train and val fmri\n",
        "        train_lh_fmri = self.lh_fmri[train_idxs]\n",
        "        train_rh_fmri = self.rh_fmri[train_idxs]\n",
        "        val_lh_fmri = self.lh_fmri[val_idxs]\n",
        "        val_rh_fmri = self.rh_fmri[val_idxs]\n",
        "\n",
        "        ####### Layers PCA (4 + 8) #######\n",
        "        # Assign train and val features\n",
        "        train_features = self.raw_features[\"Transformer Layer 4\"][train_idxs]\n",
        "        train_features = np.hstack([train_features, self.raw_features[\"Transformer Layer 8\"][train_idxs]])\n",
        "        val_features = self.raw_features[\"Transformer Layer 4\"][val_idxs]\n",
        "        val_features = np.hstack([val_features, self.raw_features[\"Transformer Layer 8\"][val_idxs]])\n",
        "\n",
        "        # Fit PCA model\n",
        "        pca = PCA(n_components=200)\n",
        "        train_pca_features = pca.fit_transform(torch.tensor(train_features).flatten(1).numpy())\n",
        "        del train_features # free memory\n",
        "\n",
        "        # Fit linear regressions\n",
        "        lh_lin_reg = LinearRegression().fit(train_pca_features, train_lh_fmri)\n",
        "        rh_lin_reg = LinearRegression().fit(train_pca_features, train_rh_fmri)\n",
        "        del train_pca_features # free memory\n",
        "\n",
        "        # Transform validation features\n",
        "        val_txt_pca_features = pca.transform(torch.tensor(val_features).flatten(1).numpy())\n",
        "        del val_features, pca # free memory\n",
        "\n",
        "        # Predict validation dict\n",
        "        lh_val_pred = lh_lin_reg.predict(val_txt_pca_features)\n",
        "        rh_val_pred = rh_lin_reg.predict(val_txt_pca_features)\n",
        "        del val_txt_pca_features, lh_lin_reg, rh_lin_reg # free memory\n",
        "\n",
        "        # Calculate correlations\n",
        "        lh_correlation, rh_correlation = self.calculate_correlations(lh_val_pred, rh_val_pred, val_lh_fmri, val_rh_fmri)\n",
        "        lh_median_roi_correlation, rh_median_roi_correlation = self.calculate_median_correlations(lh_correlation, rh_correlation)\n",
        "\n",
        "        # Store correlations\n",
        "        correlations[\"Transformer Layer PCA(4 + 8)\"] = {\"lh\": lh_median_roi_correlation, \"rh\": rh_median_roi_correlation} \n",
        "\n",
        "        ####### Layers PCA(4) + PCA(8) #######\n",
        "        # Assign train and val features\n",
        "        train_features_4 = self.raw_features[\"Transformer Layer 4\"][train_idxs]\n",
        "        train_features_8 = self.raw_features[\"Transformer Layer 8\"][train_idxs]\n",
        "        val_features_4 = self.raw_features[\"Transformer Layer 4\"][val_idxs]\n",
        "        val_features_8 = self.raw_features[\"Transformer Layer 8\"][val_idxs]\n",
        "\n",
        "        # Fit PCA models\n",
        "        pca_4 = PCA(n_components=200)\n",
        "        train_pca_features_4 = pca_4.fit_transform(torch.tensor(train_features_4).flatten(1).numpy())\n",
        "        del train_features_4 # free memory\n",
        "        pca_8 = PCA(n_components=200)\n",
        "        train_pca_features_8 = pca_8.fit_transform(torch.tensor(train_features_8).flatten(1).numpy())\n",
        "        del train_features_8 # free memory\n",
        "        train_pca_features = np.hstack([train_pca_features_4, train_pca_features_8])\n",
        "        del train_pca_features_4, train_pca_features_8 # free memory\n",
        "\n",
        "        # Fit linear regressions\n",
        "        lh_lin_reg = LinearRegression().fit(train_pca_features, train_lh_fmri)\n",
        "        rh_lin_reg = LinearRegression().fit(train_pca_features, train_rh_fmri)\n",
        "        del train_pca_features # free memory\n",
        "\n",
        "        # Transform validation features\n",
        "        val_txt_pca_features_4 = pca_4.transform(torch.tensor(val_features_4).flatten(1).numpy())\n",
        "        del val_features_4 # free memory\n",
        "        val_txt_pca_features_8 = pca_4.transform(torch.tensor(val_features_8).flatten(1).numpy())\n",
        "        del val_features_8 # free memory\n",
        "        val_txt_pca_features = np.hstack([val_txt_pca_features_4, val_txt_pca_features_8])\n",
        "        del val_txt_pca_features_4, val_txt_pca_features_8, pca_4, pca_8 # free memory\n",
        "\n",
        "        # Predict validation dict\n",
        "        lh_val_pred = lh_lin_reg.predict(val_txt_pca_features)\n",
        "        rh_val_pred = rh_lin_reg.predict(val_txt_pca_features)\n",
        "        del val_txt_pca_features, lh_lin_reg, rh_lin_reg # free memory\n",
        "\n",
        "        # Calculate correlations\n",
        "        lh_correlation, rh_correlation = self.calculate_correlations(lh_val_pred, rh_val_pred, val_lh_fmri, val_rh_fmri)\n",
        "        lh_median_roi_correlation, rh_median_roi_correlation = self.calculate_median_correlations(lh_correlation, rh_correlation)\n",
        "\n",
        "        # Store correlations\n",
        "        correlations[\"Transformer Layer PCA(4) + PCA(8)\"] = {\"lh\": lh_median_roi_correlation, \"rh\": rh_median_roi_correlation} \n",
        "\n",
        "        return correlations\n",
        "\n",
        "    def return_idxs(self) -> np.ndarray:\n",
        "        return np.arange(len(self.raw_features[list(self.raw_features.keys())[0]])) \n",
        "    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **Executing the k-Fold Procedure**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Raise exception to prevent accidental execution\n",
        "raise Exception(\"This code is not meant to be run. It is only meant to be used as a reference for the code used to generate the submission.\")\n",
        "\n",
        "# Initialize required objects\n",
        "subject = Subject(\"subj01\")\n",
        "\n",
        "# Initialize kfold procedure\n",
        "kfold_procedure = KFoldVisPCA200L4L8(\n",
        "    subject=subject, \n",
        "    model_name=\"CLIP Vision\", \n",
        "    description=\"CLIP Vision, Layer 4+8, PCA 200, Linear regression, 8-fold cross validation\", \n",
        "    )\n",
        "\n",
        "# Run kfold\n",
        "KFold(folds=8, procedure=kfold_procedure).run()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **CLIP Text + Vision Model, Vision Layer 4 + 8 & Text Layer 11, Combined PCA200, Linear Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_kfold_result('008')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **KFoldProcedure Class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KFoldVisL4L8TextL11PCA200(KFoldProcedure):\n",
        "    \"\"\"Combining layers 4 and 8 of CLIP Vision and layer 11 of CLIP Text, PCA 200, Linear regression, 8-fold cross validation\"\"\"\n",
        "    def __init__(self, \n",
        "                 subject: Subject, \n",
        "                 model_name: str = None,\n",
        "                 description: str = None) -> None:\n",
        "        assert isinstance(subject, Subject), \"subject must be an instance of Subject\"\n",
        "        self.subject = subject\n",
        "        self.model_name = model_name\n",
        "        self.description = description\n",
        "        self.correlations = {}\n",
        "        super().__init__()\n",
        "\n",
        "    def prepare(self):\n",
        "        # Prepare data\n",
        "        subject.create_dataloaders(processor, batch_size = 300)\n",
        "        train_img_dataloader = subject.train_img_dataloader\n",
        "        train_txt_dataloader = subject.train_txt_dataloader\n",
        "        \n",
        "        # Extract raw img features\n",
        "        feature_extractor = CLIPFeatureExtractor(idxs=[4,8], last_hidden_layer=False, model=vis_model, dataloader=train_img_dataloader)\n",
        "        feature_extractor.extract_raw_features_from_model()\n",
        "        self.raw_img_features = feature_extractor.feature_dict\n",
        "        del feature_extractor # free memory\n",
        "\n",
        "        # Extract raw txt features\n",
        "        feature_extractor = CLIPFeatureExtractor(idxs=[11], last_hidden_layer=False, model=txt_model, dataloader=train_txt_dataloader)\n",
        "        feature_extractor.extract_raw_features_from_model()\n",
        "        self.raw_txt_features = feature_extractor.feature_dict\n",
        "        del feature_extractor # free memory\n",
        "\n",
        "        # Load challenge rois\n",
        "        self.subject.load_challenge_rois()\n",
        "        self.lh_challenge_rois = self.subject.lh_challenge_rois\n",
        "        self.rh_challenge_rois = self.subject.rh_challenge_rois\n",
        "        self.roi_name_maps = self.subject.roi_name_maps\n",
        "\n",
        "        # Load neural data\n",
        "        self.subject.load_neural_data()\n",
        "        self.lh_fmri = self.subject.lh_fmri\n",
        "        self.rh_fmri = self.subject.rh_fmri\n",
        "        self.subject = None # free memory\n",
        "\n",
        "        # Prepare correlation dict\n",
        "        self.fold_correlations = {}\n",
        "\n",
        "    def run(self, train_idxs: np.ndarray, val_idxs: np.ndarray) -> Dict[str, Dict[str, np.ndarray]]:        \n",
        "        correlations = {}\n",
        "\n",
        "        # Assign train and val fmri\n",
        "        train_lh_fmri = self.lh_fmri[train_idxs]\n",
        "        train_rh_fmri = self.rh_fmri[train_idxs]\n",
        "        val_lh_fmri = self.lh_fmri[val_idxs]\n",
        "        val_rh_fmri = self.rh_fmri[val_idxs]\n",
        "\n",
        "        ####### Layers PCA (4 + 8) #######\n",
        "        # Assign train and val features\n",
        "        train_features = np.concatenate([torch.tensor(self.raw_img_features[\"Transformer Layer 4\"][train_idxs]).flatten(1).numpy(), \n",
        "                                         torch.tensor(self.raw_img_features[\"Transformer Layer 8\"][train_idxs]).flatten(1).numpy(),\n",
        "                                         torch.tensor(self.raw_txt_features[\"Transformer Layer 11\"][train_idxs]).flatten(1).numpy()], axis=1)\n",
        "        val_features = np.concatenate([torch.tensor(self.raw_img_features[\"Transformer Layer 4\"][val_idxs]).flatten(1).numpy(), \n",
        "                                       torch.tensor(self.raw_img_features[\"Transformer Layer 8\"][val_idxs]).flatten(1).numpy(),\n",
        "                                       torch.tensor(self.raw_txt_features[\"Transformer Layer 11\"][val_idxs]).flatten(1).numpy()], axis=1)\n",
        "\n",
        "        # Fit PCA model\n",
        "        pca = PCA(n_components=200)\n",
        "        train_pca_features = pca.fit_transform(torch.tensor(train_features).flatten(1).numpy())\n",
        "        del train_features # free memory\n",
        "\n",
        "        # Fit linear regressions\n",
        "        lh_lin_reg = LinearRegression().fit(train_pca_features, train_lh_fmri)\n",
        "        rh_lin_reg = LinearRegression().fit(train_pca_features, train_rh_fmri)\n",
        "        del train_pca_features # free memory\n",
        "\n",
        "        # Transform validation features\n",
        "        val_pca_features = pca.transform(torch.tensor(val_features).flatten(1).numpy())\n",
        "        del val_features, pca # free memory\n",
        "\n",
        "        # Predict validation dict\n",
        "        lh_val_pred = lh_lin_reg.predict(val_pca_features)\n",
        "        rh_val_pred = rh_lin_reg.predict(val_pca_features)\n",
        "        del val_pca_features, lh_lin_reg, rh_lin_reg # free memory\n",
        "\n",
        "        # Calculate correlations\n",
        "        lh_correlation, rh_correlation = self.calculate_correlations(lh_val_pred, rh_val_pred, val_lh_fmri, val_rh_fmri)\n",
        "        lh_median_roi_correlation, rh_median_roi_correlation = self.calculate_median_correlations(lh_correlation, rh_correlation)\n",
        "\n",
        "        # Store correlations\n",
        "        correlations[\"PCA(Vis4 + Vis8 + Txt11)\"] = {\"lh\": lh_median_roi_correlation, \"rh\": rh_median_roi_correlation} \n",
        "\n",
        "        return correlations\n",
        "\n",
        "    def return_idxs(self) -> np.ndarray:\n",
        "        return np.arange(len(self.raw_img_features[list(self.raw_img_features.keys())[0]])) \n",
        "    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **Executing the k-Fold Procedure**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Raise exception to prevent accidental execution\n",
        "raise Exception(\"This code is not meant to be run. It is only meant to be used as a reference for the code used to generate the submission.\")\n",
        "\n",
        "# Initialize required objects\n",
        "subject = Subject(\"subj01\")\n",
        "\n",
        "# Initialize kfold procedure\n",
        "kfold_procedure = KFoldVisL4L8TextL11PCA200(\n",
        "    subject=subject, \n",
        "    model_name=\"CLIP Vision + Text Model (V4, V8, T11)\", \n",
        "    description=\"CLIP Vision + Text Model, Vision Layer 4+8 & Text Layer 11, Combiend PCA 200, Linear regression, 8-fold cross validation\", \n",
        "    )\n",
        "\n",
        "# Run kfold\n",
        "KFold(folds=8, procedure=kfold_procedure).run()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **CLIP Text + Vision Model, Combined PCA200, Linear Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_kfold_result(\"012\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **KFoldProcedure Class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KFoldCombinedCLIPSingleSubjectComb(KFoldProcedure):\n",
        "    \"\"\"A procedure that runs k-fold on all layers of a combination of the text and vision CLIP model on a single subject.\"\"\"\n",
        "    def __init__(self, \n",
        "                 subject: Subject, \n",
        "                 model_name: str = None,\n",
        "                 description: str = None,\n",
        "                 layers: list = [],\n",
        "                 last_hidden_state = False) -> None:\n",
        "        self.subject = subject\n",
        "        self.model_name = model_name\n",
        "        self.description = description\n",
        "        self.layers = layers.copy()\n",
        "        self.layers2 = layers.copy()\n",
        "        self.last_hidden_state = last_hidden_state\n",
        "        self.correlations = {}\n",
        "        super().__init__()\n",
        "\n",
        "    def prepare(self):\n",
        "        # Prepare data\n",
        "        self.subject.create_dataloaders(processor, batch_size=400)\n",
        "        train_img_dataloader = self.subject.train_img_dataloader\n",
        "        train_txt_dataloader = self.subject.train_txt_dataloader\n",
        "\n",
        "        # Extract raw features from text model\n",
        "        feature_extractor = CLIPFeatureExtractor(idxs=self.layers, last_hidden_layer=self.last_hidden_state, model=txt_model, dataloader=train_txt_dataloader)\n",
        "        feature_extractor.extract_raw_features_from_model()\n",
        "        self.raw_txt_features = feature_extractor.feature_dict\n",
        "        del feature_extractor # free memory\n",
        "\n",
        "        # Extract raw features from vision model\n",
        "        feature_extractor = CLIPFeatureExtractor(idxs=self.layers2, last_hidden_layer=self.last_hidden_state, model=vis_model, dataloader=train_img_dataloader)\n",
        "        feature_extractor.extract_raw_features_from_model()\n",
        "        self.raw_img_features = feature_extractor.feature_dict\n",
        "        del feature_extractor # free memory\n",
        "\n",
        "        # Load challenge rois\n",
        "        self.subject.load_challenge_rois()\n",
        "        self.lh_challenge_rois = self.subject.lh_challenge_rois\n",
        "        self.rh_challenge_rois = self.subject.rh_challenge_rois\n",
        "        self.roi_name_maps = self.subject.roi_name_maps\n",
        "\n",
        "        # Load neural data\n",
        "        self.subject.load_neural_data()\n",
        "        self.lh_fmri = self.subject.lh_fmri\n",
        "        self.rh_fmri = self.subject.rh_fmri\n",
        "        self.subject = None # free memory\n",
        "\n",
        "        # Prepare correlation dict\n",
        "        self.fold_correlations = {}\n",
        "\n",
        "    def run(self, train_idxs: np.ndarray, val_idxs: np.ndarray) -> Dict[str, Dict[str, np.ndarray]]:\n",
        "        # Loop over all layers          \n",
        "        correlations = {}\n",
        "        for layer in self.raw_txt_features.keys():\n",
        "            print(f\"> {layer}\")\n",
        "            # Assign train and val text features\n",
        "            train_txt_features = self.raw_txt_features[layer][train_idxs]\n",
        "            val_txt_features = self.raw_txt_features[layer][val_idxs]\n",
        "            # Assign train and val vision features\n",
        "            train_img_features = self.raw_img_features[layer][train_idxs]\n",
        "            val_img_features = self.raw_img_features[layer][val_idxs]\n",
        "            # Combine features\n",
        "            train_features = np.concatenate([torch.tensor(train_txt_features).flatten(1).numpy(), \n",
        "                                             torch.tensor(train_img_features).flatten(1).numpy()], axis=1)\n",
        "            val_features = np.concatenate([torch.tensor(val_txt_features).flatten(1).numpy(), \n",
        "                                           torch.tensor(val_img_features).flatten(1).numpy()], axis=1)\n",
        "            del train_txt_features, train_img_features, val_txt_features, val_img_features\n",
        "\n",
        "            # Assign train and val fmri\n",
        "            train_lh_fmri = self.lh_fmri[train_idxs]\n",
        "            train_rh_fmri = self.rh_fmri[train_idxs]\n",
        "            val_lh_fmri = self.lh_fmri[val_idxs]\n",
        "            val_rh_fmri = self.rh_fmri[val_idxs]\n",
        "\n",
        "            # Fit PCA models\n",
        "            print(f\"Fitting PCA model for {layer}...\")\n",
        "            pca = PCA(n_components=200)\n",
        "            train_pca_features = pca.fit_transform(torch.tensor(train_features).flatten(1).numpy())\n",
        "            del train_features # free memory\n",
        "\n",
        "            # Fit linear regression\n",
        "            print(f\"Fitting linear regression models for {layer}...\")\n",
        "            lh_lin_reg = LinearRegression().fit(train_pca_features, train_lh_fmri)\n",
        "            rh_lin_reg = LinearRegression().fit(train_pca_features, train_rh_fmri)\n",
        "            del train_pca_features, train_lh_fmri, train_rh_fmri # free memory\n",
        "\n",
        "            # Transform validation features\n",
        "            print(f\"Transforming validation features for {layer}...\")\n",
        "            val_pca_features = pca.transform(torch.tensor(val_features).flatten(1).numpy())\n",
        "            del val_features, pca # free memory\n",
        "\n",
        "            # Predict validation set\n",
        "            print(f\"Predicting validation set for {layer}...\")\n",
        "            lh_val_pred = lh_lin_reg.predict(val_pca_features)\n",
        "            rh_val_pred = rh_lin_reg.predict(val_pca_features)\n",
        "            del val_pca_features, lh_lin_reg, rh_lin_reg # free memory\n",
        "            \n",
        "            # Calculate correlations\n",
        "            print(f\"Calculating correlations for {layer}...\\n\")\n",
        "            lh_correlation, rh_correlation = self.calculate_correlations(lh_val_pred, rh_val_pred, val_lh_fmri, val_rh_fmri)\n",
        "            lh_median_roi_correlation, rh_median_roi_correlation = self.calculate_median_correlations(lh_correlation, rh_correlation)\n",
        "            \n",
        "            # Store correlations\n",
        "            correlations[layer] = {\"lh\": lh_median_roi_correlation, \"rh\": rh_median_roi_correlation} \n",
        "        return correlations\n",
        "\n",
        "    def return_idxs(self) -> np.ndarray:\n",
        "        return np.arange(len(self.raw_txt_features[list(self.raw_txt_features.keys())[0]])) "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **Executing the k-Fold Procedure**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Raise exception to prevent accidental execution\n",
        "raise Exception(\"This code is not meant to be run. It is only meant to be used as a reference for the code used to generate the submission.\")\n",
        "\n",
        "# Initialize required objects\n",
        "subject = Subject(\"subj01\")\n",
        "\n",
        "# Initialize kfold procedure\n",
        "kfold_procedure = KFoldCombinedCLIPSingleSubjectComb(\n",
        "    subject=subject,\n",
        "    model_name=\"CLIP Vision + Text (Combined PCA)\", \n",
        "    description=\"CLIP Vision + Text Model, Embedding layer and first 4 Transformer Layers, Combined PCA 200, Single layer linear regression, 8-fold cross validation\", \n",
        "    layers=[0,1,2,3,4], # only first 5 layers\n",
        "    last_hidden_state=False\n",
        "    )\n",
        "# Run first kfold\n",
        "KFold(folds=8, procedure=kfold_procedure).run()\n",
        "\n",
        "# Initalize kfold procedure for second batch\n",
        "kfold_procedure = KFoldCombinedCLIPSingleSubjectComb(\n",
        "    subject=subject,\n",
        "    model_name=\"CLIP Vision + Text (Combined PCA)\", \n",
        "    description=\"CLIP Vision + Text Model, Transformer layers 5 to 9, Combined PCA 200, Single layer linear regression, 8-fold cross validation\", \n",
        "    layers=[5,6,7,8,9], # the next 5 layers\n",
        "    last_hidden_state=False\n",
        "    )\n",
        "# Run second kfold\n",
        "KFold(folds=8, procedure=kfold_procedure).run()\n",
        "\n",
        "# Initalize kfold procedure for third batch\n",
        "kfold_procedure = KFoldCombinedCLIPSingleSubjectComb(\n",
        "    subject=subject,\n",
        "    model_name=\"CLIP Vision + Text (Combined PCA)\", \n",
        "    description=\"CLIP Vision + Text Model, Transformer layers 10 to 12 and Final Layer, Combined PCA 200, Single layer linear regression, 8-fold cross validation\", \n",
        "    layers=[10,11,12], # the next 4 layers\n",
        "    last_hidden_state=True # and final layer\n",
        "    )\n",
        "# Run third kfold\n",
        "KFold(folds=8, procedure=kfold_procedure).run()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "N8bFfDgocEKE"
      },
      "source": [
        "# ToDo:\n",
        "\n",
        "## **Discussion & Conclusions**\n",
        "* Interpret the results (e.g. low r in first visual cortecies when using the Text vs Visual Model) \n",
        "* Potential confounds? \n",
        "* What are the limitations? \n",
        "    * cropped images \n",
        "* Outlook/Improvements/What would you do if you could redo the experiment?*\n",
        "\n",
        "In this study CLIP is used to predict brain activation across the whole brain in response to image stimuli. Four CLIP based models are tested: a text model, a vision model, a combined text and vision model and a vision model of layer 4 and 8 combined. The results indicate that the text model is the worst performing model to predict brain activation. Interestingly, the early visual ROI show low correlations in the text model, where later layers are better predicted by this model. This makes sense as the early visual layers in the brain are important for visual processing, the text model did not manage to predict this visual activation pattern based on the text model of CLIP. The vision model however, showed high correlations for early visual layers and outperformed the text model also on later ROIs layers. The combined model of text and vision performed equal or worse than the vision model for the ROIs. This was an unexpected finding, as we hypothesized that combining visual information and textual information would improve the predictions of the model due to the whole brain evaluation contrary to only visual ROI predictions. Because the vision model outperformed the other two models, further testing was done on specific layers. Combining layer 4 and 8 showed high correlations and predicted the fMRI activation better than the two seperate layers. \n",
        "\n",
        "One limitation in this study is the use of cropped images from the COCO dataset, as where presented to us in the challenge data. However, we used the captions of the images in the original COCO dataset. Due to the cropping we can not be certain that the caption of the COCO images still correspond well to the cropped images used in the challenge data. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Challenge Submission: Models Submitted For Algonauts Challenge**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DNyqKJcNfWnl"
      },
      "source": [
        "### **SubmissionProcedure & CreateSubmission Classes**\n",
        "\n",
        "Below are the models we submitted to the algonauts 2023 competition.\n",
        "\n",
        "We wrote two classes, CreateSubmission and SubmissionProcedure, to make the process of generating predictions for submission as straight forward as possible. The CreateSubmission class creates folders for submission and loops over all subjects to apply the specific SubmissionProcedure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzhAlABKskqy"
      },
      "outputs": [],
      "source": [
        "class SubmissionProcedure:\n",
        "    \"\"\"Used to create a submission procedure that is executed for each subject in the CreateSubmission class.\"\"\"\n",
        "\n",
        "    def run(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class CreateSubmission:\n",
        "    \"\"\"Create a new challenge submission.\"\"\"\n",
        "    def __init__(self, \n",
        "                 subjects : List[Subject],\n",
        "                 procedure: SubmissionProcedure):\n",
        "        self.subjects = subjects\n",
        "        self.procedure = procedure\n",
        "\n",
        "    def create_submission_folder(self) -> None:\n",
        "        # create new submission folder with newest name\n",
        "        submissions = glob.glob(f\"submissions/submission*\")\n",
        "        if len(submissions) == 0:\n",
        "            # create first submission folder\n",
        "            self.folder_name = \"submission001\"\n",
        "            os.mkdir(f\"submissions/{self.folder_name}\")\n",
        "        else:\n",
        "            # create next submission folder\n",
        "            last_submission = sorted(submissions)[-1]\n",
        "            last_submission_number = int(last_submission.split(\"/\")[-1].split(\"submission\")[-1])\n",
        "            next_submission_number = last_submission_number + 1\n",
        "            self.folder_name = f\"submission{str(next_submission_number).zfill(3)}\"\n",
        "            os.mkdir(f\"submissions/{self.folder_name}\")\n",
        "        # Write text file with model description\n",
        "        with open(f\"submissions/{self.folder_name}/info.txt\", \"w\") as f:\n",
        "            f.write(self.procedure.description)\n",
        "        # create a folder for each subject\n",
        "        for subject in self.subjects:\n",
        "            os.mkdir(f\"submissions/{self.folder_name}/{subject.subject}\")\n",
        "\n",
        "    def save_predictions(self, subject: Subject, lh_predictions: np.ndarray, rh_predictions: np.ndarray) -> None:\n",
        "        \"\"\"Save predictions for a subject.\"\"\"\n",
        "        lh_predictions = lh_predictions.astype(np.float32)\n",
        "        rh_predictions = rh_predictions.astype(np.float32)\n",
        "        save_path = f\"submissions/{self.folder_name}/{subject.subject}\"\n",
        "        # Save predictions\n",
        "        np.save(f\"{save_path}/lh_pred_test.npy\", lh_predictions)\n",
        "        np.save(f\"{save_path}/rh_pred_test.npy\", rh_predictions)\n",
        "\n",
        "    def run(self):\n",
        "        self.create_submission_folder()\n",
        "        for subject in self.subjects:\n",
        "            print(f\"############################\")\n",
        "            print(f\"# Subject: {subject.subject}\")\n",
        "            print(f\"############################\")\n",
        "            lh_predictions, rh_predictions = self.procedure.run(subject)\n",
        "            self.save_predictions(subject, lh_predictions, rh_predictions)\n",
        "            print(f\"\\n\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2Nu-JvcZEHaI"
      },
      "source": [
        "### **First and Second Submission - Vision Only Final Layer / Text Only Final Layer, PCA 100**\n",
        "\n",
        "To familiarize ourselves with the CodaLab submission system we selected our first two models without cross validation and used a 90%-10% train-validation split to guide our decision instead.\n",
        "\n",
        "We ended up submitting two linear models based on PCA (100 components) transformed features of the final layer of the CLIPVisionModel (submission score = 43.262) and CLIPTextModel (submission score = 34.210)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CLIPVisionFinalLayerPCA100LinearRegression(SubmissionProcedure):\n",
        "    def __init__(self):\n",
        "        self.description = \"CLIP Vision Model, Final Layer, PCA 100, Linear Regression, First test submission.\"\n",
        "\n",
        "    def run(self, subject: Subject) -> np.ndarray:\n",
        "        \"\"\"Run the model on a subject.\"\"\"\n",
        "        # Prepare data\n",
        "        subject.create_dataloaders(processor=processor, batch_size=300)\n",
        "        subject.load_neural_data()\n",
        "        train_img_dataloader = subject.train_img_dataloader\n",
        "        test_img_dataloader = subject.test_img_dataloader\n",
        "        lh_fmri = subject.lh_fmri\n",
        "        rh_fmri = subject.rh_fmri\n",
        "        del subject # free up memory\n",
        "\n",
        "        # Prepare feature extractor\n",
        "        train_feature_extractor = CLIPFeatureExtractor(idxs=[], last_hidden_layer=True, model=vis_model, dataloader=train_img_dataloader)\n",
        "        test_feature_extractor = CLIPFeatureExtractor(idxs=[], last_hidden_layer=True, model=vis_model, dataloader=test_img_dataloader)\n",
        "\n",
        "        # Extract features\n",
        "        train_feature_extractor.extract_raw_features_from_model()\n",
        "        raw_train_features = train_feature_extractor.feature_dict[\"Final Layer\"]\n",
        "        del train_feature_extractor\n",
        "\n",
        "        # Fit PCA\n",
        "        pca = PCA(n_components=100)\n",
        "        pca_transformed_train_features = pca.fit_transform(torch.tensor(raw_train_features).flatten(1).numpy())\n",
        "        del raw_train_features\n",
        "\n",
        "        # Fit linear regression\n",
        "        lh_lin_reg = LinearRegression().fit(pca_transformed_train_features, lh_fmri)\n",
        "        rh_lin_reg = LinearRegression().fit(pca_transformed_train_features, rh_fmri)\n",
        "        del pca_transformed_train_features\n",
        "\n",
        "        # Extract test features\n",
        "        test_feature_extractor.extract_raw_features_from_model()\n",
        "        raw_test_features = test_feature_extractor.feature_dict[\"Final Layer\"]\n",
        "        del test_feature_extractor\n",
        "\n",
        "        # Transform test features\n",
        "        pca_transformed_test_features = pca.transform(torch.tensor(raw_test_features).flatten(1).numpy())\n",
        "        del raw_test_features\n",
        "        \n",
        "        # Predict\n",
        "        lh_predictions = lh_lin_reg.predict(pca_transformed_test_features)\n",
        "        rh_predictions = rh_lin_reg.predict(pca_transformed_test_features)\n",
        "        return lh_predictions, rh_predictions\n",
        "\n",
        "class CLIPTextFinalLayerPCA100LinearRegression(SubmissionProcedure):\n",
        "    def __init__(self):\n",
        "        self.description = \"CLIP Text Model, Final Layer, PCA 100, Linear Regression, Second test submission.\"\n",
        "\n",
        "    def run(self, subject: Subject) -> np.ndarray:\n",
        "        \"\"\"Run the model on a subject.\"\"\"\n",
        "        # Prepare data\n",
        "        subject.create_dataloaders(processor=processor, batch_size=300)\n",
        "        subject.load_neural_data()\n",
        "        train_txt_dataloader = subject.train_txt_dataloader\n",
        "        test_txt_dataloader = subject.test_txt_dataloader\n",
        "        lh_fmri = subject.lh_fmri\n",
        "        rh_fmri = subject.rh_fmri\n",
        "        del subject # free up memory\n",
        "\n",
        "        # Prepare feature extractor\n",
        "        train_feature_extractor = CLIPFeatureExtractor(idxs=[], last_hidden_layer=True, model=txt_model, dataloader=train_txt_dataloader)\n",
        "        test_feature_extractor = CLIPFeatureExtractor(idxs=[], last_hidden_layer=True, model=txt_model, dataloader=test_txt_dataloader)\n",
        "\n",
        "        # Extract features\n",
        "        train_feature_extractor.extract_raw_features_from_model()\n",
        "        raw_train_features = train_feature_extractor.feature_dict[\"Final Layer\"]\n",
        "        del train_feature_extractor\n",
        "\n",
        "        # Fit PCA\n",
        "        pca = PCA(n_components=100)\n",
        "        pca_transformed_train_features = pca.fit_transform(torch.tensor(raw_train_features).flatten(1).numpy())\n",
        "        del raw_train_features\n",
        "\n",
        "        # Fit linear regression\n",
        "        lh_lin_reg = LinearRegression().fit(pca_transformed_train_features, lh_fmri)\n",
        "        rh_lin_reg = LinearRegression().fit(pca_transformed_train_features, rh_fmri)\n",
        "        del pca_transformed_train_features\n",
        "\n",
        "        # Extract test features\n",
        "        test_feature_extractor.extract_raw_features_from_model()\n",
        "        raw_test_features = test_feature_extractor.feature_dict[\"Final Layer\"]\n",
        "        del test_feature_extractor\n",
        "\n",
        "        # Transform test features\n",
        "        pca_transformed_test_features = pca.transform(torch.tensor(raw_test_features).flatten(1).numpy())\n",
        "        del raw_test_features\n",
        "        \n",
        "        # Predict\n",
        "        lh_predictions = lh_lin_reg.predict(pca_transformed_test_features)\n",
        "        rh_predictions = rh_lin_reg.predict(pca_transformed_test_features)\n",
        "        return lh_predictions, rh_predictions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Executing First and Second Submission**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUvVXR_iRePv"
      },
      "outputs": [],
      "source": [
        "raise Exception(\"This code is not meant to be run. It is only meant to be used as a reference for the code used to generate the submission.\")\n",
        "\n",
        "# First test submission\n",
        "subjects = [Subject(f\"subj0{i}\") for i in range(1, 9)]\n",
        "CreateSubmission(subjects, procedure=CLIPVisionFinalLayerPCA100LinearRegression()).run()\n",
        "\n",
        "# Second test submission\n",
        "CreateSubmission(subjects, procedure=CLIPTextFinalLayerPCA100LinearRegression()).run()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8qINLO7OEQw-"
      },
      "source": [
        "### **Third Submission - Vision Only Layer 7 PCA 200**\n",
        "\n",
        "For our third submission we used the 8-fold cross validation described above for the [CLIP Vision Model, PCA200, Linear Regression](#cv_vis_pca200_linreg) which indicated that layer 7 might be the best performing feature space. This submission resulted in our second highest score (49.318)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CLIPVisionLayer7PCA200LinearRegression(SubmissionProcedure):\n",
        "    def __init__(self):\n",
        "        self.description = \"CLIP Vision Model, Transformer Layer 7, PCA 200, Linear Regression, Determined by 8-fold cross validation against all other layers.\"\n",
        "\n",
        "    def run(self, subject: Subject) -> np.ndarray:\n",
        "        \"\"\"Run the model on a subject.\"\"\"\n",
        "        # Prepare data\n",
        "        subject.create_dataloaders(processor=processor, batch_size=300)\n",
        "        subject.load_neural_data()\n",
        "        train_img_dataloader = subject.train_img_dataloader\n",
        "        test_img_dataloader = subject.test_img_dataloader\n",
        "        lh_fmri = subject.lh_fmri\n",
        "        rh_fmri = subject.rh_fmri\n",
        "        del subject # free up memory\n",
        "\n",
        "        # Prepare feature extractor\n",
        "        train_feature_extractor = CLIPFeatureExtractor(idxs=[7], last_hidden_layer=False, model=vis_model, dataloader=train_img_dataloader)\n",
        "        test_feature_extractor = CLIPFeatureExtractor(idxs=[7], last_hidden_layer=False, model=vis_model, dataloader=test_img_dataloader)\n",
        "\n",
        "        # Extract features\n",
        "        train_feature_extractor.extract_raw_features_from_model()\n",
        "        raw_train_features = train_feature_extractor.feature_dict[\"Transformer Layer 7\"]\n",
        "        del train_feature_extractor\n",
        "\n",
        "        # Fit PCA\n",
        "        pca = PCA(n_components=200)\n",
        "        pca_transformed_train_features = pca.fit_transform(torch.tensor(raw_train_features).flatten(1).numpy())\n",
        "        del raw_train_features\n",
        "\n",
        "        # Fit linear regression\n",
        "        lh_lin_reg = LinearRegression().fit(pca_transformed_train_features, lh_fmri)\n",
        "        rh_lin_reg = LinearRegression().fit(pca_transformed_train_features, rh_fmri)\n",
        "        del pca_transformed_train_features\n",
        "\n",
        "        # Extract test features\n",
        "        test_feature_extractor.extract_raw_features_from_model()\n",
        "        raw_test_features = test_feature_extractor.feature_dict[\"Transformer Layer 7\"]\n",
        "        del test_feature_extractor\n",
        "\n",
        "        # Transform test features\n",
        "        pca_transformed_test_features = pca.transform(torch.tensor(raw_test_features).flatten(1).numpy())\n",
        "        del raw_test_features\n",
        "        \n",
        "        # Predict\n",
        "        lh_predictions = lh_lin_reg.predict(pca_transformed_test_features)\n",
        "        rh_predictions = rh_lin_reg.predict(pca_transformed_test_features)\n",
        "        return lh_predictions, rh_predictions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Executing Third Submission**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98uW4lGaDnNX"
      },
      "outputs": [],
      "source": [
        "raise Exception(\"This code is not meant to be run. It is only meant to be used as a reference for the code used to generate the submission.\")\n",
        "\n",
        "# Third submission\n",
        "subjects = [Subject(f\"subj0{i}\") for i in range(1, 9)]\n",
        "CreateSubmission(subjects, procedure=CLIPVisionLayer7PCA200LinearRegression()).run()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "J1y1xP5KEdRL"
      },
      "source": [
        "### **Fourth Submission - Vision Only Layer 4 and 8 Combined PCA 200**\n",
        "\n",
        "For our fourth submission we used the 8-fold cross validation described above for the [CLIP Vision Model, Layer 4 + 8, Normal & Combined PCA200, Linear Regression](#cv_vis4&8_pca200_linreg). This submission resulted in our highest score to date (50.541)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CLIPVisionL4L8PCA200LinearRegression(SubmissionProcedure):\n",
        "    def __init__(self):\n",
        "        self.description = \"CLIP Vision Model, Transformer Layer 4 + Transformer Layer 8, PCA 200, Linear Regression, First test submission.\"\n",
        "\n",
        "    def run(self, subject: Subject) -> np.ndarray:\n",
        "        \"\"\"Run the model on a subject.\"\"\"\n",
        "        # Prepare data\n",
        "        subject.create_dataloaders(processor=processor, batch_size=400)\n",
        "        subject.load_neural_data()\n",
        "        train_img_dataloader = subject.train_img_dataloader\n",
        "        test_img_dataloader = subject.test_img_dataloader\n",
        "        lh_fmri = subject.lh_fmri\n",
        "        rh_fmri = subject.rh_fmri\n",
        "        del subject # free up memory\n",
        "\n",
        "        # Prepare feature extractor\n",
        "        train_feature_extractor = CLIPFeatureExtractor(idxs=[4,8], last_hidden_layer=False, model=vis_model, dataloader=train_img_dataloader)\n",
        "        test_feature_extractor = CLIPFeatureExtractor(idxs=[4,8], last_hidden_layer=False, model=vis_model, dataloader=test_img_dataloader)\n",
        "\n",
        "        # Extract features\n",
        "        train_feature_extractor.extract_raw_features_from_model()\n",
        "        raw_train_features = train_feature_extractor.feature_dict[\"Transformer Layer 4\"]\n",
        "        raw_train_features = np.hstack([raw_train_features, train_feature_extractor.feature_dict[\"Transformer Layer 8\"]])\n",
        "        del train_feature_extractor\n",
        "\n",
        "        # Fit PCA\n",
        "        pca = PCA(n_components=200)\n",
        "        pca_transformed_train_features = pca.fit_transform(torch.tensor(raw_train_features).flatten(1).numpy())\n",
        "        del raw_train_features\n",
        "\n",
        "        # Fit linear regression\n",
        "        lh_lin_reg = LinearRegression().fit(pca_transformed_train_features, lh_fmri)\n",
        "        rh_lin_reg = LinearRegression().fit(pca_transformed_train_features, rh_fmri)\n",
        "        del pca_transformed_train_features\n",
        "\n",
        "        # Extract test features\n",
        "        test_feature_extractor.extract_raw_features_from_model()\n",
        "        raw_test_features = test_feature_extractor.feature_dict[\"Transformer Layer 4\"]\n",
        "        raw_test_features = np.hstack([raw_test_features, test_feature_extractor.feature_dict[\"Transformer Layer 8\"]])\n",
        "        del test_feature_extractor\n",
        "\n",
        "        # Transform test features\n",
        "        pca_transformed_test_features = pca.transform(torch.tensor(raw_test_features).flatten(1).numpy())\n",
        "        del raw_test_features\n",
        "        \n",
        "        # Predict\n",
        "        lh_predictions = lh_lin_reg.predict(pca_transformed_test_features)\n",
        "        rh_predictions = rh_lin_reg.predict(pca_transformed_test_features)\n",
        "        return lh_predictions, rh_predictions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Executing Fourth Submission**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psiHe8pbDoyc"
      },
      "outputs": [],
      "source": [
        "raise Exception(\"This code is not meant to be run. It is only meant to be used as a reference for the code used to generate the submission.\")\n",
        "\n",
        "# Fourth submission\n",
        "subjects = [Subject(f\"subj0{i}\") for i in range(1, 9)]\n",
        "CreateSubmission(subjects, procedure=CLIPVisionL4L8PCA200LinearRegression()).run()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "9nJ8S9FFhbVC",
        "Pla4gW1pwvZU",
        "mtZW44DBNXcp",
        "tUnPfbp3rCOP",
        "faFr51PBrqmO",
        "MbjZCFo3r9kF",
        "sM9oO8bsRIem",
        "H0mjWJ7mQWfO",
        "DNyqKJcNfWnl",
        "mCzak82XzxRt",
        "2Nu-JvcZEHaI",
        "8qINLO7OEQw-"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "001b02ca3dc74604a9c22836bae59597": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "051cfcdd831c4ffa9d7dd733f15a8c13": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06a15544d53644499336eab320c8aa95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0857f1e9c73a4166847cd60c33e7cf80": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08f9891abc1944c8b05d6c2870ba9a73": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13088f0d783b44bea5c28ab181b4b87e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15a368b20c464bdd812fb4bc9a1053e1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d63d31e02314f70b978d320c12e63ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c2290a3a2ab49c5a275a49fc3699b92",
              "IPY_MODEL_5a35a89a46ef475bba9e3ce5c7f318a7",
              "IPY_MODEL_a4fc7e2a1dcf4e9bbd942f91d7957192"
            ],
            "layout": "IPY_MODEL_dc8ddff14c15407d951393db93f29641"
          }
        },
        "27862ec0aa8d44cdaec17fd1805576ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c1fd69c69f14eafa46aaf7c6af2f56a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_912d934e59fc49ebab1010a7058ea73d",
            "placeholder": "​",
            "style": "IPY_MODEL_9bf0cd86aa0d44258b1852b6042dbe67",
            "value": "Downloading (…)olve/main/vocab.json: 100%"
          }
        },
        "2e05a00d6d84442baabd06f4233a099b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30bb378acad54c34b78519adb15b7f4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "334f81744357430baa4b414cc46f5a44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59981536dd814077b35c73f8a5627148",
            "placeholder": "​",
            "style": "IPY_MODEL_27862ec0aa8d44cdaec17fd1805576ca",
            "value": " 605M/605M [00:01&lt;00:00, 313MB/s]"
          }
        },
        "33ee0fe1762a48129537562d4f8b1a9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37d56c9464504d2fa7a9c265e2e0072e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39d71e707edb4c6a897f79a509bf4a59": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b5a44d407a6419eade6f5aed2269916": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13088f0d783b44bea5c28ab181b4b87e",
            "placeholder": "​",
            "style": "IPY_MODEL_fc8d05a96d1f46d4bb9d027cdb07f92c",
            "value": " 525k/525k [00:00&lt;00:00, 21.4MB/s]"
          }
        },
        "3d9dcf4c7889486a95d2d3e7ed57da6b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41d8638ec4864ccdb7ac02f1927cf114": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9aa279ff9ed245ea86a8e7a602e09d0d",
            "placeholder": "​",
            "style": "IPY_MODEL_500f199f21f543ad94a292c126340ea5",
            "value": " 4.19k/4.19k [00:00&lt;00:00, 219kB/s]"
          }
        },
        "44d5cbc41774425f8c3af4feb7874973": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4775615048f44d9d843ab443cf83c8cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_001b02ca3dc74604a9c22836bae59597",
            "max": 605247071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_37d56c9464504d2fa7a9c265e2e0072e",
            "value": 605247071
          }
        },
        "4e3453eb91074e52b80f43d739e0c9fb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "500f199f21f543ad94a292c126340ea5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5249f98d892743ffb30aa57b2d1d9ef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_faaa6d3b623f4216b4fceecdd8cd5160",
              "IPY_MODEL_4775615048f44d9d843ab443cf83c8cb",
              "IPY_MODEL_334f81744357430baa4b414cc46f5a44"
            ],
            "layout": "IPY_MODEL_051cfcdd831c4ffa9d7dd733f15a8c13"
          }
        },
        "52cc8ea18aa54c97bebe7f6c285fb9a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e05a00d6d84442baabd06f4233a099b",
            "placeholder": "​",
            "style": "IPY_MODEL_f24601e7d8484bf1a7a7db6b62e91ada",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "5606173f54044918819a30f2df91023a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59981536dd814077b35c73f8a5627148": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a35a89a46ef475bba9e3ce5c7f318a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e3453eb91074e52b80f43d739e0c9fb",
            "max": 2224041,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_76aae9de81e2493d8e5b04fb45009d43",
            "value": 2224041
          }
        },
        "5c370cb2109d4fc5adcd11dbe37bc01a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6902a68963db4d9aa477ba5af56c39b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a162c415af8e400fb38f19769128eca4",
            "placeholder": "​",
            "style": "IPY_MODEL_ea9ea6ac747d4c428f601f2f465fe796",
            "value": " 389/389 [00:00&lt;00:00, 16.6kB/s]"
          }
        },
        "69dcec1406a94c60aec85254920ec193": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c2290a3a2ab49c5a275a49fc3699b92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dce48adf5a7f414c800c2b8fdb2de61b",
            "placeholder": "​",
            "style": "IPY_MODEL_c3d7716a99f04ce99c7d8a83f88145b9",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "705443d223dd4f4499578ccc9e70f1dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39d71e707edb4c6a897f79a509bf4a59",
            "placeholder": "​",
            "style": "IPY_MODEL_30bb378acad54c34b78519adb15b7f4c",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "7231997bb5784f02b80755fc040d6cef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15a368b20c464bdd812fb4bc9a1053e1",
            "placeholder": "​",
            "style": "IPY_MODEL_06a15544d53644499336eab320c8aa95",
            "value": "Downloading (…)rocessor_config.json: 100%"
          }
        },
        "72bdbb183bee4b34b9c0dc01e357ce55": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76aae9de81e2493d8e5b04fb45009d43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "771860af34e34ad49b205e7abf30dc3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec989ac6694a40a0affe8944f2914683",
            "placeholder": "​",
            "style": "IPY_MODEL_8852719cd30a4bac9248214657ee1c3b",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "79bd52f8de9848aaad6a6bcd91e66c14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_771860af34e34ad49b205e7abf30dc3a",
              "IPY_MODEL_b7e26a6e7bd246c2aee46a9754c492e9",
              "IPY_MODEL_c4abd9365ec64c98ad24735c2c5dbda0"
            ],
            "layout": "IPY_MODEL_72bdbb183bee4b34b9c0dc01e357ce55"
          }
        },
        "7c9508713a7845e2bd6b8ea2e3787334": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8852719cd30a4bac9248214657ee1c3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88b3fe4e4423438f86130298b4a0864c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "912d934e59fc49ebab1010a7058ea73d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "935c68ca9cd14009b8e577ad12e7cd53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "943c019d75f34694b446d9655f85e0b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "954619d623b4404d8687f901badc96d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "972cdd7daaef4627bdf11577aaed5d5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69dcec1406a94c60aec85254920ec193",
            "placeholder": "​",
            "style": "IPY_MODEL_33ee0fe1762a48129537562d4f8b1a9f",
            "value": "Downloading (…)olve/main/merges.txt: 100%"
          }
        },
        "9998add0831444c3b4f06e1e6cf59f64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9aa279ff9ed245ea86a8e7a602e09d0d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ad55ab2e2ea42ce87361f48c1d005b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7231997bb5784f02b80755fc040d6cef",
              "IPY_MODEL_bf8c86299d9c4fbe82949ad0d9ec95c8",
              "IPY_MODEL_b54a83fc541547bb9bd379e3955fb9f7"
            ],
            "layout": "IPY_MODEL_a0bd1c6d972446a3be08b15c4db92c9f"
          }
        },
        "9bf0cd86aa0d44258b1852b6042dbe67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0bd1c6d972446a3be08b15c4db92c9f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a162c415af8e400fb38f19769128eca4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4fc7e2a1dcf4e9bbd942f91d7957192": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5606173f54044918819a30f2df91023a",
            "placeholder": "​",
            "style": "IPY_MODEL_e00b499de982455ba2a1f3a3d0a84e57",
            "value": " 2.22M/2.22M [00:00&lt;00:00, 5.37MB/s]"
          }
        },
        "ad4d456dd48842a0840466b009fa14fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_705443d223dd4f4499578ccc9e70f1dc",
              "IPY_MODEL_c0700fa7799b4aed83bda55ef1bdfb1f",
              "IPY_MODEL_41d8638ec4864ccdb7ac02f1927cf114"
            ],
            "layout": "IPY_MODEL_fbe7063254014f5291997bf29e6c348b"
          }
        },
        "b54a83fc541547bb9bd379e3955fb9f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e679fac3d9d74ac0965334c0351407db",
            "placeholder": "​",
            "style": "IPY_MODEL_88b3fe4e4423438f86130298b4a0864c",
            "value": " 316/316 [00:00&lt;00:00, 19.8kB/s]"
          }
        },
        "b7e26a6e7bd246c2aee46a9754c492e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0857f1e9c73a4166847cd60c33e7cf80",
            "max": 568,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d4b46c23f306406691e94acdc848141c",
            "value": 568
          }
        },
        "b8b9dd9e331944d4b863d98d78f44bca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_972cdd7daaef4627bdf11577aaed5d5c",
              "IPY_MODEL_f39bc82e387748b8867634e7d0188fcb",
              "IPY_MODEL_3b5a44d407a6419eade6f5aed2269916"
            ],
            "layout": "IPY_MODEL_f221ca976f35450ea60590381bbecb31"
          }
        },
        "bbcfcae1f7ff4ab18314876e73761a86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c1fd69c69f14eafa46aaf7c6af2f56a",
              "IPY_MODEL_d5068b71be774d82b9b9f9bdfbace393",
              "IPY_MODEL_c31617c1198848fc8dc37e02e5f6f021"
            ],
            "layout": "IPY_MODEL_943c019d75f34694b446d9655f85e0b3"
          }
        },
        "bea8b402738341b18efb86863774081e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf8c86299d9c4fbe82949ad0d9ec95c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44d5cbc41774425f8c3af4feb7874973",
            "max": 316,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9998add0831444c3b4f06e1e6cf59f64",
            "value": 316
          }
        },
        "c0700fa7799b4aed83bda55ef1bdfb1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c9508713a7845e2bd6b8ea2e3787334",
            "max": 4186,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fde355ae288d4902a1e87bc0a21e4845",
            "value": 4186
          }
        },
        "c31617c1198848fc8dc37e02e5f6f021": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d67a25caaa1348fb957cc3c45a52f7f6",
            "placeholder": "​",
            "style": "IPY_MODEL_935c68ca9cd14009b8e577ad12e7cd53",
            "value": " 862k/862k [00:00&lt;00:00, 17.2MB/s]"
          }
        },
        "c3d7716a99f04ce99c7d8a83f88145b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c49047ca0d054b68aa2a658930bf9b09": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4abd9365ec64c98ad24735c2c5dbda0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bea8b402738341b18efb86863774081e",
            "placeholder": "​",
            "style": "IPY_MODEL_df612418202f47cba6e9419dd8a9a5f4",
            "value": " 568/568 [00:00&lt;00:00, 28.4kB/s]"
          }
        },
        "c805ccecb74048308da68c22791d8b6b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c908c38eb9534f76ab0f4a23dec936aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d9dcf4c7889486a95d2d3e7ed57da6b",
            "max": 389,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_db44e6edf3c7451b83c1d48bb420fd8f",
            "value": 389
          }
        },
        "d4b46c23f306406691e94acdc848141c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5068b71be774d82b9b9f9bdfbace393": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08f9891abc1944c8b05d6c2870ba9a73",
            "max": 862328,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f480668dc8a942baa8415814ef878647",
            "value": 862328
          }
        },
        "d67a25caaa1348fb957cc3c45a52f7f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dac1f64b0a4c42d5971a8f2a2290e565": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_52cc8ea18aa54c97bebe7f6c285fb9a1",
              "IPY_MODEL_c908c38eb9534f76ab0f4a23dec936aa",
              "IPY_MODEL_6902a68963db4d9aa477ba5af56c39b6"
            ],
            "layout": "IPY_MODEL_c49047ca0d054b68aa2a658930bf9b09"
          }
        },
        "db44e6edf3c7451b83c1d48bb420fd8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc8ddff14c15407d951393db93f29641": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dce48adf5a7f414c800c2b8fdb2de61b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df612418202f47cba6e9419dd8a9a5f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e00b499de982455ba2a1f3a3d0a84e57": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e679fac3d9d74ac0965334c0351407db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea9ea6ac747d4c428f601f2f465fe796": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec989ac6694a40a0affe8944f2914683": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f221ca976f35450ea60590381bbecb31": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f24601e7d8484bf1a7a7db6b62e91ada": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f39bc82e387748b8867634e7d0188fcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c805ccecb74048308da68c22791d8b6b",
            "max": 524657,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_954619d623b4404d8687f901badc96d5",
            "value": 524657
          }
        },
        "f3a0e1fe32864099ab696f37e50a3a3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f480668dc8a942baa8415814ef878647": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "faaa6d3b623f4216b4fceecdd8cd5160": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c370cb2109d4fc5adcd11dbe37bc01a",
            "placeholder": "​",
            "style": "IPY_MODEL_f3a0e1fe32864099ab696f37e50a3a3e",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "fbe7063254014f5291997bf29e6c348b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc8d05a96d1f46d4bb9d027cdb07f92c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fde355ae288d4902a1e87bc0a21e4845": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
