{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required packages\n",
    "from transformers import AutoProcessor, CLIPModel, CLIPTextModel, CLIPVisionModel\n",
    "import torch\n",
    "import torch_directml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from PIL import Image\n",
    "import glob\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.decomposition import IncrementalPCA \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr as corr\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup cuda device\n",
    "AMD = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if AMD:\n",
    "    device = torch_directml.device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Models\n",
    "vision_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "text_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "vision_model.to(device)\n",
    "text_model.to(device)\n",
    "vision_model.eval()\n",
    "text_model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes and functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Classes for Batching\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_list, processor):\n",
    "        self.image_list = image_list\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_list[idx])\n",
    "        image = self.processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "        return image[\"pixel_values\"].squeeze()\n",
    "\n",
    "class TextDateset(Dataset):\n",
    "    def __init__(self, text, processor):\n",
    "        self.text = processor(text=text, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text[\"input_ids\"][idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image_list_and_captions():\n",
    "    # Used for testing only\n",
    "    # Loads images and captions from local data folder\n",
    "    image_list = glob.glob('data/training_images/*.png')\n",
    "    captions = []\n",
    "    df = pd.read_csv('data/algonauts_2023_caption_data.csv')\n",
    "    for filename in image_list:\n",
    "        im_id = int(filename.split('/')[-1].split('.')[0].split('nsd-')[-1])\n",
    "        captions.append(df[df[\"nsdId\"] == im_id][\"caption\"].values[0])\n",
    "    return image_list, captions\n",
    "\n",
    "def prepare_data(image_list, captions, batch_size):\n",
    "    # Creates dataloaders for images and captions\n",
    "    n = len(image_list)\n",
    "    batches = n // batch_size\n",
    "    text_dataset = TextDateset(captions, processor)\n",
    "    text_dataloader = DataLoader(text_dataset, batch_size=batch_size, shuffle=False)\n",
    "    vision_dataset = ImageDataset(image_list, processor)\n",
    "    vision_dataloader = DataLoader(vision_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return text_dataloader, vision_dataloader, batches\n",
    "\n",
    "def prepare_layer_dict(keys = [\"Embedding Layer\", \n",
    "                                \"Layer 1\", \n",
    "                                \"Layer 2\", \n",
    "                                \"Layer 3\", \n",
    "                                \"Layer 4\", \n",
    "                                \"Layer 5\", \n",
    "                                \"Layer 6\", \n",
    "                                \"Layer 7\", \n",
    "                                \"Layer 8\", \n",
    "                                \"Layer 9\", \n",
    "                                \"Layer 10\", \n",
    "                                \"Layer 11\", \n",
    "                                \"Layer 12\", \n",
    "                                \"Final Layer\"]):\n",
    "    # These dicts are used to store features and pca models\n",
    "    layer_dict = {key: None for key in keys}\n",
    "    return layer_dict\n",
    "\n",
    "def plot_layer_activations(text_features: dict, vision_features: dict):\n",
    "    fig, axes = plt.subplots(14, 2, figsize=(10, 25))\n",
    "    for i, layer in enumerate(text_features):\n",
    "        axes[i, 0].imshow(text_features[layer].mean(1).numpy())\n",
    "        axes[i, 0].set_title(f\"Text Model: {layer}\")\n",
    "    for i, layer in enumerate(vision_features):\n",
    "        axes[i, 1].imshow(vision_features[layer].mean(1).numpy())\n",
    "        axes[i, 1].set_title(f\"Vision Model: {layer}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_partial_fit(i, model_output, pca_dict, batch_size, components):\n",
    "    # Fits PCA model to features\n",
    "    keys = list(pca_dict.keys())  \n",
    "    assert len(model_output.hidden_states) + 1 == len(keys), \"Number of layers and keys do not match\"        \n",
    "    if i == 0:\n",
    "        for j in range(len(keys)):\n",
    "            pca_dict[keys[j]] = IncrementalPCA(n_components=components, batch_size=batch_size)\n",
    "    for j in range(len(keys)-1):\n",
    "        pca_dict[keys[j]].partial_fit(model_output.hidden_states[j].flatten(1).detach().cpu().numpy())\n",
    "    pca_dict[keys[-1]].partial_fit(model_output.last_hidden_state.flatten(1).detach().cpu().numpy())\n",
    "    return pca_dict\n",
    "\n",
    "def pca_transform_features(pca_dict, key, hidden_state):\n",
    "    # Transforms features using PCA model\n",
    "    assert key in pca_dict.keys(), \"Key not in pca_dict\"\n",
    "    return pca_dict[key].transform(hidden_state.flatten(1).detach().cpu().numpy())\n",
    "\n",
    "def fit_pca(text_model: CLIPTextModel,\n",
    "            vision_model: CLIPVisionModel,\n",
    "            text_dataloader: DataLoader,\n",
    "            vision_dataloader: DataLoader,\n",
    "            pca_components: int):\n",
    "    # Prepare pca dicts\n",
    "    text_pca = prepare_layer_dict()\n",
    "    vision_pca = prepare_layer_dict()\n",
    "\n",
    "    # Fit pca\n",
    "    batch_size = vision_dataloader.batch_size\n",
    "    batches = len(vision_dataloader)\n",
    "    if pca_components > batch_size:\n",
    "        pca_components = batch_size\n",
    "    with torch.no_grad():\n",
    "        for i, (image_batch, text_batch) in enumerate(zip(vision_dataloader, text_dataloader)):\n",
    "            batch_time = time.time()\n",
    "            print(f\"Processing batch {i+1}/{batches}...\")\n",
    "            # move batch to GPU\n",
    "            image_batch = image_batch.to(device)\n",
    "            text_batch = text_batch.to(device) \n",
    "            # Running Text Model\n",
    "            txt_output = text_model(text_batch, output_hidden_states=True)\n",
    "            text_pca = pca_partial_fit(i, txt_output, text_pca, batch_size = batch_size, components = pca_components)\n",
    "            # Running Vision Model\n",
    "            vis_output = vision_model(image_batch, output_hidden_states=True)\n",
    "            vision_pca = pca_partial_fit(i, vis_output, vision_pca, batch_size = batch_size, components = pca_components)\n",
    "            print(f\"Batch {i+1}/{batches} took {time.time() - batch_time:.2f} seconds\")\n",
    "            print(f\"Predicted time remaining: {(time.time() - batch_time) * (batches - i) / 60:.2f} minutes\")\n",
    "    print(text_pca[\"Embedding Layer\"].n_components_)\n",
    "    print(vision_pca[\"Embedding Layer\"].n_components_)\n",
    "    return text_pca, vision_pca"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_features(i, model_output, feature_dict):\n",
    "    keys = list(feature_dict.keys())  \n",
    "    assert len(model_output.hidden_states) + 1 == len(keys), \"Number of layers and keys do not match\"        \n",
    "    if i == 0:\n",
    "        for j in range(len(keys)-1):\n",
    "            feature_dict[keys[j]] = model_output.hidden_states[j]\n",
    "        feature_dict[keys[-1]] = model_output.last_hidden_state\n",
    "    else:\n",
    "        for j in range(len(keys)-1):\n",
    "            feature_dict[keys[j]] = torch.cat((feature_dict[keys[j]], model_output.hidden_states[j]), dim=0)\n",
    "        feature_dict[keys[-1]] = torch.cat((feature_dict[keys[-1]], model_output.last_hidden_state), dim=0)\n",
    "    return feature_dict\n",
    "\n",
    "def concat_features_pca_transform(i, model_output, feature_dict, pca_dict):\n",
    "    keys = list(feature_dict.keys())  \n",
    "    assert len(model_output.hidden_states) + 1 == len(keys), \"Number of layers and feature dict keys do not match\"\n",
    "    assert len(model_output.hidden_states) + 1 == len(pca_dict.keys()), \"Number of layers and pca dict keys do not match\"\n",
    "    if i == 0:\n",
    "        for j in range(len(keys)-1):\n",
    "            hidden_state = model_output.hidden_states[j]\n",
    "            key = keys[j]\n",
    "            feature_dict[key] = pca_transform_features(pca_dict, key, hidden_state)\n",
    "        hidden_state = model_output.last_hidden_state\n",
    "        key = keys[-1]\n",
    "        feature_dict[key] = pca_transform_features(pca_dict, key, hidden_state)\n",
    "    else:\n",
    "        for j in range(len(keys)-1):\n",
    "            hidden_state = model_output.hidden_states[j]\n",
    "            key = keys[j]\n",
    "            feature_dict[key] = np.concatenate((feature_dict[key], pca_transform_features(pca_dict, key, hidden_state)), 0)\n",
    "        hidden_state = model_output.last_hidden_state\n",
    "        key = keys[-1]\n",
    "        feature_dict[key] = np.concatenate((feature_dict[key], pca_transform_features(pca_dict, key, hidden_state)), 0)\n",
    "    return feature_dict\n",
    "\n",
    "def extract_raw_features(text_dataloader: DataLoader,\n",
    "                          vision_dataloader: DataLoader,\n",
    "                          text_model: CLIPTextModel,\n",
    "                          vision_model: CLIPVisionModel):\n",
    "    # Uses the standard concat_features function to extract the raw hidden layer features\n",
    "\n",
    "    # Prepare feature dicts\n",
    "    text_features = prepare_layer_dict()\n",
    "    vision_features = prepare_layer_dict() \n",
    "\n",
    "    # Extract features\n",
    "    batches = len(vision_dataloader)\n",
    "    with torch.no_grad():\n",
    "        for i, (image_batch, text_batch) in enumerate(zip(vision_dataloader, text_dataloader)):\n",
    "            batch_time = time.time()\n",
    "            print(f\"Processing batch {i+1}/{batches}...\")\n",
    "            # move batch to GPU\n",
    "            image_batch = image_batch.to(device)\n",
    "            text_batch = text_batch.to(device) \n",
    "            # Running Text Model\n",
    "            txt_output = text_model(text_batch, output_hidden_states=True)\n",
    "            text_features = concat_features(i, txt_output, text_features)\n",
    "            # Running Vision Model\n",
    "            vis_output = vision_model(image_batch, output_hidden_states=True)\n",
    "            vision_features = concat_features(i, vis_output, vision_features)\n",
    "            print(f\"Batch {i+1}/{batches} took {time.time() - batch_time:.2f} seconds\")\n",
    "            print(f\"Predicted time remaining: {(time.time() - batch_time) * (batches - i) / 60:.2f} minutes\")\n",
    "    print(text_features[list(text_features.keys())[0]].shape)\n",
    "    print(vision_features[list(vision_features.keys())[0]].shape)\n",
    "    return text_features, vision_features\n",
    "\n",
    "def extract_pca_features(text_pca: dict, \n",
    "                         vision_pca: dict, \n",
    "                         text_dataloader: DataLoader, \n",
    "                         vision_dataloader: DataLoader, \n",
    "                         text_model: CLIPTextModel, \n",
    "                         vision_model: CLIPVisionModel):\n",
    "    # Uses the concat_features_pca_transform function to extract the pca transformed hidden layer features\n",
    "\n",
    "    # Prepare pca transformed feature dicts\n",
    "    text_features_pca_transf = prepare_layer_dict()\n",
    "    vision_features_pca_transf = prepare_layer_dict() \n",
    "\n",
    "    # Extract features\n",
    "    batches = len(vision_dataloader)\n",
    "    with torch.no_grad():\n",
    "        for i, (image_batch, text_batch) in enumerate(zip(vision_dataloader, text_dataloader)):\n",
    "            batch_time = time.time()\n",
    "            print(f\"Processing batch {i+1}/{batches}...\")\n",
    "            # move batch to GPU\n",
    "            image_batch = image_batch.to(device)\n",
    "            text_batch = text_batch.to(device) \n",
    "            # Running Text Model\n",
    "            text_output = text_model(text_batch, output_hidden_states=True)\n",
    "            text_features_pca_transf = concat_features_pca_transform(i, text_output, text_features_pca_transf, text_pca)\n",
    "            # Running Vision Model\n",
    "            vision_output = vision_model(image_batch, output_hidden_states=True)\n",
    "            vision_features_pca_transf = concat_features_pca_transform(i, vision_output, vision_features_pca_transf, vision_pca)\n",
    "            print(f\"Batch {i+1}/{batches} took {time.time() - batch_time:.2f} seconds\")\n",
    "            print(f\"Predicted time remaining: {(time.time() - batch_time) * (batches - i) / 60:.2f} minutes\")\n",
    "    print(text_features_pca_transf[list(text_features_pca_transf.keys())[0]].shape)\n",
    "    print(vision_features_pca_transf[list(vision_features_pca_transf.keys())[0]].shape)\n",
    "    return text_features_pca_transf, vision_features_pca_transf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with some local data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "batch_size = 50\n",
    "image_list, captions = prepare_image_list_and_captions()\n",
    "text_dataloader, vision_dataloader, batches = prepare_data(image_list[:100], captions[:100], batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/2...\n",
      "Batch 1/2 took 0.25 seconds\n",
      "Predicted time remaining: 0.01 minutes\n",
      "Processing batch 2/2...\n",
      "Batch 2/2 took 0.29 seconds\n",
      "Predicted time remaining: 0.00 minutes\n",
      "torch.Size([100, 28, 512])\n",
      "torch.Size([100, 50, 768])\n",
      "Processing batch 1/2...\n",
      "Batch 1/2 took 2.80 seconds\n",
      "Predicted time remaining: 0.09 minutes\n",
      "Processing batch 2/2...\n",
      "Batch 2/2 took 11.19 seconds\n",
      "Predicted time remaining: 0.19 minutes\n",
      "50\n",
      "50\n",
      "Processing batch 1/2...\n",
      "Batch 1/2 took 1.02 seconds\n",
      "Predicted time remaining: 0.03 minutes\n",
      "Processing batch 2/2...\n",
      "Batch 2/2 took 0.78 seconds\n",
      "Predicted time remaining: 0.01 minutes\n",
      "(100, 50)\n",
      "(100, 50)\n"
     ]
    }
   ],
   "source": [
    "# Extract raw features\n",
    "text_features, vision_features = extract_raw_features(text_dataloader, vision_dataloader, text_model, vision_model)\n",
    "# Fit pca\n",
    "text_pca, vision_pca = fit_pca(text_model, vision_model, text_dataloader, vision_dataloader, pca_components = 50)\n",
    "# Extract pca transformed features\n",
    "text_features_pca_transf, vision_features_pca_transf = extract_pca_features(text_pca, vision_pca, text_dataloader, vision_dataloader, text_model, vision_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luke-\\AppData\\Local\\Temp\\ipykernel_29264\\4125893739.py:9: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  lh_fmri = lh_fmri[[image_ids]]\n",
      "C:\\Users\\luke-\\AppData\\Local\\Temp\\ipykernel_29264\\4125893739.py:10: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rh_fmri = rh_fmri[[image_ids]]\n"
     ]
    }
   ],
   "source": [
    "# Load Neural data\n",
    "lh_fmri = np.load('data/training_fmri/lh_training_fmri.npy')\n",
    "rh_fmri = np.load('data/training_fmri/rh_training_fmri.npy')\n",
    "# selecting only the images that were part of the test\n",
    "image_ids = []\n",
    "for filename in image_list[:100]:\n",
    "    im_id = int(filename.split('/')[-1].split('.')[0].split('_nsd-')[0].split(\"train-\")[1])-1\n",
    "    image_ids.append(im_id)\n",
    "lh_fmri = lh_fmri[[image_ids]]\n",
    "rh_fmri = rh_fmri[[image_ids]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in train and validation\n",
    "text_features_pca_transf_train = text_features_pca_transf[\"Final Layer\"][:80]\n",
    "text_features_pca_transf_val = text_features_pca_transf[\"Final Layer\"][80:]\n",
    "lh_fmri_train = lh_fmri[:80]\n",
    "lh_fmri_val = lh_fmri[80:]\n",
    "rh_fmri_train = rh_fmri[:80]\n",
    "rh_fmri_val = rh_fmri[80:] # some change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit linear regressions on the training data\n",
    "reg_lh = LinearRegression().fit(text_features_pca_transf_train, lh_fmri_train)\n",
    "reg_rh = LinearRegression().fit(text_features_pca_transf_train, rh_fmri_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use fitted linear regressions to predict the validation and test fMRI data\n",
    "lh_fmri_val_pred = reg_lh.predict(text_features_pca_transf_val)\n",
    "rh_fmri_val_pred = reg_rh.predict(text_features_pca_transf_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19004/19004 [00:00<00:00, 20609.70it/s]\n",
      "100%|██████████| 20544/20544 [00:01<00:00, 20124.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# Empty correlation array of shape: (LH vertices)\n",
    "lh_correlation = np.zeros(lh_fmri_val_pred.shape[1])\n",
    "# Correlate each predicted LH vertex with the corresponding ground truth vertex\n",
    "for v in tqdm(range(lh_fmri_val_pred.shape[1])):\n",
    "    lh_correlation[v] = corr(lh_fmri_val_pred[:,v], lh_fmri_val[:,v])[0]\n",
    "lh_median_correlation = [np.median(lh_correlation[r]) for r in range(len(lh_correlation))]\n",
    "\n",
    "# Empty correlation array of shape: (RH vertices)\n",
    "rh_correlation = np.zeros(rh_fmri_val_pred.shape[1])\n",
    "# Correlate each predicted RH vertex with the corresponding ground truth vertex\n",
    "for v in tqdm(range(rh_fmri_val_pred.shape[1])):\n",
    "    rh_correlation[v] = corr(rh_fmri_val_pred[:,v], rh_fmri_val[:,v])[0]\n",
    "rh_median_correlation = [np.median(rh_correlation[r]) for r in range(len(rh_correlation))]    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
