{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading packages, setting up gpu, loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luke-\\Desktop\\Python Repositories\\algonauts-2023\\.conda\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# required packages\n",
    "from transformers import AutoProcessor, CLIPTextModel, CLIPVisionModel, PreTrainedModel\n",
    "import torch\n",
    "import torch_directml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "import time\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.decomposition import IncrementalPCA, PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr as corr\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup cuda device\n",
    "AMD = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if AMD:\n",
    "    device = torch_directml.device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-base-patch32 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'visual_projection.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_projection.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.embeddings.position_embedding.weight', 'logit_scale', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at openai/clip-vit-base-patch32 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'visual_projection.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'logit_scale', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPTextModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining Models\n",
    "global vis_model, txt_model, processor\n",
    "vis_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "txt_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "vis_model.eval()\n",
    "txt_model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ImageDataset & TextDataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ImageDataset and TextDataset classes are used to create datasets for torch dataloaders.\n",
    "The ImageDataset is used for the images and the TextDataset for the captions of the coco images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Classes for Batching\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_list, processor):\n",
    "        self.image_list = image_list\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_list[idx])\n",
    "        image = self.processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "        return image[\"pixel_values\"].squeeze()\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, max_length, processor):\n",
    "        self.text = processor(text=text, return_tensors=\"pt\", padding=\"max_length\", max_length=max_length)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text[\"input_ids\"][idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subject Class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Subject class is initialized with a valid subject id (e.g., \"subj01\"). It stores all relevant paths and can load the data for the given subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subject:\n",
    "    \"\"\"Class to access all relevant data for a given subject\"\"\"\n",
    "    def __init__(self, subject=\"subj01\"):\n",
    "        assert subject in [\"subj01\", \"subj02\", \"subj03\", \"subj04\", \"subj05\", \"subj06\", \"subj07\", \"subj08\",], \"Invalid subject\"\n",
    "        self.subject = subject\n",
    "        self.data_dir = \"data/algonauts_2023_challenge_data\"\n",
    "        self.training_images_dir = f\"{self.data_dir}/{subject}/training_split/training_images\"\n",
    "        self.test_images_dir = f\"{self.data_dir}/{subject}/test_split/test_images\"\n",
    "        self.training_fmri_dir = f\"{self.data_dir}/{subject}/training_split/training_fmri\"\n",
    "        self.roi_masks_dir = f\"{self.data_dir}/{subject}/roi_masks\"\n",
    "        self.submission_dir = f\"algonauts_2023_challenge_submission\"\n",
    "        # Load these as needed\n",
    "        self.train_img_list = None\n",
    "        self.test_img_list = None\n",
    "        self.train_cap_list = None\n",
    "        self.test_cap_list = None\n",
    "        self.lh_fmri = None\n",
    "        self.rh_fmri = None\n",
    "        self.lh_roi_masks = None\n",
    "        self.rh_roi_masks = None\n",
    "        self.roi_name_maps = None\n",
    "        self.lh_challenge_rois = None\n",
    "        self.rh_challenge_rois = None\n",
    "        self.train_img_dataloader = None\n",
    "        self.test_img_dataloader = None\n",
    "        self.train_cap_dataloader = None\n",
    "        self.test_cap_dataloader = None            \n",
    "        \n",
    "    def load_image_paths(self) -> None:\n",
    "        \"\"\"Loads the image paths from the training and test directories\"\"\"\n",
    "        self.train_img_list = glob.glob(f\"{self.training_images_dir}/*.png\")\n",
    "        self.train_img_list.sort()\n",
    "        self.test_img_list = glob.glob(f\"{self.test_images_dir}/*.png\")\n",
    "        self.test_img_list.sort()\n",
    "        print(f\"Training images: {len(self.train_img_list)}\")\n",
    "        print(f\"Test images: {len(self.test_img_list)}\")\n",
    "\n",
    "    def load_captions(self) -> None:\n",
    "        \"\"\"Loads and matches the captions from the csv file\"\"\"\n",
    "        if self.train_img_list is None:\n",
    "            self.load_image_paths()\n",
    "        train_cap_file = pd.read_csv(f'{self.data_dir}/algonauts_2023_caption_data.csv')\n",
    "        img_match = [int(i[-9:-4]) for i in self.train_img_list]\n",
    "        self.train_cap_list = train_cap_file[(train_cap_file['subject'] == self.subject) & (train_cap_file['nsdId'].isin(img_match))]['caption'].tolist()\n",
    "        self.test_cap_list = train_cap_file[(train_cap_file['subject'] == self.subject) & (~train_cap_file['nsdId'].isin(img_match))]['caption'].tolist()\n",
    "        print(f\"Training captions: {len(self.train_cap_list)}\")\n",
    "        print(f\"Test captions: {len(self.test_cap_list)}\")\n",
    "    \n",
    "    def load_neural_data(self) -> None:\n",
    "        \"\"\"Loads the neural data from the .npy files\"\"\"\n",
    "        self.lh_fmri = np.load(f\"{self.training_fmri_dir}/lh_training_fmri.npy\")\n",
    "        self.rh_fmri = np.load(f\"{self.training_fmri_dir}/rh_training_fmri.npy\")\n",
    "        print(f\"Left hemisphere neural data loaded. Shape: {self.lh_fmri.shape}\")\n",
    "        print(f\"Right hemisphere neural data loaded. Shape: {self.rh_fmri.shape}\")\n",
    "\n",
    "    def create_dataloaders(self, processor, batch_size) -> None:\n",
    "        \"\"\"Creates the dataloaders for the images and captions\"\"\"\n",
    "        if self.train_img_list is None:\n",
    "            self.load_image_paths()\n",
    "        if self.train_cap_list is None:\n",
    "            self.load_captions()\n",
    "        max_caption_len = processor(text=self.train_cap_list + self.test_cap_list, return_tensors=\"pt\", padding=True)[\"input_ids\"].shape[1]   \n",
    "        train_txt_dataset = TextDataset(self.train_cap_list, max_caption_len, processor)\n",
    "        test_txt_dataset = TextDataset(self.test_cap_list, max_caption_len, processor)\n",
    "        train_img_dataset = ImageDataset(self.train_img_list, processor)\n",
    "        test_img_dataset = ImageDataset(self.test_img_list, processor)\n",
    "        self.train_img_dataloader = DataLoader(train_img_dataset, batch_size=batch_size, shuffle=False)\n",
    "        self.test_img_dataloader = DataLoader(test_img_dataset, batch_size=batch_size, shuffle=False)\n",
    "        self.train_txt_dataloader = DataLoader(train_txt_dataset, batch_size=batch_size, shuffle=False)\n",
    "        self.test_txt_dataloader = DataLoader(test_txt_dataset, batch_size=batch_size, shuffle=False)\n",
    "        print(f\"Train image dataloader: {len(self.train_img_dataloader)} batches\")\n",
    "        print(f\"Test image dataloader: {len(self.test_img_dataloader)} batches\")\n",
    "        print(f\"Train caption dataloader: {len(self.train_txt_dataloader)} batches\")\n",
    "        print(f\"Test caption dataloader: {len(self.test_txt_dataloader)} batches\")\n",
    "\n",
    "    def load_challenge_rois(self) -> None:\n",
    "        \"\"\"Loads the challenge rois from the .npy files\"\"\"\n",
    "        # Load the ROI classes mapping dictionaries\n",
    "        roi_mapping_files = ['mapping_prf-visualrois.npy', 'mapping_floc-bodies.npy',\n",
    "            'mapping_floc-faces.npy', 'mapping_floc-places.npy',\n",
    "            'mapping_floc-words.npy', 'mapping_streams.npy']\n",
    "        self.roi_name_maps = []\n",
    "        for r in roi_mapping_files:\n",
    "            self.roi_name_maps.append(np.load(f\"{self.roi_masks_dir}/{r}\", allow_pickle=True).item())\n",
    "\n",
    "        # Load the ROI brain surface maps\n",
    "        lh_challenge_roi_files = ['lh.prf-visualrois_challenge_space.npy',\n",
    "            'lh.floc-bodies_challenge_space.npy', 'lh.floc-faces_challenge_space.npy',\n",
    "            'lh.floc-places_challenge_space.npy', 'lh.floc-words_challenge_space.npy',\n",
    "            'lh.streams_challenge_space.npy']\n",
    "        rh_challenge_roi_files = ['rh.prf-visualrois_challenge_space.npy',\n",
    "            'rh.floc-bodies_challenge_space.npy', 'rh.floc-faces_challenge_space.npy',\n",
    "            'rh.floc-places_challenge_space.npy', 'rh.floc-words_challenge_space.npy',\n",
    "            'rh.streams_challenge_space.npy']\n",
    "        self.lh_challenge_rois = []\n",
    "        self.rh_challenge_rois = []\n",
    "        for r in range(len(lh_challenge_roi_files)):\n",
    "            self.lh_challenge_rois.append(np.load(f\"{self.roi_masks_dir}/{lh_challenge_roi_files[r]}\"))\n",
    "            self.rh_challenge_rois.append(np.load(f\"{self.roi_masks_dir}/{rh_challenge_roi_files[r]}\"))\n",
    "\n",
    "    def load_roi_masks(self, roi=\"V1v\", hemisphere=\"lh\"):\n",
    "        valid_roi = [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \n",
    "                     \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \n",
    "                     \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \n",
    "                     \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \n",
    "                     \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \n",
    "                     \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \n",
    "                     \"midparietal\", \"ventral\", \"lateral\", \"parietal\",\n",
    "                     \"all-vertices\"]\n",
    "        valid_hemisphere = [\"lh\", \"rh\"]\n",
    "        assert roi in valid_roi, \"Invalid ROI\"\n",
    "        assert hemisphere in valid_hemisphere, \"Invalid hemisphere\"\n",
    "\n",
    "        # Define the ROI class based on the selected ROI\n",
    "        if roi in [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\"]:\n",
    "            roi_class = 'prf-visualrois'\n",
    "        elif roi in [\"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\"]:\n",
    "            roi_class = 'floc-bodies'\n",
    "        elif roi in [\"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\"]:\n",
    "            roi_class = 'floc-faces'\n",
    "        elif roi in [\"OPA\", \"PPA\", \"RSC\"]:\n",
    "            roi_class = 'floc-places'\n",
    "        elif roi in [\"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\"]:\n",
    "            roi_class = 'floc-words'\n",
    "        elif roi in [\"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]:\n",
    "            roi_class = 'streams'\n",
    "        else:\n",
    "            roi_class = roi\n",
    "        roi_class_dir = f\"{hemisphere}.{roi_class}_fsaverage_space.npy\"\n",
    "        roi_map_dir = f\"mapping_{roi_class}.npy\"\n",
    "        fsaverage_roi_class = np.load(f\"{self.roi_masks_dir}/{roi_class_dir}\")\n",
    "        roi_map = None\n",
    "        if roi != \"all-vertices\":\n",
    "            roi_map = np.load(f\"{self.roi_masks_dir}/{roi_map_dir}\", allow_pickle=True).item()\n",
    "        return fsaverage_roi_class, roi_map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLIPFeatureExtractor Class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CLIPFeatureExtractor class is used to extract the hidden states from a clip model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPFeatureExtractor():\n",
    "    \"\"\"Extracts the features from hidden states of a CLIP model.\"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            idxs: list = [i for i in range(13)], # hidden layer indices to extract features from. Standard CLIP has an embedding layer and 12 transformer layers.\n",
    "            last_hidden_layer: bool = False, # whether to extract features from the last hidden layer\n",
    "            model: PreTrainedModel = None, # CLIP model\n",
    "            dataloader: DataLoader = None, # dataloader for batching\n",
    "            ) -> None:\n",
    "        self.idxs = idxs\n",
    "        self.last_hidden_layer = last_hidden_layer\n",
    "        self.generate_feature_dict()\n",
    "        if self.last_hidden_layer:\n",
    "            self.idxs.append(13) # adds an additional idx to allow for loop zip()\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "    \n",
    "    def generate_feature_dict(self) -> None:\n",
    "        \"\"\"Generates a feature dict according to the idxs and last_hidden_layer attributes.\"\"\"\n",
    "        feature_dict = {}\n",
    "        for idx in self.idxs:\n",
    "            if idx == 0:\n",
    "                feature_dict[\"Embedding Layer\"] = None\n",
    "            else:\n",
    "                feature_dict[f\"Transformer Layer {idx}\"] = None\n",
    "        if self.last_hidden_layer:\n",
    "            feature_dict[\"Final Layer\"] = None\n",
    "        self.feature_dict = feature_dict\n",
    "    \n",
    "    def concat_features(self, features: dict) -> None:\n",
    "        \"\"\"Adds extracted features to the feature dict.\n",
    "        Args:\n",
    "            features: features extracted from the output of a CLIP model\"\"\"\n",
    "        keys = list(self.feature_dict.keys())\n",
    "        # check if feature_dict is empty\n",
    "        if self.feature_dict[keys[0]] is None:\n",
    "            self.feature_dict = features\n",
    "        else:\n",
    "            for key in keys:\n",
    "                self.feature_dict[key] = np.concatenate((self.feature_dict[key], features[key]), axis=0)\n",
    "\n",
    "    def extract_raw_features(self, output) -> None: \n",
    "        \"\"\"Extracts features from the hidden states of a CLIP model and concates them to the feature_dict.\n",
    "        Args:\n",
    "            output: output of a CLIP model\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        for idx, key in zip(self.idxs, self.feature_dict.keys()):\n",
    "            if key == \"Final Layer\":\n",
    "                features[key] = output.last_hidden_state.cpu().detach().numpy()\n",
    "            else:\n",
    "                features[key] = output.hidden_states[idx].cpu().detach().numpy()\n",
    "        self.concat_features(features)\n",
    "    \n",
    "    def extract_raw_features_from_model(self) -> None:\n",
    "        \"\"\"Runs the CLIP model on the dataloader and extracts features from the hidden states.\"\"\"\n",
    "        self.model = self.model.to(device)\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.dataloader):\n",
    "                batch = batch.to(device)\n",
    "                output = self.model(batch, output_hidden_states=True)\n",
    "                self.extract_raw_features(output)\n",
    "                batch = None # clear batch from memory\n",
    "                output = None # clear output from memory\n",
    "        self.model = self.model.to(\"cpu\")        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KFoldProcedure & KFold Classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KFoldProcedure class is used to define a procedure that is supposed to be executed during each fold of the k-fold validation. \n",
    "It can be supplied to a KFold class which executes its run() function on all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KFoldProcedure:\n",
    "    \"\"\"This class is used to define a procedure that is run on each fold of a k-fold cross validation.\"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        self.roi_names = []\n",
    "\n",
    "    def prepare(self) -> None:\n",
    "        \"\"\"Operations that should be executed before the fold loop\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def run(self, train_idxs: np.ndarray, val_idxs: np.ndarray) -> Dict[str, Dict[str, np.ndarray]]:\n",
    "        \"\"\"This should return a dict of correlations.\n",
    "        dict format: {\"layer\": {\"lh\": np.ndarray, \"lh\": np.ndarray}}\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def return_idxs(self):\n",
    "        \"\"\"Returns idxs to create folds in the KFold class.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def return_roi_names(self) -> List[str]:\n",
    "        \"\"\"Required for the plot function in the KFold class.\"\"\"\n",
    "        return self.roi_names    \n",
    "    \n",
    "class KFold:\n",
    "    \"\"\"Run a k-fold cross validation with a given procedure.\"\"\"\n",
    "    def __init__(self, folds: int = 8, seed: int = 5, procedure: KFoldProcedure = None) -> None:\n",
    "        assert folds > 1, \"folds must be greater than 1\"\n",
    "        assert seed > 0, \"seed must be greater than 0\"\n",
    "        assert isinstance(folds, int), \"folds must be an integer\"\n",
    "        assert isinstance(seed, int), \"seed must be an integer\"\n",
    "        assert isinstance(procedure, KFoldProcedure), \"procedure must be an instance of KFoldProcedure\"\n",
    "        self.folds = folds\n",
    "        self.seed = seed\n",
    "        self.procedure = procedure\n",
    "        self.fold_correlations = {}\n",
    "        self.mean_correlations = None\n",
    "\n",
    "    def run(self) -> None:\n",
    "        \"\"\"Runs the procedure on each fold and accesses the correlations.\"\"\"\n",
    "        self.procedure.prepare()\n",
    "        # Create k folds   \n",
    "        fold_idxs = self.procedure.return_idxs()\n",
    "        np.random.seed(self.seed)\n",
    "        np.random.shuffle(fold_idxs)\n",
    "        self.fold_idxs = np.array_split(fold_idxs, self.folds)\n",
    "\n",
    "        for fold in range(self.folds):\n",
    "            # Select validation and train set\n",
    "            val_idxs = self.fold_idxs[fold]\n",
    "            train_idxs = np.concatenate([self.fold_idxs[j] for j in range(self.folds) if j != fold])\n",
    "            \n",
    "            # Info for current fold\n",
    "            print(f\"#############################################\")\n",
    "            print(f\"# Fold: {fold+1}/ {self.folds}\")         \n",
    "            print(f\"# Train size: {len(train_idxs)}\")\n",
    "            print(f\"# Validation size: {len(val_idxs)}\")\n",
    "            print(f\"#############################################\")\n",
    "\n",
    "            # Run procedure\n",
    "            self.correlations[fold] = self.procedure.run(train_idxs, val_idxs)\n",
    "        # Get ROI names\n",
    "        self.roi_names = self.procedure.return_roi_names()\n",
    "    \n",
    "    def calculate_mean_accross_folds(self):\n",
    "        \"\"\"Calculates the mean across folds for each layer\"\"\"\n",
    "        self.mean_correlations = {}\n",
    "        for layer in self.fold_correlations[0].keys():\n",
    "            self.mean_correlations[layer] = {}\n",
    "            for hemi in self.fold_correlations[0][layer].keys():\n",
    "                self.mean_correlations[layer][hemi] = np.nanmean([self.fold_correlations[fold][layer][hemi] for fold in range(self.folds)], axis=0)\n",
    "    \n",
    "    def plot_results(self):\n",
    "        \"\"\"Creates two side by side bar plots for the two hemispheres showing the mean correlations of each layer\"\"\"\n",
    "        if self.mean_correlations is None:\n",
    "            self.calculate_mean_accross_folds()\n",
    "        width = 0.5\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "        for i, layer in enumerate(self.mean_correlations):\n",
    "            ax[0].bar(np.arange(len(self.mean_correlations[layer][\"lh\"])) - width*i, self.mean_correlations[layer][\"lh\"], width, label=f\"{layer} lh\")\n",
    "            ax[1].bar(np.arange(len(self.mean_correlations[layer][\"rh\"])) - width*i, self.mean_correlations[layer][\"rh\"], width, label=f\"{layer} rh\")\n",
    "        ax[0].set_xticks(range(len(self.roi_names)))\n",
    "        ax[0].set_xticklabels(self.roi_names, rotation=90)\n",
    "        ax[0].set_ylabel(\"Correlation\")\n",
    "        ax[0].set_xlabel(\"ROI\")\n",
    "        ax[0].legend()\n",
    "        ax[0].set_title(\"Mean correlation with ROIs accross folds (left hemisphere))\")\n",
    "        ax[1].set_xticks(range(len(self.roi_names)))\n",
    "        ax[1].set_xticklabels(self.roi_names, rotation=90)\n",
    "        ax[1].set_ylabel(\"Correlation\")\n",
    "        ax[1].set_xlabel(\"ROI\")\n",
    "        ax[1].legend()\n",
    "        ax[1].set_title(\"Mean correlation with ROIs accross folds (right hemisphere)\")\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the procedures we used to select our models for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KFoldSingleCLIPSingleSubject(KFoldProcedure):\n",
    "    \"\"\"A procedure that runs k-fold on all layers of a single CLIP model on a single subject.\"\"\"\n",
    "    def __init__(self, \n",
    "                 feature_extractor: CLIPFeatureExtractor,\n",
    "                 subject: Subject, \n",
    "                 pca: PCA) -> None:\n",
    "        assert isinstance(feature_extractor, CLIPFeatureExtractor), \"feature_extractor must be an instance of CLIPFeatureExtractor\"\n",
    "        assert isinstance(subject, Subject), \"subject must be an instance of Subject\"\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.subject = subject\n",
    "        self.pca = pca\n",
    "        self.correlations = {}\n",
    "\n",
    "    def prepare(self):\n",
    "        # Extract raw features\n",
    "        self.feature_extractor.extract_raw_features_from_model()\n",
    "        # Load challenge rois\n",
    "        self.subject.load_challenge_rois()\n",
    "        # Load neural data\n",
    "        self.subject.load_neural_data()\n",
    "        self.fold_correlations = {}\n",
    "\n",
    "    def run(self, train_idxs: np.ndarray, val_idxs: np.ndarray) -> Dict[str, Dict[str, np.ndarray]]:\n",
    "        # Loop over all layers          \n",
    "        correlations = {}\n",
    "        for layer in self.feature_extractor.feature_dict.keys():\n",
    "            print(f\"> {layer}\")\n",
    "\n",
    "            # Fit PCA models\n",
    "            print(f\"Fitting PCA model for {layer}...\")\n",
    "            train_pca_features = self.pca.fit_transform(torch.tensor(self.feature_extractor.feature_dict[layer][train_idxs]).flatten(1).numpy())\n",
    "            \n",
    "            # Fit linear regression\n",
    "            print(f\"Fitting linear regression models for {layer}...\")\n",
    "            lh_lin_reg = LinearRegression().fit(train_pca_features, self.subject.lh_fmri[train_idxs])\n",
    "            rh_lin_reg = LinearRegression().fit(train_pca_features, self.subject.rh_fmri[train_idxs])\n",
    "            del train_pca_features # free memory\n",
    "\n",
    "            # Transform validation features\n",
    "            print(f\"Transforming validation features for {layer}...\")\n",
    "            val_txt_pca_features = self.pca.transform(torch.tensor(self.feature_extractor.feature_dict[layer][val_idxs]).flatten(1).numpy())\n",
    "            \n",
    "            # Predict validation set\n",
    "            print(f\"Predicting validation set for {layer}...\")\n",
    "            lh_val_pred = lh_lin_reg.predict(val_txt_pca_features)\n",
    "            rh_val_pred = rh_lin_reg.predict(val_txt_pca_features)\n",
    "            del val_txt_pca_features, lh_lin_reg, rh_lin_reg # free memory\n",
    "            \n",
    "            # Calculate correlations\n",
    "            print(f\"Calculating correlations for {layer}...\\n\")\n",
    "            # Left hemisphere\n",
    "            lh_correlation = np.zeros(lh_val_pred.shape[1])\n",
    "            for v in tqdm(range(lh_val_pred.shape[1])):\n",
    "                lh_correlation[v] = corr(lh_val_pred[:,v], self.subject.lh_fmri[val_idxs][:,v])[0]\n",
    "            # Right hemisphere\n",
    "            rh_correlation = np.zeros(rh_val_pred.shape[1])\n",
    "            for v in tqdm(range(rh_val_pred.shape[1])):\n",
    "                rh_correlation[v] = corr(rh_val_pred[:,v], self.subject.rh_fmri[val_idxs][:,v])[0]\n",
    "\n",
    "            # Get median correlations with ROI\n",
    "            print(f\"Calculating median correlation with ROIs for {layer}...\")\n",
    "            # Select the correlation results vertices of each ROI\n",
    "            self.roi_names = []\n",
    "            lh_roi_correlation = []\n",
    "            rh_roi_correlation = []\n",
    "            for r1 in range(len(self.subject.lh_challenge_rois)):\n",
    "                for r2 in self.subject.roi_name_maps[r1].items():\n",
    "                    if r2[0] != 0: # zeros indicate to vertices falling outside the ROI of interest\n",
    "                        self.roi_names.append(r2[1])\n",
    "                        lh_roi_idx = np.where(self.subject.lh_challenge_rois[r1] == r2[0])[0]\n",
    "                        rh_roi_idx = np.where(self.subject.rh_challenge_rois[r1] == r2[0])[0]\n",
    "                        lh_roi_correlation.append(lh_correlation[lh_roi_idx])\n",
    "                        rh_roi_correlation.append(rh_correlation[rh_roi_idx])\n",
    "            self.roi_names.append('All vertices')\n",
    "            lh_roi_correlation.append(lh_correlation)\n",
    "            rh_roi_correlation.append(rh_correlation)\n",
    "            lh_median_roi_correlation = [np.median(lh_roi_correlation[r])\n",
    "                for r in range(len(lh_roi_correlation))]\n",
    "            rh_median_roi_correlation = [np.median(rh_roi_correlation[r])\n",
    "                for r in range(len(rh_roi_correlation))]\n",
    "            \n",
    "            # Store correlations\n",
    "            correlations[layer] = {\"lh\": lh_median_roi_correlation, \"rh\": rh_median_roi_correlation} \n",
    "        return correlations\n",
    "\n",
    "    def return_idxs(self) -> np.ndarray:\n",
    "        return np.arange(len(self.feature_extractor.feature_dict[list(self.feature_extractor.feature_dict.keys())[0]])) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SubmissionProcedure & CreateSubmission Classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SubmissionProcedure class is used to define a procedure which fits a model to predict the test fmri activity. \n",
    "It can be supplied to a CreateSubmission class which executes its run() function on all subjects to generate a submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubmissionProcedure:\n",
    "    \"\"\"Used to create a submission procedure that is executed for each subject in the CreateSubmission class.\"\"\"\n",
    "\n",
    "    def run(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class CreateSubmission:\n",
    "    \"\"\"Create a new challenge submission.\"\"\"\n",
    "    def __init__(self, \n",
    "                 subjects : List[Subject],\n",
    "                 procedure: SubmissionProcedure):\n",
    "        self.subjects = subjects\n",
    "        self.procedure = procedure\n",
    "\n",
    "    def create_submission_folder(self) -> None:\n",
    "        # create new submission folder with newest name\n",
    "        submissions = glob.glob(f\"submissions/submission*\")\n",
    "        if len(submissions) == 0:\n",
    "            # create first submission folder\n",
    "            self.folder_name = \"submission001\"\n",
    "            os.mkdir(f\"submissions/{self.folder_name}\")\n",
    "        else:\n",
    "            # create next submission folder\n",
    "            last_submission = sorted(submissions)[-1]\n",
    "            last_submission_number = int(last_submission.split(\"/\")[-1].split(\"submission\")[-1])\n",
    "            next_submission_number = last_submission_number + 1\n",
    "            self.folder_name = f\"submission{str(next_submission_number).zfill(3)}\"\n",
    "            os.mkdir(f\"submissions/{self.folder_name}\")\n",
    "        # Write text file with model description\n",
    "        with open(f\"submissions/{self.folder_name}/info.txt\", \"w\") as f:\n",
    "            f.write(self.procedure.description)\n",
    "        # create a folder for each subject\n",
    "        for subject in self.subjects:\n",
    "            os.mkdir(f\"submissions/{self.folder_name}/{subject.subject}\")\n",
    "\n",
    "    def save_predictions(self, subject: Subject, lh_predictions: np.ndarray, rh_predictions: np.ndarray) -> None:\n",
    "        \"\"\"Save predictions for a subject.\"\"\"\n",
    "        lh_predictions = lh_predictions.astype(np.float32)\n",
    "        rh_predictions = rh_predictions.astype(np.float32)\n",
    "        save_path = f\"submissions/{self.folder_name}/{subject.subject}\"\n",
    "        # Save predictions\n",
    "        np.save(f\"{save_path}/lh_pred_test.npy\", lh_predictions)\n",
    "        np.save(f\"{save_path}/rh_pred_test.npy\", rh_predictions)\n",
    "\n",
    "    def run(self):\n",
    "        self.create_submission_folder()\n",
    "        for subject in self.subjects:\n",
    "            lh_predictions, rh_predictions = self.procedure.run(subject)\n",
    "            self.save_predictions(subject, lh_predictions, rh_predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the models we submitted to the algonauts 2023 competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPVisionLayer12PCA100LinearRegression(SubmissionProcedure):\n",
    "    def __init__(self, description: str):\n",
    "        self.description = description\n",
    "\n",
    "    def run(self, subject: Subject) -> np.ndarray:\n",
    "        \"\"\"Run the model on a subject.\"\"\"\n",
    "        # Prepare\n",
    "        subject.create_dataloaders(processor=processor, batch_size=300)\n",
    "        subject.load_neural_data()\n",
    "        train_feature_extractor = CLIPFeatureExtractor(idxs=[12], last_hidden_layer=False, model=vis_model, dataloader=subject.train_img_dataloader)\n",
    "        test_feature_extractor = CLIPFeatureExtractor(idxs=[12], last_hidden_layer=False, model=vis_model, dataloader=subject.test_img_dataloader)\n",
    "        # Extract features\n",
    "        train_feature_extractor.extract_raw_features_from_model()\n",
    "        # Fit PCA\n",
    "        pca = PCA(n_components=100)\n",
    "        pca_transformed_train_features = pca.fit_transform(torch.tensor(train_feature_extractor.feature_dict[\"Transformer Layer 12\"]).flatten(1).numpy())\n",
    "        # Fit linear regression\n",
    "        lh_lin_reg = LinearRegression().fit(pca_transformed_train_features, subject.lh_fmri)\n",
    "        rh_lin_reg = LinearRegression().fit(pca_transformed_train_features, subject.rh_fmri)\n",
    "        del pca_transformed_train_features, train_feature_extractor\n",
    "        # Extract test features\n",
    "        test_feature_extractor.extract_raw_features_from_model()\n",
    "        pca_transformed_test_features = pca.transform(torch.tensor(test_feature_extractor.feature_dict[\"Transformer Layer 12\"]).flatten(1).numpy())\n",
    "        # Predict\n",
    "        lh_predictions = lh_lin_reg.predict(pca_transformed_test_features)\n",
    "        rh_predictions = rh_lin_reg.predict(pca_transformed_test_features)\n",
    "        return lh_predictions, rh_predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Cross Validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the selected kfold procedure, different requirements like creating a subject or feature extractor are needed.\n",
    "\n",
    "In this example we use the KFoldSingleCLIPSingleSubject procedure which requires a subject, feature extractor and pca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 9841\n",
      "Test images: 159\n",
      "Training captions: 9841\n",
      "Test captions: 159\n",
      "Train image dataloader: 33 batches\n",
      "Test image dataloader: 1 batches\n",
      "Train caption dataloader: 33 batches\n",
      "Test caption dataloader: 1 batches\n"
     ]
    }
   ],
   "source": [
    "subject = Subject(\"subj01\")\n",
    "subject.create_dataloaders(processor=processor, batch_size=300)\n",
    "feature_extractor = CLIPFeatureExtractor(idxs=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], last_hidden_layer=True, model=vis_model, dataloader=subject.train_img_dataloader)\n",
    "pca = PCA(n_components=200)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining all required objects, we can create a the KFoldProcedure and KFold objects and run the cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_procedure = KFoldSingleCLIPSingleSubject(feature_extractor=feature_extractor,  subject=subject, pca=pca)\n",
    "kfold = KFold(folds=8, procedure=kfold_procedure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the Kfold procedure the mean correlations across all folds are plotted for each layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold.plot_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
